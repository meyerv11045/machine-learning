{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Linear regression can be used to predict continues values. The below example is for uber pricing with the distance traveled as the x data and the price of the ride as the y data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACiZYcWF9ryt",
        "outputId": "4395b4bd-9437-49f9-a8b3-3bd9d86ad3e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "dist = [9, 1.4, 15, 73, 0.5, 9.1, 12, 5.8, 4.1] # miles\n",
        "fare = [8.82, 8.09, 30, 81.4, 7.61, 19.91, 27.96, 26.96, 10.96] # $\n",
        "\n",
        "x = np.asarray(dist).reshape((-1,1))\n",
        "y = np.asarray(fare)\n",
        " \n",
        "model = LinearRegression(fit_intercept=True, copy_X=True)\n",
        "model.fit(x,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQ2Is01N-3Co",
        "outputId": "6239d76b-10cc-41b4-bc9e-c96c709e441a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "intercept:  10.32573065351679\n",
            "slope:  [0.99136585]\n"
          ]
        }
      ],
      "source": [
        "print('intercept: ', model.intercept_)\n",
        "print('slope: ', model.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af59frF__U7P"
      },
      "source": [
        "Model for Linear Regression: $h(x) = \\theta_0 + \\theta_1 x $\n",
        "In this example of uber pricing data, the resulting $\\theta_0 = 10.3$ and $\\theta_1 = 0.99$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "ssi_OXm9AmL4",
        "outputId": "1f51e441-bee6-4888-c721-059d5278f9d7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.scatter(x,y)\n",
        "\n",
        "xfit = np.linspace(0,100,1000)\n",
        "\n",
        "yfit = model.predict(xfit.reshape((-1,1)))\n",
        "\n",
        "plt.plot(xfit, yfit)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDVf08e_B9Dz"
      },
      "source": [
        "# Linear Regression of Single Variable\n",
        "\n",
        "Below we define the sum of squares cost function (note: $2m$ in denominator to simplfy the derivative whem optimizing).\n",
        "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2 $$\n",
        "\n",
        "The quadratic nature of the cost function above means the cost surface is convex and thus has a global minimum.\n",
        "\n",
        "$$ \\theta^* = argmin_\\theta J(\\theta) $$ \n",
        "\n",
        "$$h_\\theta(x) = \\theta_0 + \\theta_1 x$$\n",
        "\n",
        "Use an optimization technique to solve for the optimal parameters $\\theta^*$, then you can make predictions for new inputs $x$ using $h_{\\theta^*}(x)$.\n",
        "\n",
        "# Gradient Descent\n",
        "An iterative optimization technique for finding the optimal parameters $\\theta^*$ of the cost function. \n",
        "\n",
        "The learning rate/factor $\\alpha$ controls how big of a step to take towards the optimal parameters. Below is the least mean squares update rule. the magnitude of the update is proportional to the error term. meaning if the prediction nearly matches the target, then the parameters are changed very little.\n",
        "\n",
        "$$\\theta_0 := \\theta_0 - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_0} $$\n",
        "\n",
        "$$\\theta_1 := \\theta_1 - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_1} $$\n",
        "\n",
        "Run the above update steps until it converges. We define convergence as when $ \\lvert \\frac{\\partial J(\\theta}{\\partial \\theta_k} \\rvert < ϵ$ where $ϵ$ is small. We monitor convergence by plotting the loss function and ensuring it goes down.\n",
        "\n",
        "We can write the update steps using vector notation: \n",
        "\n",
        "$$\\bar\\theta := \\bar\\theta - \\alpha \\nabla_\\theta J(\\bar\\theta) $$\n",
        "\n",
        "Works well on convex cost surfaces but performs poorly when the cost function results in a non-convex cost surface.\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\theta_0} =  \\frac{1}{m} \\sum (h_\\theta(x^{(i)}) -  y^{(i)})$$\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\theta_1} =  \\frac{1}{m} \\sum (h_\\theta(x^{(i)}) -  y^{(i)})x^{(i)}$$\n",
        "\n",
        "Generalized: $\\frac{\\partial J}{\\partial \\theta_j} =  \\frac{1}{m} \\sum (h_\\theta(x^{(i)}) -  y^{(i)})x_j^{(i)}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normal Equations\n",
        "We can vectorize the above equations:\n",
        "\n",
        "$h_\\theta(\\bar{x}) = \\bar \\theta^T \\bar x $ where $\\bar x = [1 , x_1]$ and $\\bar \\theta = [\\theta_0, \\theta_1]$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GcntKhGI9QK"
      },
      "source": [
        "# Learning Rate Experiments\n",
        "\n",
        "$$J(\\theta) = (\\theta_0 - 5)^2 + 10 $$\n",
        "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_0} = 2\\theta_0 - 10$$\n",
        "\n",
        "The experiments run below demonstrate that $\\alpha = 1$ is bad and things never converge. Takeaway: learning rate plays a critical role in the convergence of gradient descent to the optimal parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8jyW2NbLOjL"
      },
      "source": [
        "Summary:\n",
        "Today i saw the connection between machine learning and optimization techniques being learned in Math 4630. The optimization techniques form the basis of finding the optimal parameters in many ML methods. These curve fitting applications in data science/ML are basically just optimization problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5iilXhRExJY",
        "outputId": "3b8a8a5c-dbd0-4506-b0b9-5389d12898b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial theta: 1 \t final theta: 1 \t iterations: 0 \t alpha: 1\n",
            "-------------------\n",
            "initial theta: 1 \t final theta: 1 \t iterations: 0 \t alpha: 0.5\n",
            "-------------------\n",
            "initial theta: 1 \t final theta: 1 \t iterations: 0 \t alpha: 0.4\n",
            "-------------------\n",
            "initial theta: 1 \t final theta: 1 \t iterations: 0 \t alpha: 0.1\n",
            "-------------------\n",
            "initial theta: 1 \t final theta: 1 \t iterations: 0 \t alpha: 0.01\n",
            "-------------------\n",
            "initial theta: 2 \t final theta: 2 \t iterations: 0 \t alpha: 1\n",
            "-------------------\n",
            "initial theta: 2 \t final theta: 2 \t iterations: 0 \t alpha: 0.5\n",
            "-------------------\n",
            "initial theta: 2 \t final theta: 2 \t iterations: 0 \t alpha: 0.4\n",
            "-------------------\n",
            "initial theta: 2 \t final theta: 2 \t iterations: 0 \t alpha: 0.1\n",
            "-------------------\n",
            "initial theta: 2 \t final theta: 2 \t iterations: 0 \t alpha: 0.01\n",
            "-------------------\n",
            "initial theta: 7 \t final theta: 3 \t iterations: 1 \t alpha: 1\n",
            "-------------------\n",
            "initial theta: 7 \t final theta: 5.0 \t iterations: 1 \t alpha: 0.5\n",
            "-------------------\n",
            "initial theta: 7 \t final theta: 5.0032 \t iterations: 4 \t alpha: 0.4\n",
            "-------------------\n",
            "initial theta: 7 \t final theta: 5.009444732965739 \t iterations: 24 \t alpha: 0.1\n",
            "-------------------\n",
            "initial theta: 7 \t final theta: 5.265239111789508 \t iterations: 100 \t alpha: 0.01\n",
            "-------------------\n",
            "initial theta: 8 \t final theta: 2 \t iterations: 1 \t alpha: 1\n",
            "-------------------\n",
            "initial theta: 8 \t final theta: 5.0 \t iterations: 1 \t alpha: 0.5\n",
            "-------------------\n",
            "initial theta: 8 \t final theta: 5.0048 \t iterations: 4 \t alpha: 0.4\n",
            "-------------------\n",
            "initial theta: 8 \t final theta: 5.00906694364711 \t iterations: 26 \t alpha: 0.1\n",
            "-------------------\n",
            "initial theta: 8 \t final theta: 5.397858667684258 \t iterations: 100 \t alpha: 0.01\n",
            "-------------------\n",
            "initial theta: 25 \t final theta: -15 \t iterations: 1 \t alpha: 1\n",
            "-------------------\n",
            "initial theta: 25 \t final theta: 5.0 \t iterations: 1 \t alpha: 0.5\n",
            "-------------------\n",
            "initial theta: 25 \t final theta: 5.0064 \t iterations: 5 \t alpha: 0.4\n",
            "-------------------\n",
            "initial theta: 25 \t final theta: 5.0081129638414605 \t iterations: 35 \t alpha: 0.1\n",
            "-------------------\n",
            "initial theta: 25 \t final theta: 7.652391117895066 \t iterations: 100 \t alpha: 0.01\n",
            "-------------------\n"
          ]
        }
      ],
      "source": [
        "thetas = [1, 2, 7, 8, 25]\n",
        "alphas = [1, 0.5, 0.4, 0.1, 0.01]\n",
        "iters = 50\n",
        "\n",
        "for t in thetas:\n",
        "  for alpha in alphas:\n",
        "    theta = t\n",
        "    iters = 0\n",
        "    while theta - 5.0 > 1e-2 and iters < 100:\n",
        "      theta -= alpha * (2* theta - 10)\n",
        "      iters += 1\n",
        "    \n",
        "    print(f'initial theta: {t} \\t final theta: {theta} \\t iterations: {iters} \\t alpha: {alpha}' )\n",
        "    print('-------------------') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Regression of Multiple Variables\n",
        "Remember to use $x_0 = 1$ to make the vector multiplication work\n",
        "\n",
        "$$h_\\theta(\\bar x) = \\theta_0 x_0 + \\theta_1 x_ 1 + ... + \\theta_n x_n = \\bar \\theta^T \\bar x$$\n",
        "\n",
        "$$J(\\bar \\theta) = \\frac{1}{2m} \\sum_{i=1}^m (\\bar \\theta^T \\bar x^{(i)} - y^{(i)})^2 $$\n",
        "\n",
        "We can find the optimal parameters by minimizing the cost function using gradient descent or we can use the closed form solution by setting the gradient of the cost function equal to zero and solving the system of equations.\n",
        "\n",
        "\n",
        "## Closed Form Solution\n",
        "Set the gradient of the cost function equal to zero and solve the system of equations:\n",
        "\n",
        "$$\\nabla_\\theta J = \\bar 0$$\n",
        "\n",
        "We use a $m$ by $n$ design matrix containing all our features for the training dataset:\n",
        "\n",
        "$X = \\begin{bmatrix}\n",
        "\\bar x ^{(1)T} \\\\\n",
        "\\bar x ^{(2)T} \\\\\n",
        "\\vdots \\\\\n",
        "\\bar x ^{(m)T}\n",
        "\\end{bmatrix}$ which is $m$ x $n$\n",
        "\n",
        "We also use a $m$ by $1$ matrix containing all our targets for the training dataset:\n",
        "\n",
        "$\\bar \\y = \\begin{bmatrix}\n",
        "\\bar y^{(1)} \\\\\n",
        "\\bar y ^{(2)} \\\\\n",
        "\\vdots \\\\\n",
        "\\bar y ^{(m)}\n",
        "\\end{bmatrix}$\n",
        "\n",
        "\n",
        "Finally we want to find the optimal parameters $\\bar \\theta^*$ which is an $n + 1$ x $1$ matrix \n",
        "\n",
        "Note: $\\bar \\theta^T \\bar x = \\bar x^T \\bar \\theta$ \n",
        "\n",
        "$$J(\\bar \\theta) = (X\\bar\\theta - \\bar y) = \\bar z$$\n",
        "\n",
        "$$\\bar z^T \\bar z = \\sum_i^m(z^{(i)})^2$$\n",
        "\n",
        "The above is a nice way to write sum of squares in matrix form\n",
        "\n",
        "$$J(\\bar \\theta) = \\frac{1}{2m} (X\\bar\\theta - \\bar y)^T (X\\bar\\theta - \\bar y) $$\n",
        "\n",
        "$$\\nabla_\\theta J(\\theta) = X^TX \\bar \\theta - X^Ty$$\n",
        "\n",
        "so if we set the above equal to $\\bar 0$ and solve, we can get:\n",
        "\n",
        "$$\\bar \\theta^* = (X^T X)^{-1}X^Ty$$\n",
        "\n",
        "Inverting the matrix (pseudo or normal) is $O(n^3)$ so sometimes gradient descent is faster.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gradient Descent Methods\n",
        "A training epoch is one cycle through the entire dataset.\n",
        "\n",
        "## Batch\n",
        "Computes the cost for the entire dataset and only then updates the model parameters for each epoch.\n",
        "\n",
        "Performs model update at end of a training epoch.\n",
        "\n",
        "### Pros\n",
        "- Can parallelize the calculation of errors over the dataset and bring the results together to perform one model update\n",
        "- \n",
        "\n",
        "### Cons\n",
        "- Can get stuck in a local minumum (converges too soon) that is not the global min\n",
        "- Often implemented such that the entire dataset must be able to fit into memory\n",
        "- Slow training speed on large datasets due to the computational expense of using the entire dataset for each iteration\n",
        "\n",
        "## Stochastic\n",
        "Computes the cost for only a single sample at each epoch. \n",
        "\n",
        "Should be used for large datasets becuase it converges quickly after sampling only a small amount of the dataset, from which a batch or mini-batch technique can then be used to get finer \n",
        "tuned optimal parameters. \n",
        "\n",
        "\n",
        "## Stuff\n",
        "If dataset is small, batch is always good\n",
        "If dataset is large, batch will always converge but is costly/slow.\n",
        "\n",
        "If dataset is small, stochastic is usually good but can be noisy and move off optimal local min due to noise in the updates from variations in the training samples at each step. \n",
        "If dataset is large, stochastic is good for getting and estimate since it makes early progress\n",
        "\n",
        "For all datasets, stochastic gradient descent will have osciallations whereas bath will be fixed at the optimum. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
