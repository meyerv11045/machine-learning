{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Machine Learning Notebook Here's my collection of notes for reference and different persectives for how to think about complex topics.","title":"Home"},{"location":"#machine-learning-notebook","text":"Here's my collection of notes for reference and different persectives for how to think about complex topics.","title":"Machine Learning Notebook"},{"location":"machine_learning/activation_functions/","text":"Activation Functions Mathematical equations that determine the output of a node/neuron in a neural network Determines whether each neuron should be activated based on the neuron's input and the specific activation function Normalize the output of a neuron to a range: [0,1] or [-1,1] Must be computationally efficient b/c they are calculated across thousands or millions of neurons for each pass Having a computationally efficient derivative is good too for backpropagation Can be as simple as a step function that turns the neuron output on or off (e.g. 0 or 1) Binary Step Function (Doesn't allow multi-value outputs so it can't do multi-class classification) Can be a trasnformation that maps input signals to output signals Linear activations functions collapse a neural network into one layer b/c a linear combination of linear functions is still a linear function (not very useful) Can't use backpropagation with linear activation functions b/c the derivative is a constant so its impossible to see the effect of changing weights Non-linear activation functions allow the neural network to learn more complex data like images, videos, and audio Allow backpropagation b/c the derivative function is related to the inputs Allow sacking of multiple hidden layers unlike linear activation functions Most all processes can be modeled by a neural network w/ nonlinear activation functions Why do we need activation functions? Matrix multiplication and vector addition are simply linear transformations so in order to learn any nonlinear representations of data, we need to introduce a non-linear function or else the most complex classification boundary it can learn will be linear (e.g. line, plane, etc. ) which isn't that useful for interesting problems. In order to introduce non-linearity (otherwise matrix ops will result in only linear transformations), we apply non-linear activation functions pointwise to the results of the linear transformations at each layer. Sigmoid/Logistic \\sigma(x) = \\dfrac{1}{1 + e^{-x}} \\dfrac{d}{dx}\\sigma(x) = \\sigma(x)(1 - \\sigma(x)) Pros: Smooth Gradient prevents jumps in the output values Output vaues bound btw 0 and 1, normalizing the each neuron's output Clear predictions - any values above 2 or below -2 are very close to the edge of the curve Cons: Vanishing Gradient- almost no change in the prediction for very high and very low input values Can result in the network refusing to learn further or being too slow to reach an accurate prediction Outputs not zero centered Computationally expensive Detailed Derivation: \\begin{align}\\dfrac{d}{dx} \\sigma(x) &= \\dfrac{d}{dx} \\left[ \\dfrac{1}{1 + e^{-x}} \\right] \\\\ &= \\dfrac{d}{dx} \\left( 1 + \\mathrm{e}^{-x} \\right)^{-1} \\\\ &= -(1 + e^{-x})^{-2}(-e^{-x}) \\\\ &= \\dfrac{e^{-x}}{\\left(1 + e^{-x}\\right)^2} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{e^{-x}}{1 + e^{-x}} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{(1 + e^{-x}) - 1}{1 + e^{-x}} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( \\dfrac{1 + e^{-x}}{1 + e^{-x}} - \\dfrac{1}{1 + e^{-x}} \\right) \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( 1 - \\dfrac{1}{1 + e^{-x}} \\right) \\\\ &= \\sigma(x) \\cdot (1 - \\sigma(x)) \\end{align} TanH/ Hyperbolic Tangent Similar in advantages and disadvantages except it has the advantage of being zero centered , making it easier to models input that have strongly negative, neutral, and strongly positive values ReLU (Rectified Linear Unit) Pros: Computationally efficient - allows the network to converge quickly Non-linear - although it seems lik e a lienar function, its derivative allows for backpropagation Cons: The Dying ReLU problem - as inputs approach zero or are negative, the gradient becomes zero, so the network can't perform backpropagation and therefore cannot learn Leaky ReLU Pros: Prevents dying ReLU problem - the smallpositive slope in the negative area enables backpropagation for negative inputs Otherwise like ReLU Cons: Inconsistent Results - does't provide consistent predicitions for negative input values https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/","title":"Activation Functions"},{"location":"machine_learning/activation_functions/#activation-functions","text":"Mathematical equations that determine the output of a node/neuron in a neural network Determines whether each neuron should be activated based on the neuron's input and the specific activation function Normalize the output of a neuron to a range: [0,1] or [-1,1] Must be computationally efficient b/c they are calculated across thousands or millions of neurons for each pass Having a computationally efficient derivative is good too for backpropagation Can be as simple as a step function that turns the neuron output on or off (e.g. 0 or 1) Binary Step Function (Doesn't allow multi-value outputs so it can't do multi-class classification) Can be a trasnformation that maps input signals to output signals Linear activations functions collapse a neural network into one layer b/c a linear combination of linear functions is still a linear function (not very useful) Can't use backpropagation with linear activation functions b/c the derivative is a constant so its impossible to see the effect of changing weights Non-linear activation functions allow the neural network to learn more complex data like images, videos, and audio Allow backpropagation b/c the derivative function is related to the inputs Allow sacking of multiple hidden layers unlike linear activation functions Most all processes can be modeled by a neural network w/ nonlinear activation functions","title":"Activation Functions"},{"location":"machine_learning/activation_functions/#why-do-we-need-activation-functions","text":"Matrix multiplication and vector addition are simply linear transformations so in order to learn any nonlinear representations of data, we need to introduce a non-linear function or else the most complex classification boundary it can learn will be linear (e.g. line, plane, etc. ) which isn't that useful for interesting problems. In order to introduce non-linearity (otherwise matrix ops will result in only linear transformations), we apply non-linear activation functions pointwise to the results of the linear transformations at each layer.","title":"Why do we need activation functions?"},{"location":"machine_learning/activation_functions/#sigmoidlogistic","text":"\\sigma(x) = \\dfrac{1}{1 + e^{-x}} \\dfrac{d}{dx}\\sigma(x) = \\sigma(x)(1 - \\sigma(x)) Pros: Smooth Gradient prevents jumps in the output values Output vaues bound btw 0 and 1, normalizing the each neuron's output Clear predictions - any values above 2 or below -2 are very close to the edge of the curve Cons: Vanishing Gradient- almost no change in the prediction for very high and very low input values Can result in the network refusing to learn further or being too slow to reach an accurate prediction Outputs not zero centered Computationally expensive Detailed Derivation: \\begin{align}\\dfrac{d}{dx} \\sigma(x) &= \\dfrac{d}{dx} \\left[ \\dfrac{1}{1 + e^{-x}} \\right] \\\\ &= \\dfrac{d}{dx} \\left( 1 + \\mathrm{e}^{-x} \\right)^{-1} \\\\ &= -(1 + e^{-x})^{-2}(-e^{-x}) \\\\ &= \\dfrac{e^{-x}}{\\left(1 + e^{-x}\\right)^2} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{e^{-x}}{1 + e^{-x}} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{(1 + e^{-x}) - 1}{1 + e^{-x}} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( \\dfrac{1 + e^{-x}}{1 + e^{-x}} - \\dfrac{1}{1 + e^{-x}} \\right) \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( 1 - \\dfrac{1}{1 + e^{-x}} \\right) \\\\ &= \\sigma(x) \\cdot (1 - \\sigma(x)) \\end{align}","title":"Sigmoid/Logistic"},{"location":"machine_learning/activation_functions/#tanh-hyperbolic-tangent","text":"Similar in advantages and disadvantages except it has the advantage of being zero centered , making it easier to models input that have strongly negative, neutral, and strongly positive values","title":"TanH/ Hyperbolic Tangent"},{"location":"machine_learning/activation_functions/#relu-rectified-linear-unit","text":"Pros: Computationally efficient - allows the network to converge quickly Non-linear - although it seems lik e a lienar function, its derivative allows for backpropagation Cons: The Dying ReLU problem - as inputs approach zero or are negative, the gradient becomes zero, so the network can't perform backpropagation and therefore cannot learn","title":"ReLU (Rectified Linear Unit)"},{"location":"machine_learning/activation_functions/#leaky-relu","text":"Pros: Prevents dying ReLU problem - the smallpositive slope in the negative area enables backpropagation for negative inputs Otherwise like ReLU Cons: Inconsistent Results - does't provide consistent predicitions for negative input values https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/","title":"Leaky ReLU"},{"location":"machine_learning/map/","text":"Map of ML Supervised Learning Unsupervised Learning Auto-Encoders num input layers = num output layers but smaller num hidden layers. This forces the network to learn a compressed representation of the data. A simple autoencoder could take 100 dimensional data (100 input units) and represent it low dimensionally as 50 dimensions if using 50 hidden units. Relies on the input data having features (completely random noise would be tough to learn anything from) Mathematically, it can be thought of as trying to learn an approximation to the identity function so the output is similar to the input. Allows us to discover interesting structure about the data. Reinforcement Learning A control policy \\pi maps the continuous state vector x to a continuous action vector u (often a control vector). u = \\pi(x,t,\\theta) where \\theta is the set of parameters to be learned by the control policy and t is the specific timestep Model Predictive Control Learning a control policy offline and deploying for online, realtime predictions: - Reinforcement learning - Difficulty lies in structuring rewards and exploration so that the agent learns the optimal control policy - Takes a lot of episodes to train and converge on an optimal control policy - Supervised learning - Use an an algorithm that is known to create good control trajectories to generate ground truth training data on a bunch of environment state inputs (including edge cases) - Data generateed: (environment/agent state vector, control trajectory ground truth) - Theoretically can then train a NN to approximate this planning algorithm given enough good data - Difficulty lies in generating enough high quality training data (especially for things like robot control policies)","title":"Map of ML"},{"location":"machine_learning/map/#map-of-ml","text":"","title":"Map of ML"},{"location":"machine_learning/map/#supervised-learning","text":"","title":"Supervised Learning"},{"location":"machine_learning/map/#unsupervised-learning","text":"","title":"Unsupervised Learning"},{"location":"machine_learning/map/#auto-encoders","text":"num input layers = num output layers but smaller num hidden layers. This forces the network to learn a compressed representation of the data. A simple autoencoder could take 100 dimensional data (100 input units) and represent it low dimensionally as 50 dimensions if using 50 hidden units. Relies on the input data having features (completely random noise would be tough to learn anything from) Mathematically, it can be thought of as trying to learn an approximation to the identity function so the output is similar to the input. Allows us to discover interesting structure about the data.","title":"Auto-Encoders"},{"location":"machine_learning/map/#reinforcement-learning","text":"A control policy \\pi maps the continuous state vector x to a continuous action vector u (often a control vector). u = \\pi(x,t,\\theta) where \\theta is the set of parameters to be learned by the control policy and t is the specific timestep","title":"Reinforcement Learning"},{"location":"machine_learning/map/#model-predictive-control","text":"Learning a control policy offline and deploying for online, realtime predictions: - Reinforcement learning - Difficulty lies in structuring rewards and exploration so that the agent learns the optimal control policy - Takes a lot of episodes to train and converge on an optimal control policy - Supervised learning - Use an an algorithm that is known to create good control trajectories to generate ground truth training data on a bunch of environment state inputs (including edge cases) - Data generateed: (environment/agent state vector, control trajectory ground truth) - Theoretically can then train a NN to approximate this planning algorithm given enough good data - Difficulty lies in generating enough high quality training data (especially for things like robot control policies)","title":"Model Predictive Control"},{"location":"machine_learning/reinforcement_learning/","text":"Reinforcement Learning - the science of learning to make decisions from interaction Differs from other machine learning paradigms in that: No supervision, only a reward signal Feedback can be delayed, not instantaneous Time matters Earlier decisions affect later interactions Sequential decision problems can be solved w/ RL RL is a framework for how to think about these problems as well as a set of algorithms that can be used to solved these problems Core Concepts: Environment Reward Signal External to the learning algorithm even if its internal to the learning system as a whole Agent containing: Agent state Policy Value function (probably) Model (optionally) Rewards A reward R_t is a scalar feedback signal Indicates how well agent is doing at step t The agent's job is to maximize cumulative reward G_t = R_{t+1} + R_{t+2} + ... RL is based on the reward hypothesis Any goal can be formalized as the outcome of maximizing a cumulative reward Dense- reward on every step Sparse- reward only when the desired event happens Can be negative or positive rewads (Carrot vs. Stick in helping train the algorithm) Values Value - the expected cumulative reward from state s States use the v function to find their value v(s) = E[G_t | S_t = s] where G_t = R_{t+1} + R_{t+2} + ... Goal is to maximize value by picking suitable actions Rewards and values define desirability of state or acton Returns and values can be defined recursively G_t = R_{t+1} + G_{t+1} Actions Goal: Select actions to maximize value Actions may have long term consequences meaning rewards may be delayed Might be better to sacrific immediate reward to gain more long-term reward Ex: Financial investments might take months to mature Blocking opponent moves might help winning chances many moves in the future Policy - a mapping from states to actions Action Values It is possible to condition the value on actions q(s,a) = E[G_t | S_t = s, A_t = a] State action pairs use the Q-function to find their value Agent Components Agent State Policy Value Function Model State Actions depend on the state of the agent Both agent and environment may have an internal state In the simplest case, there is only one shared state by the environemnt and agent Often there are multiple (sometimes infinite) different states The agent's state is usually different than the environment's state The agent might not know the full state of the environment Environment State The environment's internal state Usually not visible to the agent May contain lots of irrelevant information if visible Agent State A history is a sequence of observations, action, rewards H_t = O_0,A_0,R_1,O_1,...,O_{t-1},A_{t-1},R_t,O_t The agent state S_t can be constructed from the history and is considerd to be a function of the history S_{t+1} = f(S_t,A_t,R_{t+1},O_{t+1}) where f is a state update function Means the next agent state is dependent on the current agent state, the action taken, the reward, and the observation A simple update function can be concatinating the previous states together to create the current state (Done w/ the Atari game agents by cocatenating frames together since frames are the state) Actions depend on this state Agent state is typically much smaller than the environment state and the full history Fully Observable Environments When the agent sees the full environment state observation = environment state Agent state could just be the observation S_t = O_t = environment state Then the agent is in a Markov Decision Process Ex: Single player board games in which the player can see the entire board Smart Pong is an example of fully observable environment (VERIFY) Markov Decision Processes MDPs provide a useful mathematical framework for talking about many RL topics Easier to reason about than the full problem (long markovian) Limited b/c of markov assumption Def- A decision process is Markov if the future is independent of the past given the present H_t => S_t => H_{t+1} Once the current state ( S_t ) is known , the history ( H_t ) may be thrown away (useful for space consideration) The current state gives enough information for the next state to be predicted The environment state is typically Markov The history is Markov Partially Observable Environments The agent gets parial information A robot w/ camera vision doesn't have its absolute location A poker playing agent only observes public cards Now the observation is not Markov Formally this is a partially observable Markov decision process (POMDP) The environment state can still be markov, but the agent does not know it The state should contain enough information for good policies/value predictions Policy Defines the agent's behavior A map from agent state to action Deterministic Policy - function that outputs an action based on an input agent state Stochastic Policy - a probability of selecting each action in each state Value Function: The actual value function is the expected return V_\\pi(s)=E[G_t|S_t=s,\\pi] \u200b = E[R_{t+1}+\\gamma R_{t+2}+\\gamma^2R_{t+3}+... | S_t = s,\\pi] The return has a recursive form since G_t = R_{t+1}+\\gamma G_{t+1} Bellman Equation : V_\\pi (s) = E[R_{t+1} + \\gamma G_{t+1} | S_t = s,A_t \\sim \\pi (s)] where a \\sim \\pi (s) means action a is chosen by policy \\pi in state s Discount Factor ( \\gamma \\in [0,1] ) trades off importance of immediate vs. long-term rewards Closer to 1 means higher importane on immediate rewards (AKA discounting future rewards in favor of immediate rewards) The value depends on the policy Can be used to evaluate desirability of states Can be used to select btw actions","title":"Reinforcement Learning"},{"location":"machine_learning/reinforcement_learning/#core-concepts","text":"Environment Reward Signal External to the learning algorithm even if its internal to the learning system as a whole Agent containing: Agent state Policy Value function (probably) Model (optionally)","title":"Core Concepts:"},{"location":"machine_learning/reinforcement_learning/#rewards","text":"A reward R_t is a scalar feedback signal Indicates how well agent is doing at step t The agent's job is to maximize cumulative reward G_t = R_{t+1} + R_{t+2} + ... RL is based on the reward hypothesis Any goal can be formalized as the outcome of maximizing a cumulative reward Dense- reward on every step Sparse- reward only when the desired event happens Can be negative or positive rewads (Carrot vs. Stick in helping train the algorithm)","title":"Rewards"},{"location":"machine_learning/reinforcement_learning/#values","text":"Value - the expected cumulative reward from state s States use the v function to find their value v(s) = E[G_t | S_t = s] where G_t = R_{t+1} + R_{t+2} + ... Goal is to maximize value by picking suitable actions Rewards and values define desirability of state or acton Returns and values can be defined recursively G_t = R_{t+1} + G_{t+1}","title":"Values"},{"location":"machine_learning/reinforcement_learning/#actions","text":"Goal: Select actions to maximize value Actions may have long term consequences meaning rewards may be delayed Might be better to sacrific immediate reward to gain more long-term reward Ex: Financial investments might take months to mature Blocking opponent moves might help winning chances many moves in the future Policy - a mapping from states to actions","title":"Actions"},{"location":"machine_learning/reinforcement_learning/#action-values","text":"It is possible to condition the value on actions q(s,a) = E[G_t | S_t = s, A_t = a] State action pairs use the Q-function to find their value","title":"Action Values"},{"location":"machine_learning/reinforcement_learning/#agent-components","text":"Agent State Policy Value Function Model","title":"Agent Components"},{"location":"machine_learning/reinforcement_learning/#state","text":"Actions depend on the state of the agent Both agent and environment may have an internal state In the simplest case, there is only one shared state by the environemnt and agent Often there are multiple (sometimes infinite) different states The agent's state is usually different than the environment's state The agent might not know the full state of the environment","title":"State"},{"location":"machine_learning/reinforcement_learning/#environment-state","text":"The environment's internal state Usually not visible to the agent May contain lots of irrelevant information if visible","title":"Environment State"},{"location":"machine_learning/reinforcement_learning/#agent-state","text":"A history is a sequence of observations, action, rewards H_t = O_0,A_0,R_1,O_1,...,O_{t-1},A_{t-1},R_t,O_t The agent state S_t can be constructed from the history and is considerd to be a function of the history S_{t+1} = f(S_t,A_t,R_{t+1},O_{t+1}) where f is a state update function Means the next agent state is dependent on the current agent state, the action taken, the reward, and the observation A simple update function can be concatinating the previous states together to create the current state (Done w/ the Atari game agents by cocatenating frames together since frames are the state) Actions depend on this state Agent state is typically much smaller than the environment state and the full history","title":"Agent State"},{"location":"machine_learning/reinforcement_learning/#fully-observable-environments","text":"When the agent sees the full environment state observation = environment state Agent state could just be the observation S_t = O_t = environment state Then the agent is in a Markov Decision Process Ex: Single player board games in which the player can see the entire board Smart Pong is an example of fully observable environment (VERIFY)","title":"Fully Observable Environments"},{"location":"machine_learning/reinforcement_learning/#markov-decision-processes","text":"MDPs provide a useful mathematical framework for talking about many RL topics Easier to reason about than the full problem (long markovian) Limited b/c of markov assumption Def- A decision process is Markov if the future is independent of the past given the present H_t => S_t => H_{t+1} Once the current state ( S_t ) is known , the history ( H_t ) may be thrown away (useful for space consideration) The current state gives enough information for the next state to be predicted The environment state is typically Markov The history is Markov","title":"Markov Decision Processes"},{"location":"machine_learning/reinforcement_learning/#partially-observable-environments","text":"The agent gets parial information A robot w/ camera vision doesn't have its absolute location A poker playing agent only observes public cards Now the observation is not Markov Formally this is a partially observable Markov decision process (POMDP) The environment state can still be markov, but the agent does not know it The state should contain enough information for good policies/value predictions","title":"Partially Observable Environments"},{"location":"machine_learning/reinforcement_learning/#policy","text":"Defines the agent's behavior A map from agent state to action Deterministic Policy - function that outputs an action based on an input agent state Stochastic Policy - a probability of selecting each action in each state","title":"Policy"},{"location":"machine_learning/reinforcement_learning/#value-function","text":"The actual value function is the expected return V_\\pi(s)=E[G_t|S_t=s,\\pi] \u200b = E[R_{t+1}+\\gamma R_{t+2}+\\gamma^2R_{t+3}+... | S_t = s,\\pi] The return has a recursive form since G_t = R_{t+1}+\\gamma G_{t+1} Bellman Equation : V_\\pi (s) = E[R_{t+1} + \\gamma G_{t+1} | S_t = s,A_t \\sim \\pi (s)] where a \\sim \\pi (s) means action a is chosen by policy \\pi in state s Discount Factor ( \\gamma \\in [0,1] ) trades off importance of immediate vs. long-term rewards Closer to 1 means higher importane on immediate rewards (AKA discounting future rewards in favor of immediate rewards) The value depends on the policy Can be used to evaluate desirability of states Can be used to select btw actions","title":"Value Function:"}]}