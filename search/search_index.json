{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Machine Learning Notebook Here's my collection of notes for reference and different persectives for how to think about complex topics.","title":"Home"},{"location":"#machine-learning-notebook","text":"Here's my collection of notes for reference and different persectives for how to think about complex topics.","title":"Machine Learning Notebook"},{"location":"machine_learning/activation_functions/","text":"Activation Functions Mathematical equations that determine the output of a node/neuron in a neural network Determines whether each neuron should be activated based on the neuron's input and the specific activation function Normalize the output of a neuron to a range: [0,1] or [-1,1] Must be computationally efficient b/c they are calculated across thousands or millions of neurons for each pass Having a computationally efficient derivative is good too for backpropagation Can be as simple as a step function that turns the neuron output on or off (e.g. 0 or 1) Binary Step Function (Doesn't allow multi-value outputs so it can't do multi-class classification) Can be a trasnformation that maps input signals to output signals Linear activations functions collapse a neural network into one layer b/c a linear combination of linear functions is still a linear function (not very useful) Can't use backpropagation with linear activation functions b/c the derivative is a constant so its impossible to see the effect of changing weights Non-linear activation functions allow the neural network to learn more complex data like images, videos, and audio Allow backpropagation b/c the derivative function is related to the inputs Allow sacking of multiple hidden layers unlike linear activation functions Most all processes can be modeled by a neural network w/ nonlinear activation functions Why do we need activation functions? Matrix multiplication and vector addition are simply linear transformations so in order to learn any nonlinear representations of data, we need to introduce a non-linear function or else the most complex classification boundary it can learn will be linear (e.g. line, plane, etc. ) which isn't that useful for interesting problems. In order to introduce non-linearity (otherwise matrix ops will result in only linear transformations), we apply non-linear activation functions pointwise to the results of the linear transformations at each layer. Sigmoid/Logistic \\sigma(x) = \\dfrac{1}{1 + e^{-x}} \\dfrac{d}{dx}\\sigma(x) = \\sigma(x)(1 - \\sigma(x)) Pros: Smooth Gradient prevents jumps in the output values Output vaues bound btw 0 and 1, normalizing the each neuron's output Clear predictions - any values above 2 or below -2 are very close to the edge of the curve Cons: Vanishing Gradient- almost no change in the prediction for very high and very low input values Can result in the network refusing to learn further or being too slow to reach an accurate prediction Outputs not zero centered Computationally expensive Detailed Derivation: \\begin{align}\\dfrac{d}{dx} \\sigma(x) &= \\dfrac{d}{dx} \\left[ \\dfrac{1}{1 + e^{-x}} \\right] \\\\ &= \\dfrac{d}{dx} \\left( 1 + \\mathrm{e}^{-x} \\right)^{-1} \\\\ &= -(1 + e^{-x})^{-2}(-e^{-x}) \\\\ &= \\dfrac{e^{-x}}{\\left(1 + e^{-x}\\right)^2} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{e^{-x}}{1 + e^{-x}} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{(1 + e^{-x}) - 1}{1 + e^{-x}} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( \\dfrac{1 + e^{-x}}{1 + e^{-x}} - \\dfrac{1}{1 + e^{-x}} \\right) \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( 1 - \\dfrac{1}{1 + e^{-x}} \\right) \\\\ &= \\sigma(x) \\cdot (1 - \\sigma(x)) \\end{align} TanH/ Hyperbolic Tangent Similar in advantages and disadvantages except it has the advantage of being zero centered , making it easier to models input that have strongly negative, neutral, and strongly positive values ReLU (Rectified Linear Unit) Pros: Computationally efficient - allows the network to converge quickly Non-linear - although it seems lik e a lienar function, its derivative allows for backpropagation Cons: The Dying ReLU problem - as inputs approach zero or are negative, the gradient becomes zero, so the network can't perform backpropagation and therefore cannot learn Leaky ReLU Pros: Prevents dying ReLU problem - the smallpositive slope in the negative area enables backpropagation for negative inputs Otherwise like ReLU Cons: Inconsistent Results - does't provide consistent predicitions for negative input values https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/","title":"Activation Functions"},{"location":"machine_learning/activation_functions/#activation-functions","text":"Mathematical equations that determine the output of a node/neuron in a neural network Determines whether each neuron should be activated based on the neuron's input and the specific activation function Normalize the output of a neuron to a range: [0,1] or [-1,1] Must be computationally efficient b/c they are calculated across thousands or millions of neurons for each pass Having a computationally efficient derivative is good too for backpropagation Can be as simple as a step function that turns the neuron output on or off (e.g. 0 or 1) Binary Step Function (Doesn't allow multi-value outputs so it can't do multi-class classification) Can be a trasnformation that maps input signals to output signals Linear activations functions collapse a neural network into one layer b/c a linear combination of linear functions is still a linear function (not very useful) Can't use backpropagation with linear activation functions b/c the derivative is a constant so its impossible to see the effect of changing weights Non-linear activation functions allow the neural network to learn more complex data like images, videos, and audio Allow backpropagation b/c the derivative function is related to the inputs Allow sacking of multiple hidden layers unlike linear activation functions Most all processes can be modeled by a neural network w/ nonlinear activation functions","title":"Activation Functions"},{"location":"machine_learning/activation_functions/#why-do-we-need-activation-functions","text":"Matrix multiplication and vector addition are simply linear transformations so in order to learn any nonlinear representations of data, we need to introduce a non-linear function or else the most complex classification boundary it can learn will be linear (e.g. line, plane, etc. ) which isn't that useful for interesting problems. In order to introduce non-linearity (otherwise matrix ops will result in only linear transformations), we apply non-linear activation functions pointwise to the results of the linear transformations at each layer.","title":"Why do we need activation functions?"},{"location":"machine_learning/activation_functions/#sigmoidlogistic","text":"\\sigma(x) = \\dfrac{1}{1 + e^{-x}} \\dfrac{d}{dx}\\sigma(x) = \\sigma(x)(1 - \\sigma(x)) Pros: Smooth Gradient prevents jumps in the output values Output vaues bound btw 0 and 1, normalizing the each neuron's output Clear predictions - any values above 2 or below -2 are very close to the edge of the curve Cons: Vanishing Gradient- almost no change in the prediction for very high and very low input values Can result in the network refusing to learn further or being too slow to reach an accurate prediction Outputs not zero centered Computationally expensive Detailed Derivation: \\begin{align}\\dfrac{d}{dx} \\sigma(x) &= \\dfrac{d}{dx} \\left[ \\dfrac{1}{1 + e^{-x}} \\right] \\\\ &= \\dfrac{d}{dx} \\left( 1 + \\mathrm{e}^{-x} \\right)^{-1} \\\\ &= -(1 + e^{-x})^{-2}(-e^{-x}) \\\\ &= \\dfrac{e^{-x}}{\\left(1 + e^{-x}\\right)^2} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{e^{-x}}{1 + e^{-x}} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{(1 + e^{-x}) - 1}{1 + e^{-x}} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( \\dfrac{1 + e^{-x}}{1 + e^{-x}} - \\dfrac{1}{1 + e^{-x}} \\right) \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( 1 - \\dfrac{1}{1 + e^{-x}} \\right) \\\\ &= \\sigma(x) \\cdot (1 - \\sigma(x)) \\end{align}","title":"Sigmoid/Logistic"},{"location":"machine_learning/activation_functions/#tanh-hyperbolic-tangent","text":"Similar in advantages and disadvantages except it has the advantage of being zero centered , making it easier to models input that have strongly negative, neutral, and strongly positive values","title":"TanH/ Hyperbolic Tangent"},{"location":"machine_learning/activation_functions/#relu-rectified-linear-unit","text":"Pros: Computationally efficient - allows the network to converge quickly Non-linear - although it seems lik e a lienar function, its derivative allows for backpropagation Cons: The Dying ReLU problem - as inputs approach zero or are negative, the gradient becomes zero, so the network can't perform backpropagation and therefore cannot learn","title":"ReLU (Rectified Linear Unit)"},{"location":"machine_learning/activation_functions/#leaky-relu","text":"Pros: Prevents dying ReLU problem - the smallpositive slope in the negative area enables backpropagation for negative inputs Otherwise like ReLU Cons: Inconsistent Results - does't provide consistent predicitions for negative input values https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/","title":"Leaky ReLU"},{"location":"machine_learning/map/","text":"Map of ML Supervised Learning Unsupervised Learning Auto-Encoders num input layers = num output layers but smaller num hidden layers. This forces the network to learn a compressed representation of the data. A simple autoencoder could take 100 dimensional data (100 input units) and represent it low dimensionally as 50 dimensions if using 50 hidden units. Relies on the input data having features (completely random noise would be tough to learn anything from) Mathematically, it can be thought of as trying to learn an approximation to the identity function so the output is similar to the input. Allows us to discover interesting structure about the data. Reinforcement Learning A control policy \\pi maps the continuous state vector x to a continuous action vector u (often a control vector). u = \\pi(x,t,\\theta) where \\theta is the set of parameters to be learned by the control policy and t is the specific timestep","title":"Map of ML"},{"location":"machine_learning/map/#map-of-ml","text":"","title":"Map of ML"},{"location":"machine_learning/map/#supervised-learning","text":"","title":"Supervised Learning"},{"location":"machine_learning/map/#unsupervised-learning","text":"","title":"Unsupervised Learning"},{"location":"machine_learning/map/#auto-encoders","text":"num input layers = num output layers but smaller num hidden layers. This forces the network to learn a compressed representation of the data. A simple autoencoder could take 100 dimensional data (100 input units) and represent it low dimensionally as 50 dimensions if using 50 hidden units. Relies on the input data having features (completely random noise would be tough to learn anything from) Mathematically, it can be thought of as trying to learn an approximation to the identity function so the output is similar to the input. Allows us to discover interesting structure about the data.","title":"Auto-Encoders"},{"location":"machine_learning/map/#reinforcement-learning","text":"A control policy \\pi maps the continuous state vector x to a continuous action vector u (often a control vector). u = \\pi(x,t,\\theta) where \\theta is the set of parameters to be learned by the control policy and t is the specific timestep","title":"Reinforcement Learning"},{"location":"machine_learning/reinforcement_learning/","text":"Reinforcement Learning - the science of learning to make decisions from interaction Differs from other machine learning paradigms in that: No supervision, only a reward signal Feedback can be delayed, not instantaneous Time matters Earlier decisions affect later interactions Sequential decision problems can be solved w/ RL RL is a framework for how to think about these problems as well as a set of algorithms that can be used to solved these problems Core Concepts: Environment Reward Signal External to the learning algorithm even if its internal to the learning system as a whole Agent containing: Agent state Policy Value function (probably) Model (optionally) Rewards A reward R_t is a scalar feedback signal Indicates how well agent is doing at step t The agent's job is to maximize cumulative reward G_t = R_{t+1} + R_{t+2} + ... RL is based on the reward hypothesis Any goal can be formalized as the outcome of maximizing a cumulative reward Dense- reward on every step Sparse- reward only when the desired event happens Can be negative or positive rewads (Carrot vs. Stick in helping train the algorithm) Values Value - the expected cumulative reward from state s States use the v function to find their value v(s) = E[G_t | S_t = s] where G_t = R_{t+1} + R_{t+2} + ... Goal is to maximize value by picking suitable actions Rewards and values define desirability of state or acton Returns and values can be defined recursively G_t = R_{t+1} + G_{t+1} Actions Goal: Select actions to maximize value Actions may have long term consequences meaning rewards may be delayed Might be better to sacrific immediate reward to gain more long-term reward Ex: Financial investments might take months to mature Blocking opponent moves might help winning chances many moves in the future Policy - a mapping from states to actions Action Values It is possible to condition the value on actions q(s,a) = E[G_t | S_t = s, A_t = a] State action pairs use the Q-function to find their value Agent Components Agent State Policy Value Function Model State Actions depend on the state of the agent Both agent and environment may have an internal state In the simplest case, there is only one shared state by the environemnt and agent Often there are multiple (sometimes infinite) different states The agent's state is usually different than the environment's state The agent might not know the full state of the environment Environment State The environment's internal state Usually not visible to the agent May contain lots of irrelevant information if visible Agent State A history is a sequence of observations, action, rewards H_t = O_0,A_0,R_1,O_1,...,O_{t-1},A_{t-1},R_t,O_t The agent state S_t can be constructed from the history and is considerd to be a function of the history S_{t+1} = f(S_t,A_t,R_{t+1},O_{t+1}) where f is a state update function Means the next agent state is dependent on the current agent state, the action taken, the reward, and the observation A simple update function can be concatinating the previous states together to create the current state (Done w/ the Atari game agents by cocatenating frames together since frames are the state) Actions depend on this state Agent state is typically much smaller than the environment state and the full history Fully Observable Environments When the agent sees the full environment state observation = environment state Agent state could just be the observation S_t = O_t = environment state Then the agent is in a Markov Decision Process Ex: Single player board games in which the player can see the entire board Smart Pong is an example of fully observable environment (VERIFY) Markov Decision Processes MDPs provide a useful mathematical framework for talking about many RL topics Easier to reason about than the full problem (long markovian) Limited b/c of markov assumption Def- A decision process is Markov if the future is independent of the past given the present H_t => S_t => H_{t+1} Once the current state ( S_t ) is known , the history ( H_t ) may be thrown away (useful for space consideration) The current state gives enough information for the next state to be predicted The environment state is typically Markov The history is Markov Partially Observable Environments The agent gets parial information A robot w/ camera vision doesn't have its absolute location A poker playing agent only observes public cards Now the observation is not Markov Formally this is a partially observable Markov decision process (POMDP) The environment state can still be markov, but the agent does not know it The state should contain enough information for good policies/value predictions Policy Defines the agent's behavior A map from agent state to action Deterministic Policy - function that outputs an action based on an input agent state Stochastic Policy - a probability of selecting each action in each state Value Function: The actual value function is the expected return V_\\pi(s)=E[G_t|S_t=s,\\pi] \u200b = E[R_{t+1}+\\gamma R_{t+2}+\\gamma^2R_{t+3}+... | S_t = s,\\pi] The return has a recursive form since G_t = R_{t+1}+\\gamma G_{t+1} Bellman Equation : V_\\pi (s) = E[R_{t+1} + \\gamma G_{t+1} | S_t = s,A_t \\sim \\pi (s)] where a \\sim \\pi (s) means action a is chosen by policy \\pi in state s Discount Factor ( \\gamma \\in [0,1] ) trades off importance of immediate vs. long-term rewards Closer to 1 means higher importane on immediate rewards (AKA discounting future rewards in favor of immediate rewards) The value depends on the policy Can be used to evaluate desirability of states Can be used to select btw actions","title":"Reinforcement Learning"},{"location":"machine_learning/reinforcement_learning/#core-concepts","text":"Environment Reward Signal External to the learning algorithm even if its internal to the learning system as a whole Agent containing: Agent state Policy Value function (probably) Model (optionally)","title":"Core Concepts:"},{"location":"machine_learning/reinforcement_learning/#rewards","text":"A reward R_t is a scalar feedback signal Indicates how well agent is doing at step t The agent's job is to maximize cumulative reward G_t = R_{t+1} + R_{t+2} + ... RL is based on the reward hypothesis Any goal can be formalized as the outcome of maximizing a cumulative reward Dense- reward on every step Sparse- reward only when the desired event happens Can be negative or positive rewads (Carrot vs. Stick in helping train the algorithm)","title":"Rewards"},{"location":"machine_learning/reinforcement_learning/#values","text":"Value - the expected cumulative reward from state s States use the v function to find their value v(s) = E[G_t | S_t = s] where G_t = R_{t+1} + R_{t+2} + ... Goal is to maximize value by picking suitable actions Rewards and values define desirability of state or acton Returns and values can be defined recursively G_t = R_{t+1} + G_{t+1}","title":"Values"},{"location":"machine_learning/reinforcement_learning/#actions","text":"Goal: Select actions to maximize value Actions may have long term consequences meaning rewards may be delayed Might be better to sacrific immediate reward to gain more long-term reward Ex: Financial investments might take months to mature Blocking opponent moves might help winning chances many moves in the future Policy - a mapping from states to actions","title":"Actions"},{"location":"machine_learning/reinforcement_learning/#action-values","text":"It is possible to condition the value on actions q(s,a) = E[G_t | S_t = s, A_t = a] State action pairs use the Q-function to find their value","title":"Action Values"},{"location":"machine_learning/reinforcement_learning/#agent-components","text":"Agent State Policy Value Function Model","title":"Agent Components"},{"location":"machine_learning/reinforcement_learning/#state","text":"Actions depend on the state of the agent Both agent and environment may have an internal state In the simplest case, there is only one shared state by the environemnt and agent Often there are multiple (sometimes infinite) different states The agent's state is usually different than the environment's state The agent might not know the full state of the environment","title":"State"},{"location":"machine_learning/reinforcement_learning/#environment-state","text":"The environment's internal state Usually not visible to the agent May contain lots of irrelevant information if visible","title":"Environment State"},{"location":"machine_learning/reinforcement_learning/#agent-state","text":"A history is a sequence of observations, action, rewards H_t = O_0,A_0,R_1,O_1,...,O_{t-1},A_{t-1},R_t,O_t The agent state S_t can be constructed from the history and is considerd to be a function of the history S_{t+1} = f(S_t,A_t,R_{t+1},O_{t+1}) where f is a state update function Means the next agent state is dependent on the current agent state, the action taken, the reward, and the observation A simple update function can be concatinating the previous states together to create the current state (Done w/ the Atari game agents by cocatenating frames together since frames are the state) Actions depend on this state Agent state is typically much smaller than the environment state and the full history","title":"Agent State"},{"location":"machine_learning/reinforcement_learning/#fully-observable-environments","text":"When the agent sees the full environment state observation = environment state Agent state could just be the observation S_t = O_t = environment state Then the agent is in a Markov Decision Process Ex: Single player board games in which the player can see the entire board Smart Pong is an example of fully observable environment (VERIFY)","title":"Fully Observable Environments"},{"location":"machine_learning/reinforcement_learning/#markov-decision-processes","text":"MDPs provide a useful mathematical framework for talking about many RL topics Easier to reason about than the full problem (long markovian) Limited b/c of markov assumption Def- A decision process is Markov if the future is independent of the past given the present H_t => S_t => H_{t+1} Once the current state ( S_t ) is known , the history ( H_t ) may be thrown away (useful for space consideration) The current state gives enough information for the next state to be predicted The environment state is typically Markov The history is Markov","title":"Markov Decision Processes"},{"location":"machine_learning/reinforcement_learning/#partially-observable-environments","text":"The agent gets parial information A robot w/ camera vision doesn't have its absolute location A poker playing agent only observes public cards Now the observation is not Markov Formally this is a partially observable Markov decision process (POMDP) The environment state can still be markov, but the agent does not know it The state should contain enough information for good policies/value predictions","title":"Partially Observable Environments"},{"location":"machine_learning/reinforcement_learning/#policy","text":"Defines the agent's behavior A map from agent state to action Deterministic Policy - function that outputs an action based on an input agent state Stochastic Policy - a probability of selecting each action in each state","title":"Policy"},{"location":"machine_learning/reinforcement_learning/#value-function","text":"The actual value function is the expected return V_\\pi(s)=E[G_t|S_t=s,\\pi] \u200b = E[R_{t+1}+\\gamma R_{t+2}+\\gamma^2R_{t+3}+... | S_t = s,\\pi] The return has a recursive form since G_t = R_{t+1}+\\gamma G_{t+1} Bellman Equation : V_\\pi (s) = E[R_{t+1} + \\gamma G_{t+1} | S_t = s,A_t \\sim \\pi (s)] where a \\sim \\pi (s) means action a is chosen by policy \\pi in state s Discount Factor ( \\gamma \\in [0,1] ) trades off importance of immediate vs. long-term rewards Closer to 1 means higher importane on immediate rewards (AKA discounting future rewards in favor of immediate rewards) The value depends on the policy Can be used to evaluate desirability of states Can be used to select btw actions","title":"Value Function:"},{"location":"machine_learning/supervised_learning/","text":"Supervised Learning Function Approximation Function approximation is a technique for estimating an unknown underyling function using observations from the domain and their resulting image. Given a dataset of inputs and outputs, we can assume there is an unkwnown underyling function that maps the inputs to the outputs. We can use supervised learning techniques to approximate this function. Supervised learning achieves this by minimizing the error between predicted outputs and expected outputs from the domain by changing its parameters (the weights of the network). Thus, simple feedforward neural networks can be better thought of as function approximators rather than models of how the brain works. The true function mapping inputs to outputs is referred to as the target function since it is the target of the learning process. Thus, intuitively we can better get a sense of what the function is by observing more input-output data points. The universal approximation theorem states a feedforward NN with a linear output layer and at least 1 hidden layer with a nonlinear activation function can approximate any function mapping between two finite dimensional spaces with any desired non-zero amount of error (provided enough hidden units). Below is an example of y=x^2 being approximated by a simple feedforward NN with 2 hidden layers of 10 nodes with relu activation trained on 100 observations: Resources Function Approximators","title":"Supervised Learning"},{"location":"machine_learning/supervised_learning/#supervised-learning","text":"","title":"Supervised Learning"},{"location":"machine_learning/supervised_learning/#function-approximation","text":"Function approximation is a technique for estimating an unknown underyling function using observations from the domain and their resulting image. Given a dataset of inputs and outputs, we can assume there is an unkwnown underyling function that maps the inputs to the outputs. We can use supervised learning techniques to approximate this function. Supervised learning achieves this by minimizing the error between predicted outputs and expected outputs from the domain by changing its parameters (the weights of the network). Thus, simple feedforward neural networks can be better thought of as function approximators rather than models of how the brain works. The true function mapping inputs to outputs is referred to as the target function since it is the target of the learning process. Thus, intuitively we can better get a sense of what the function is by observing more input-output data points. The universal approximation theorem states a feedforward NN with a linear output layer and at least 1 hidden layer with a nonlinear activation function can approximate any function mapping between two finite dimensional spaces with any desired non-zero amount of error (provided enough hidden units). Below is an example of y=x^2 being approximated by a simple feedforward NN with 2 hidden layers of 10 nodes with relu activation trained on 100 observations:","title":"Function Approximation"},{"location":"machine_learning/supervised_learning/#resources","text":"Function Approximators","title":"Resources"},{"location":"optimization/func_approx/","text":"Function Approximation","title":"Function Approximation"},{"location":"optimization/func_approx/#function-approximation","text":"","title":"Function Approximation"},{"location":"optimization/mpc/","text":"Model Predictive Control Relies on a dynamic model to make predictions for future values of the controlled variables of the system. It then solves a contrained optimization problem to calculate the optimal control action that minimizes the difference between the predicted controlled values and the desired/set control values. Main limitation: High computational cost since this optimization problem must be solved at each time step The computational cost increases even more as the time horizon over which to predict future control values increases. The computational cost becomes intractable to run realtime as the number of states and contraints grows (NN approximation could fix this) Explicit MPC (eMPC) and learning-based NN approaches have been proposed to overcome this Takes on many forms: one-norm, infinite norm, PWA model, tracking Explicit MPC Goal is to rewrite the optimization problem as a multi-parametric quadtratic programming (mpQP) This is possible for linear systems but cannot be extended to nonlinear systems An explicit solution to the mpQP has been demonstrated and it is of the form of a PWA function The PWA function defined over the state space will act as a fast prediction/lookup function for control policies given a input state meaning there is no need to solve the optimization problem online! This result was achieved by Bemporad et al. (2002) Resources: ( Brief mention ) Optimality: Not always feasible to run the optimization process to complete convergence for the most optimal control output (especially when running online) Therefore, optimality is sometimes relaxed to only require a sufficient decrease in the cost/objective function Then the optimization problem would be terminated at a feasible but sub-optimal stage Recursive Feasability: MPC controllers are not gauranteed to be stabilizing (linear quadratic control is gauranted to be stabilizing) Stability must be built in when defining the optimization problem The optimization problem\u2019s feasbility can be lost when the state is driven to a region in which the optimization problem has no solution typical approach to solve this is to append constrainst to the optimization problem to guarantee that loss of feasability cannot occur (that is what was done in the papers I have read so far) When this is done, an MPC controller is said to be recursively feasible meaning it cannot put the state in a place with no solution to the optimization problem Constructing an MPC controller with the theoretical (a-priori) guarantee of recursive feasability is not always desirable or easy to do Sometimes we might have a situation where given an MPC controller and the goal is to determine if it is recursively feasbile The goal is primarly to invalidate the controller by detecting problematic states where the recursive feasability is lost Second objective is to find certificates of guaranteed recursive feasability DEF: The MPC controller is recursively feasible if and only if for all ini- tially feasible x0 and for all optimal sequences of control inputs the MPC optimization problem remains feasible for all time. DEF: The MPC controller is strongly recursively feasible if and only if for all initially feasible x0 and for all sequences of feasible control inputs the MPC optimization problem remains feasible for all time. Resource (analysis tool for determining recursive feasability of an MPC controller)","title":"Model Predictive Control"},{"location":"optimization/mpc/#model-predictive-control","text":"Relies on a dynamic model to make predictions for future values of the controlled variables of the system. It then solves a contrained optimization problem to calculate the optimal control action that minimizes the difference between the predicted controlled values and the desired/set control values. Main limitation: High computational cost since this optimization problem must be solved at each time step The computational cost increases even more as the time horizon over which to predict future control values increases. The computational cost becomes intractable to run realtime as the number of states and contraints grows (NN approximation could fix this) Explicit MPC (eMPC) and learning-based NN approaches have been proposed to overcome this Takes on many forms: one-norm, infinite norm, PWA model, tracking","title":"Model Predictive Control"},{"location":"optimization/mpc/#explicit-mpc","text":"Goal is to rewrite the optimization problem as a multi-parametric quadtratic programming (mpQP) This is possible for linear systems but cannot be extended to nonlinear systems An explicit solution to the mpQP has been demonstrated and it is of the form of a PWA function The PWA function defined over the state space will act as a fast prediction/lookup function for control policies given a input state meaning there is no need to solve the optimization problem online! This result was achieved by Bemporad et al. (2002) Resources: ( Brief mention )","title":"Explicit MPC"},{"location":"optimization/mpc/#optimality","text":"Not always feasible to run the optimization process to complete convergence for the most optimal control output (especially when running online) Therefore, optimality is sometimes relaxed to only require a sufficient decrease in the cost/objective function Then the optimization problem would be terminated at a feasible but sub-optimal stage","title":"Optimality:"},{"location":"optimization/mpc/#recursive-feasability","text":"MPC controllers are not gauranteed to be stabilizing (linear quadratic control is gauranted to be stabilizing) Stability must be built in when defining the optimization problem The optimization problem\u2019s feasbility can be lost when the state is driven to a region in which the optimization problem has no solution typical approach to solve this is to append constrainst to the optimization problem to guarantee that loss of feasability cannot occur (that is what was done in the papers I have read so far) When this is done, an MPC controller is said to be recursively feasible meaning it cannot put the state in a place with no solution to the optimization problem Constructing an MPC controller with the theoretical (a-priori) guarantee of recursive feasability is not always desirable or easy to do Sometimes we might have a situation where given an MPC controller and the goal is to determine if it is recursively feasbile The goal is primarly to invalidate the controller by detecting problematic states where the recursive feasability is lost Second objective is to find certificates of guaranteed recursive feasability DEF: The MPC controller is recursively feasible if and only if for all ini- tially feasible x0 and for all optimal sequences of control inputs the MPC optimization problem remains feasible for all time. DEF: The MPC controller is strongly recursively feasible if and only if for all initially feasible x0 and for all sequences of feasible control inputs the MPC optimization problem remains feasible for all time. Resource (analysis tool for determining recursive feasability of an MPC controller)","title":"Recursive Feasability:"},{"location":"optimization/overview/","text":"Overview Optimization is a huge field with a tremendous number of applications. It is also intertwined in ML so that is why its include in the ML notebook. Here is where I will try to keep a working glossary of areas I am being introduced to with a brief overview of what they are. Being able to map out how many pieces of a field fit together greatly helps me when diving deeper into any one area. What is Optimization? Optimizing simply stands for finding the maximum or minimum of a function. This max or min is often subject to a series of constraints. The form of the function being optimized and the constraints being adhered to (e.g. linear, nonlinear, continuous, etc.) determines the type of problem and thus the technique used to solve them. Here is the mathematical definition of an optimization problem: \\min f_0(x) subject to the contraints: f_i(x) \\leq b_i Here is the textbook on convex optimization used for reference. Linear Programming (LP) Optimizing a linear objective/cost function subject to linear equality/inequality constraints Nonlinear Programming (NLP) Optimizing an objective/cost function over a set of real variables subject to system of equations called constraints (more general class that includes LP) Quadtratic Programming (QP) Methods for optimizing a quadtratic objective function subject to linear constraints Sequential Programming Sequential Quadtratic Programming (SQP) Involves solving a series of subproblems where each is a quadratic program Sequential Linear Quadtratic Programming (SLQP) Involves solving a linear program and equality constrained quadratic program at each step Dynamic Programming (DP) Involves breaking down the main optimization problem into simpler sub-problems. The Bellman Equation describes the relationship between the value of the larger problem and the values of the sub-problems Piece Wise Affine Functions (PWAs) Approximates a function using a series of lines in 2D, planes in 3D, & hyperplanes in higher dimensional spaces. While the picture above shows the PWA given a known function, PWAs are most often used when there is a set of datapoints representing the functions inputs and outputs. The task is then to construct a PWA from the data such that it best approximates the true function that is mapping the inputs to the outputs. Once you have a PWA made from your dataset, you can then make predictions on new inputs as to what the true function would map to.","title":"Overview"},{"location":"optimization/overview/#overview","text":"Optimization is a huge field with a tremendous number of applications. It is also intertwined in ML so that is why its include in the ML notebook. Here is where I will try to keep a working glossary of areas I am being introduced to with a brief overview of what they are. Being able to map out how many pieces of a field fit together greatly helps me when diving deeper into any one area.","title":"Overview"},{"location":"optimization/overview/#what-is-optimization","text":"Optimizing simply stands for finding the maximum or minimum of a function. This max or min is often subject to a series of constraints. The form of the function being optimized and the constraints being adhered to (e.g. linear, nonlinear, continuous, etc.) determines the type of problem and thus the technique used to solve them. Here is the mathematical definition of an optimization problem: \\min f_0(x) subject to the contraints: f_i(x) \\leq b_i Here is the textbook on convex optimization used for reference.","title":"What is Optimization?"},{"location":"optimization/overview/#linear-programming-lp","text":"Optimizing a linear objective/cost function subject to linear equality/inequality constraints","title":"Linear Programming (LP)"},{"location":"optimization/overview/#nonlinear-programming-nlp","text":"Optimizing an objective/cost function over a set of real variables subject to system of equations called constraints (more general class that includes LP)","title":"Nonlinear Programming (NLP)"},{"location":"optimization/overview/#quadtratic-programming-qp","text":"Methods for optimizing a quadtratic objective function subject to linear constraints","title":"Quadtratic Programming (QP)"},{"location":"optimization/overview/#sequential-programming","text":"","title":"Sequential Programming"},{"location":"optimization/overview/#sequential-quadtratic-programming-sqp","text":"Involves solving a series of subproblems where each is a quadratic program","title":"Sequential Quadtratic Programming (SQP)"},{"location":"optimization/overview/#sequential-linear-quadtratic-programming-slqp","text":"Involves solving a linear program and equality constrained quadratic program at each step","title":"Sequential Linear Quadtratic Programming (SLQP)"},{"location":"optimization/overview/#dynamic-programming-dp","text":"Involves breaking down the main optimization problem into simpler sub-problems. The Bellman Equation describes the relationship between the value of the larger problem and the values of the sub-problems","title":"Dynamic Programming (DP)"},{"location":"optimization/overview/#piece-wise-affine-functions-pwas","text":"Approximates a function using a series of lines in 2D, planes in 3D, & hyperplanes in higher dimensional spaces. While the picture above shows the PWA given a known function, PWAs are most often used when there is a set of datapoints representing the functions inputs and outputs. The task is then to construct a PWA from the data such that it best approximates the true function that is mapping the inputs to the outputs. Once you have a PWA made from your dataset, you can then make predictions on new inputs as to what the true function would map to.","title":"Piece Wise Affine Functions (PWAs)"}]}