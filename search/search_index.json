{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Machine Learning Notebook","title":"Home"},{"location":"#machine-learning-notebook","text":"","title":"Machine Learning Notebook"},{"location":"machine_learning/classification/","text":"Classification Perceptron Very early learning algorithm that performs simple linear classification between two classes Never converges if training data is not linearly separable z = w^Tx + b g(z) = <script type=\"math/tex; mode=display\">\\begin{cases}1 & z \\geq 0 \\\\ 0 & z < 0\\end{cases} Update rule: \\theta_j := \\theta_j + \\alpha (y^{(i)} - h_\\theta(x^{(i)})x_j^{(i)} Its predictions do not have meaningful probabilistic interpretations Cannot be derived as a maximum likelihood estimation algorithm Forms the basis for multilayer perceptrons (neural networks) Backpropagation is used to train Nonlinear activitaion functions that are part of the network need to be differentiable Perceptron vs Logistic Regression Logistic Regression Fisher Scoring- when the log likelihood function is optimizied using multi-dimensional Newton's method \\theta := \\theta - H^{-1} \\nabla_\\theta \\ell(\\theta) H_{ij} = \\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta_i \\partial \\theta_j} Faster convergence than batch gradient descent but one iteration can be more expensive due to finding and inverting hessian If number of params is small, and therefore hessian is small, then it is usually much faster overall CS 229 Notes Softmax Regression aka Multinomial Logistic Regression Generalization of logistic regression to C classes If C = 2 then softmax reduces to binary logistic regression Creates linear decision boundaries between classes Adding hidden layers with nonlinear activation functions is what allows NNs to learn nonlinear decision boundaries Applies the softmax activation function to a vector of real numbers to produce a vector of probabilities S(x) = \\frac{e^x}{\\sum_{i =1}^C e^{x_i}} S: \\mathbb{R}^n \\to \\mathbb{R}^n The sum of the output probabilites = 1 Applies the exponetial elementwise to a vector Temperature parameter t is a scalar that x is multiplied by before being passed to the softmax activation 0 < t < 1 moves the probabilities closer together t > 1 moves the probabilites further apart Uses cross entropy loss to train \\mathcal{L}(y, \\hat y) = - \\sum_{j=1}^C y_j \\log \\hat y_j assumes target y uses one-hot encoding (e.g. y^{(i)} = (0, 0, 1, 0)^T ) In order to minimize -\\log \\hat y we need to make \\hat y >> 0 J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(y^{(i)}, \\hat y^{(i)}) Stanford UFLDL DeepLearningAI Video on Softmax Regression Gaussian Discriminant Analysis Naive Bayes Features are discrete unlike the continue features in Gaussian Discriminant Analysis Ex: dictionary of words showing up in an email to classify whether it is spam mispelled words in spam messages to try to get past these dictionaries of possible spam values the feature vector of words that show up in the desired dictionary is considered a bernoulli event model a multinomial event model takes into account the structure of the sentence or how the words appear in order to help prevent stuffing an email with a bunch of hidden good words that help the email get past the spam filter using a bernoulli event model naive bayes is no longer valid the feature vector will now depend on email length laplace mothing based on dictionary size instead of the size of the feature vector Support Vector Machines Multiclass Classification To classify an input into one of k classes, there are two techniques that can be applied to any of the classification methods (logistic regression, perceptron, SVM) ML Mastery One-vs-All aka one-vs-rest For each i = 1, \\dots, k train a binary classifier to succesfully classify y = i . This yields a set of parameters \\theta^{(1)}, \\dots, \\theta^{(k)} To make predictions for x , select the class i that produces the highest value for h_\\theta^{(i)}(x) = (\\theta^{(i)})^Tx One-vs-One For each class i = 1, \\dots, k train a separate binary classifier to succesfully classify y = i against each other class Results in \\frac{k(k-1)}{2} Classifiers To make predictions for x : Each model may predict a class label for x and the class label with the most votes (most frequently occuring) amongst all the models is the predicted label Each model may predict a probability of a class membership of x , so then the probabilites for each class membership are summed up and the class with the highest probability is the predicted label. Example w/ 4 classes: \u2018 red ,\u2019 \u2018 blue ,\u2019 and \u2018 green ,\u2019 \u2018 yellow .\u2019 This would be divided into six binary classification problems: 1: red vs. blue 2: red vs. green 3: red vs. yellow 4: blue vs. green 5: blue vs. yellow 6: green vs. yellow","title":"Classification"},{"location":"machine_learning/classification/#classification","text":"","title":"Classification"},{"location":"machine_learning/classification/#perceptron","text":"Very early learning algorithm that performs simple linear classification between two classes Never converges if training data is not linearly separable z = w^Tx + b g(z) = <script type=\"math/tex; mode=display\">\\begin{cases}1 & z \\geq 0 \\\\ 0 & z < 0\\end{cases} Update rule: \\theta_j := \\theta_j + \\alpha (y^{(i)} - h_\\theta(x^{(i)})x_j^{(i)} Its predictions do not have meaningful probabilistic interpretations Cannot be derived as a maximum likelihood estimation algorithm Forms the basis for multilayer perceptrons (neural networks) Backpropagation is used to train Nonlinear activitaion functions that are part of the network need to be differentiable Perceptron vs Logistic Regression","title":"Perceptron"},{"location":"machine_learning/classification/#logistic-regression","text":"Fisher Scoring- when the log likelihood function is optimizied using multi-dimensional Newton's method \\theta := \\theta - H^{-1} \\nabla_\\theta \\ell(\\theta) H_{ij} = \\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta_i \\partial \\theta_j} Faster convergence than batch gradient descent but one iteration can be more expensive due to finding and inverting hessian If number of params is small, and therefore hessian is small, then it is usually much faster overall CS 229 Notes","title":"Logistic Regression"},{"location":"machine_learning/classification/#softmax-regression","text":"aka Multinomial Logistic Regression Generalization of logistic regression to C classes If C = 2 then softmax reduces to binary logistic regression Creates linear decision boundaries between classes Adding hidden layers with nonlinear activation functions is what allows NNs to learn nonlinear decision boundaries Applies the softmax activation function to a vector of real numbers to produce a vector of probabilities S(x) = \\frac{e^x}{\\sum_{i =1}^C e^{x_i}} S: \\mathbb{R}^n \\to \\mathbb{R}^n The sum of the output probabilites = 1 Applies the exponetial elementwise to a vector Temperature parameter t is a scalar that x is multiplied by before being passed to the softmax activation 0 < t < 1 moves the probabilities closer together t > 1 moves the probabilites further apart Uses cross entropy loss to train \\mathcal{L}(y, \\hat y) = - \\sum_{j=1}^C y_j \\log \\hat y_j assumes target y uses one-hot encoding (e.g. y^{(i)} = (0, 0, 1, 0)^T ) In order to minimize -\\log \\hat y we need to make \\hat y >> 0 J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(y^{(i)}, \\hat y^{(i)}) Stanford UFLDL DeepLearningAI Video on Softmax Regression","title":"Softmax Regression"},{"location":"machine_learning/classification/#gaussian-discriminant-analysis","text":"","title":"Gaussian Discriminant Analysis"},{"location":"machine_learning/classification/#naive-bayes","text":"Features are discrete unlike the continue features in Gaussian Discriminant Analysis Ex: dictionary of words showing up in an email to classify whether it is spam mispelled words in spam messages to try to get past these dictionaries of possible spam values the feature vector of words that show up in the desired dictionary is considered a bernoulli event model a multinomial event model takes into account the structure of the sentence or how the words appear in order to help prevent stuffing an email with a bunch of hidden good words that help the email get past the spam filter using a bernoulli event model naive bayes is no longer valid the feature vector will now depend on email length laplace mothing based on dictionary size instead of the size of the feature vector","title":"Naive Bayes"},{"location":"machine_learning/classification/#support-vector-machines","text":"","title":"Support Vector Machines"},{"location":"machine_learning/classification/#multiclass-classification","text":"To classify an input into one of k classes, there are two techniques that can be applied to any of the classification methods (logistic regression, perceptron, SVM) ML Mastery","title":"Multiclass Classification"},{"location":"machine_learning/classification/#one-vs-all","text":"aka one-vs-rest For each i = 1, \\dots, k train a binary classifier to succesfully classify y = i . This yields a set of parameters \\theta^{(1)}, \\dots, \\theta^{(k)} To make predictions for x , select the class i that produces the highest value for h_\\theta^{(i)}(x) = (\\theta^{(i)})^Tx","title":"One-vs-All"},{"location":"machine_learning/classification/#one-vs-one","text":"For each class i = 1, \\dots, k train a separate binary classifier to succesfully classify y = i against each other class Results in \\frac{k(k-1)}{2} Classifiers To make predictions for x : Each model may predict a class label for x and the class label with the most votes (most frequently occuring) amongst all the models is the predicted label Each model may predict a probability of a class membership of x , so then the probabilites for each class membership are summed up and the class with the highest probability is the predicted label. Example w/ 4 classes: \u2018 red ,\u2019 \u2018 blue ,\u2019 and \u2018 green ,\u2019 \u2018 yellow .\u2019 This would be divided into six binary classification problems: 1: red vs. blue 2: red vs. green 3: red vs. yellow 4: blue vs. green 5: blue vs. yellow 6: green vs. yellow","title":"One-vs-One"},{"location":"machine_learning/clustering/","text":"Clustering k-means Given a dataset \\{ x^{(1)}, \\cdots, x^{(m)}\\} where x^{(i)} \\in \\mathbb{R}^n Want to create k clusters CS 229 Notes algorithm randomly initialize cluster centroids \\mu_1, \\cdots, \\mu_k \\in \\mathbb{R}^n repeat until convergence for every i , set c^{(i)} := argmin_j ||x^{(i)} - \\mu_j||^2 assign each datapoint to closest cluster centroid for every j , \\mu_j = \\frac{\\sum 1(c^{(i)} = j) x^{(i)}}{\\sum 1(c^{(i)} = j)} move each centroid to the avg position of the datapoints assigned to it convergence guaranteed to converge Distortion Function: J(c, \\mu) = \\sum ||x^{(i)} - \\mu_{c^{(i)}}||^2 Sum of the squared distances between the training samples and their assigned cluster centroid k-means performs coordinate descent on J J is nonconvex so might get trapped in local min can run k-means multiple times with different intializations to escape local mins hierarchical","title":"Clustering"},{"location":"machine_learning/clustering/#clustering","text":"","title":"Clustering"},{"location":"machine_learning/clustering/#k-means","text":"Given a dataset \\{ x^{(1)}, \\cdots, x^{(m)}\\} where x^{(i)} \\in \\mathbb{R}^n Want to create k clusters CS 229 Notes","title":"k-means"},{"location":"machine_learning/clustering/#algorithm","text":"randomly initialize cluster centroids \\mu_1, \\cdots, \\mu_k \\in \\mathbb{R}^n repeat until convergence for every i , set c^{(i)} := argmin_j ||x^{(i)} - \\mu_j||^2 assign each datapoint to closest cluster centroid for every j , \\mu_j = \\frac{\\sum 1(c^{(i)} = j) x^{(i)}}{\\sum 1(c^{(i)} = j)} move each centroid to the avg position of the datapoints assigned to it","title":"algorithm"},{"location":"machine_learning/clustering/#convergence","text":"guaranteed to converge Distortion Function: J(c, \\mu) = \\sum ||x^{(i)} - \\mu_{c^{(i)}}||^2 Sum of the squared distances between the training samples and their assigned cluster centroid k-means performs coordinate descent on J J is nonconvex so might get trapped in local min can run k-means multiple times with different intializations to escape local mins","title":"convergence"},{"location":"machine_learning/clustering/#hierarchical","text":"","title":"hierarchical"},{"location":"machine_learning/dim_red/","text":"Dimensionality Reduction Similar to feature selection in that it reduces the complexity of features being learned different from feature selection in that it finds a smaller set of new features, each being a combination of input features, that contain the same information as the input features (compare to only keeping the most relevant features from the input features like in feature selection) Principal Components Analysis Can model data x \\in \\mathbb{R}^n as approximately lying in some k -dimension subspace where k << d Prior to running PCA, we preprocess the data by normalizing each feature to have mean 0 and variance 1 can be ommmitted if we know different features are on the same scale (e.g. pixels in a grayscale img) Want to compute the major axis of variation (i.e. the direction on which the data approximately lies) find the direction of maximum variance For a given unit vector u and a point x , the length of the porjection of x onto. u Is given by their dot product (e.g. x^T u ). \\textit{proj}_u(x) if x is a point in our dataset, then x^Tu is a distance from the origin we want to maximize: \\begin{align}\\frac{1}{n} \\sum_{i=1}^n (x^T u)^2 &= \\frac{1}{n}\\sum_{i=1}^n (x^T u)^T (x^T u) \\\\ &= \\frac{1}{n}\\sum_{i=1}^n u^T x x^T u \\\\ &= u^T (\\frac{ 1}{n} \\sum_{i=1}^n x x^T)u \\\\ &= u^T \\Sigma u \\end{align} subject to u^T u = 1 (e.g. ||u||_2 = 1 ) gaurantees all basis vectors are orthogonal Def of variance: \\frac{ 1}{n} \\sum_{i=1}^n x x^T = \\sigma^2 We can define the lagragian of this optimization problem as \\mathcal{L}(u, \\lambda) = u^T \\Sigma u - \\lambda (u^T u - 1) \\Sigma \\in \\mathbb{R}^{n \\times n} , u \\in \\mathbb{R}^n \\nabla_u \\mathcal{L}( \\cdot) = \\Sigma u - \\lambda u = 0 yields \\Sigma u = \\lambda u n solutions every time eigenvalues are the solutions \\lambda_1, \\cdots, \\lambda_n to \\det (\\lambda - \\Sigma) = 0 principal eigenvector take the largest |\\lambda_i| and find u_1 s.t. \\Sigma u_1 = \\lambda_1 u_1 repeat to find the 2nd, 3rd, etc. principal eigenvectors z is the new data poiint in reduced space: z = (x^T u_1, \\cdots, x^T u_k)^T where $k << n to cast z back to original space x_{approx} = (z_1 u_1) + \\cdots + z_k u_k = u_k z where each u_i \\in \\mathbb{R}^n","title":"Dimensionality Reduction"},{"location":"machine_learning/dim_red/#dimensionality-reduction","text":"Similar to feature selection in that it reduces the complexity of features being learned different from feature selection in that it finds a smaller set of new features, each being a combination of input features, that contain the same information as the input features (compare to only keeping the most relevant features from the input features like in feature selection)","title":"Dimensionality Reduction"},{"location":"machine_learning/dim_red/#principal-components-analysis","text":"Can model data x \\in \\mathbb{R}^n as approximately lying in some k -dimension subspace where k << d Prior to running PCA, we preprocess the data by normalizing each feature to have mean 0 and variance 1 can be ommmitted if we know different features are on the same scale (e.g. pixels in a grayscale img) Want to compute the major axis of variation (i.e. the direction on which the data approximately lies) find the direction of maximum variance For a given unit vector u and a point x , the length of the porjection of x onto. u Is given by their dot product (e.g. x^T u ). \\textit{proj}_u(x) if x is a point in our dataset, then x^Tu is a distance from the origin we want to maximize: \\begin{align}\\frac{1}{n} \\sum_{i=1}^n (x^T u)^2 &= \\frac{1}{n}\\sum_{i=1}^n (x^T u)^T (x^T u) \\\\ &= \\frac{1}{n}\\sum_{i=1}^n u^T x x^T u \\\\ &= u^T (\\frac{ 1}{n} \\sum_{i=1}^n x x^T)u \\\\ &= u^T \\Sigma u \\end{align} subject to u^T u = 1 (e.g. ||u||_2 = 1 ) gaurantees all basis vectors are orthogonal Def of variance: \\frac{ 1}{n} \\sum_{i=1}^n x x^T = \\sigma^2 We can define the lagragian of this optimization problem as \\mathcal{L}(u, \\lambda) = u^T \\Sigma u - \\lambda (u^T u - 1) \\Sigma \\in \\mathbb{R}^{n \\times n} , u \\in \\mathbb{R}^n \\nabla_u \\mathcal{L}( \\cdot) = \\Sigma u - \\lambda u = 0 yields \\Sigma u = \\lambda u n solutions every time eigenvalues are the solutions \\lambda_1, \\cdots, \\lambda_n to \\det (\\lambda - \\Sigma) = 0 principal eigenvector take the largest |\\lambda_i| and find u_1 s.t. \\Sigma u_1 = \\lambda_1 u_1 repeat to find the 2nd, 3rd, etc. principal eigenvectors z is the new data poiint in reduced space: z = (x^T u_1, \\cdots, x^T u_k)^T where $k << n to cast z back to original space x_{approx} = (z_1 u_1) + \\cdots + z_k u_k = u_k z where each u_i \\in \\mathbb{R}^n","title":"Principal Components Analysis"},{"location":"machine_learning/dts/","text":"Decision Trees Pros Easily explainable Highly interpretable cons low accuracy compared to other methods Cannot deal with additive structure (e,g. diagonal line separating classes) Ensemble Methods Used to improve the accuracy of predictions, especially with decision trees Training different types of algorithms and then averaging their results Training on multiple train sets from the population and then averaging their results Bagging (random forrests) Boosting (Adaboost, Xgboost) Bagging and boosting are most popular ensemble methods since the other two either require collecting more data (oftentimes infeasible) or using more algorithms Bagging Boosting Decreasing bias Additive CS 229 Lecture","title":"Decision Trees"},{"location":"machine_learning/dts/#decision-trees","text":"","title":"Decision Trees"},{"location":"machine_learning/dts/#pros","text":"Easily explainable Highly interpretable","title":"Pros"},{"location":"machine_learning/dts/#cons","text":"low accuracy compared to other methods Cannot deal with additive structure (e,g. diagonal line separating classes)","title":"cons"},{"location":"machine_learning/dts/#ensemble-methods","text":"Used to improve the accuracy of predictions, especially with decision trees Training different types of algorithms and then averaging their results Training on multiple train sets from the population and then averaging their results Bagging (random forrests) Boosting (Adaboost, Xgboost) Bagging and boosting are most popular ensemble methods since the other two either require collecting more data (oftentimes infeasible) or using more algorithms","title":"Ensemble Methods"},{"location":"machine_learning/dts/#bagging","text":"","title":"Bagging"},{"location":"machine_learning/dts/#boosting","text":"Decreasing bias Additive CS 229 Lecture","title":"Boosting"},{"location":"machine_learning/feature_scaling/","text":"Feature Scaling Datasets can have features with varying magnitudes, ranges, and units Pre-processing the features can improve model performance decision tree based algorithms are invariant to feature scaling due to making decisions at each node based on a single feature Use when optimizing using gradient desecent (e.g. regressions, NNs) feature value in the cost function derivative and gradient step will cause different step sizes for each feature due to the difference in feature ranges scale the data to ensure params are updated at the same rate for all features similarly scaled features can make it converge faster applying distance based algorithms (e.g. KNN, SVMs) using distance btw data points as a measure of similarity will bias the measure towards higher magnitude features scaling data ensures unbiased measure of similarity Normalization Values are shifted and rescaled to the range [0,1] aka min-max scaling X' = \\frac{X - X_{min}}{X_{max} -X_{min}} where X_{min} and X_{max} are the min and max values of the feature more affected by outliers from sklearn.preprocessing import MinMaxScaler scale = MinMaxScaler().fit(x_train) std_x_train = scale.transform(x_train) std_x_test = scale.transform(x_test) #can also compute the fit and then transform in one step std_x_train = MinMaxScaler().fit_transform(x_train) Use when data is not gaussianly distributed useful in algorithms that don't assume any distribution of the data (KNN, neural nets) Standardization Values are centered around the mean with a unit standard deviation new mean of feature is 0 and new standard deviation is 1 X' = \\frac{X - \\mu}{\\sigma} where \\mu is the mean of that feature's values and \\sigma is the standard deviation values are not restricted to a particular range more robust to outliers from sklearn.preprocessing import StandardScaler scale = StandardScaler().fit(x_train) std_x_train = scale.transform(x_train) std_x_test = scale.transform(x_test) Use when data is gaussianly distributed can be useful in some cases where this isn't true Notes Normalization vs. Standardization depends on the problem and the learning algorithm being used Best practice to scale the training data and use the same values to scale the test set Don't need to scale the target values normally Resources Analytics Vidhya Machine Learning Mastery Scikit-Learn Preprocessing Docs","title":"Feature Scaling"},{"location":"machine_learning/feature_scaling/#feature-scaling","text":"Datasets can have features with varying magnitudes, ranges, and units Pre-processing the features can improve model performance decision tree based algorithms are invariant to feature scaling due to making decisions at each node based on a single feature","title":"Feature Scaling"},{"location":"machine_learning/feature_scaling/#use-when","text":"optimizing using gradient desecent (e.g. regressions, NNs) feature value in the cost function derivative and gradient step will cause different step sizes for each feature due to the difference in feature ranges scale the data to ensure params are updated at the same rate for all features similarly scaled features can make it converge faster applying distance based algorithms (e.g. KNN, SVMs) using distance btw data points as a measure of similarity will bias the measure towards higher magnitude features scaling data ensures unbiased measure of similarity","title":"Use when"},{"location":"machine_learning/feature_scaling/#normalization","text":"Values are shifted and rescaled to the range [0,1] aka min-max scaling X' = \\frac{X - X_{min}}{X_{max} -X_{min}} where X_{min} and X_{max} are the min and max values of the feature more affected by outliers from sklearn.preprocessing import MinMaxScaler scale = MinMaxScaler().fit(x_train) std_x_train = scale.transform(x_train) std_x_test = scale.transform(x_test) #can also compute the fit and then transform in one step std_x_train = MinMaxScaler().fit_transform(x_train)","title":"Normalization"},{"location":"machine_learning/feature_scaling/#use-when_1","text":"data is not gaussianly distributed useful in algorithms that don't assume any distribution of the data (KNN, neural nets)","title":"Use when"},{"location":"machine_learning/feature_scaling/#standardization","text":"Values are centered around the mean with a unit standard deviation new mean of feature is 0 and new standard deviation is 1 X' = \\frac{X - \\mu}{\\sigma} where \\mu is the mean of that feature's values and \\sigma is the standard deviation values are not restricted to a particular range more robust to outliers from sklearn.preprocessing import StandardScaler scale = StandardScaler().fit(x_train) std_x_train = scale.transform(x_train) std_x_test = scale.transform(x_test)","title":"Standardization"},{"location":"machine_learning/feature_scaling/#use-when_2","text":"data is gaussianly distributed can be useful in some cases where this isn't true","title":"Use when"},{"location":"machine_learning/feature_scaling/#notes","text":"Normalization vs. Standardization depends on the problem and the learning algorithm being used Best practice to scale the training data and use the same values to scale the test set Don't need to scale the target values normally","title":"Notes"},{"location":"machine_learning/feature_scaling/#resources","text":"Analytics Vidhya Machine Learning Mastery Scikit-Learn Preprocessing Docs","title":"Resources"},{"location":"machine_learning/model_selection/","text":"Model Selection Learning algorithms have many different hyperparameters that can be tuned We want to select the hyperparameters that lead to the best models We utilize algorithms to perform an optimizing search in the space of possible models to determine the bset one Given a set of models M = \\{M_1, \\dots \\} Cross Validation Hold-Out (Simple) Hold-out cross validation (aka simple cross validation): Randomly slit the train set S into a S_{train} and S_{cv} (the hold-out cross validation set) Train each model M_i on S_{train} to get a hypothesis h_i Select h_i with the smallest error on the hold out cross validation set S_{cv} Can optionally retrain the best model h_i on the entire train set S Even then, we are still selecting best model based on 0.8m training examples rather than m Wastes some of the dataset on holding out- bad for situations with scare data (e.g. m =20 ) k-fold Holds out less data compared to simple cv Randomly split train set S into k disjoint subsets S_1, \\dots, S_k of m / k training examples each For each model M_i evaluate: for each of the k folds, hold out S_j and train the model on all the other folds test the hypothesis h_{ij} on S_j to get the error the estimated generalization error of model M_i is the average of the errors over each if the k tests Pick the model M_i with the lowest estimated generalization error and retrain the model on the entire training set S . The resulting hypothesis is our final output Typical choice is k =10 Computationally more expensive than hold-out since we need to train each model k times Leave-one-out Useful for when data is scare Perform k-fold cross validation where k = m in order to leave out as little data as possible each time Holds out one training example each time and then averages together the resulting m = k errors to get an estimate for the generalization error of a model Nested Cross Validation Used to do model hyperparameter optimization and model selection at the same time while avoiding overfitting the training data k-fold CV for model hyperparam optimization is nested inside th k-fold CV for model selection this prevents hyperparam search from overfitting the dataset since it is exposed to only a subset of the data provided by the outer CV procedure Very computationally expensive Feature Selection If n >> m , its best to reduce the number of features to learn from Reducing the number of input variables to produce simpler models simpler models = more explainable since decisions are made based on less features Reduces overfitting Reduces training time/compute time For n features, there are 2^n possible feature subsets Each of the n features can be included or excluded from the subset Feature selection can be viewed as a model selection problem over 2^n possible models For large n , its too expensive to compare all 2^n models explicitly Therefore, a heuristic search procedure is used to find a good feature subset Wrapper Procedure that wraps your learning algorithm Dependent on the algorithm you are selecting features for Different learning algorithms may have different best features from the same wrapper algorithm More computationally expensive O(n^2) calls to learning algorithm Forward Search Makes assumption that best answer is a small set of features, likely those that come first Order the features are specified in matters Unable to remove feature Define feature set F = \\empty to be the indices of the features that matter Repeat until |F| = n or |F| > max features: For each feature i = 1, \\dots, n : train a model using F \\cup \\{i\\} measure the generalization using some version of cross validation Set F to be the best feature subset found in the previous step Backward Search Makes assumption that best answer is a near full set of the features Spends most time checking large subsets Cannot reinstate removed features Define feature set F = \\{1, \\dots, n \\} Repeat until |F| = \\empty : For each feature i = 1, \\dots, n : train a model using F \\setminus \\{i\\} measure the generalization using some version of cross validation Set F to be the best feature subset found in the previous step Filter Computationally cheaper than wrapper method Compute a score on the data that measures the effectiveness Indepedent of the underyling learning algorithm in determining most important features Define some score S(j) which measure how informative feature j is for the output Choose the features with the largest scores This heuristic for selecting intital features can be used to select an initial set of features to use with forward and backward search Example heuristics: mutual infromation absolute value of the correletion Mutual Information Common scoring heuristic (especially for discrete-valued features) Can be expressed as Kullback-Leibler (KL) divergence which measures the difference between two probability distributions Resources Feature Selection (ML Mastery)","title":"Model Selection"},{"location":"machine_learning/model_selection/#model-selection","text":"Learning algorithms have many different hyperparameters that can be tuned We want to select the hyperparameters that lead to the best models We utilize algorithms to perform an optimizing search in the space of possible models to determine the bset one Given a set of models M = \\{M_1, \\dots \\}","title":"Model Selection"},{"location":"machine_learning/model_selection/#cross-validation","text":"","title":"Cross Validation"},{"location":"machine_learning/model_selection/#hold-out-simple","text":"Hold-out cross validation (aka simple cross validation): Randomly slit the train set S into a S_{train} and S_{cv} (the hold-out cross validation set) Train each model M_i on S_{train} to get a hypothesis h_i Select h_i with the smallest error on the hold out cross validation set S_{cv} Can optionally retrain the best model h_i on the entire train set S Even then, we are still selecting best model based on 0.8m training examples rather than m Wastes some of the dataset on holding out- bad for situations with scare data (e.g. m =20 )","title":"Hold-Out (Simple)"},{"location":"machine_learning/model_selection/#k-fold","text":"Holds out less data compared to simple cv Randomly split train set S into k disjoint subsets S_1, \\dots, S_k of m / k training examples each For each model M_i evaluate: for each of the k folds, hold out S_j and train the model on all the other folds test the hypothesis h_{ij} on S_j to get the error the estimated generalization error of model M_i is the average of the errors over each if the k tests Pick the model M_i with the lowest estimated generalization error and retrain the model on the entire training set S . The resulting hypothesis is our final output Typical choice is k =10 Computationally more expensive than hold-out since we need to train each model k times","title":"k-fold"},{"location":"machine_learning/model_selection/#leave-one-out","text":"Useful for when data is scare Perform k-fold cross validation where k = m in order to leave out as little data as possible each time Holds out one training example each time and then averages together the resulting m = k errors to get an estimate for the generalization error of a model","title":"Leave-one-out"},{"location":"machine_learning/model_selection/#nested-cross-validation","text":"Used to do model hyperparameter optimization and model selection at the same time while avoiding overfitting the training data k-fold CV for model hyperparam optimization is nested inside th k-fold CV for model selection this prevents hyperparam search from overfitting the dataset since it is exposed to only a subset of the data provided by the outer CV procedure Very computationally expensive","title":"Nested Cross Validation"},{"location":"machine_learning/model_selection/#feature-selection","text":"If n >> m , its best to reduce the number of features to learn from Reducing the number of input variables to produce simpler models simpler models = more explainable since decisions are made based on less features Reduces overfitting Reduces training time/compute time For n features, there are 2^n possible feature subsets Each of the n features can be included or excluded from the subset Feature selection can be viewed as a model selection problem over 2^n possible models For large n , its too expensive to compare all 2^n models explicitly Therefore, a heuristic search procedure is used to find a good feature subset","title":"Feature Selection"},{"location":"machine_learning/model_selection/#wrapper","text":"Procedure that wraps your learning algorithm Dependent on the algorithm you are selecting features for Different learning algorithms may have different best features from the same wrapper algorithm More computationally expensive O(n^2) calls to learning algorithm","title":"Wrapper"},{"location":"machine_learning/model_selection/#forward-search","text":"Makes assumption that best answer is a small set of features, likely those that come first Order the features are specified in matters Unable to remove feature Define feature set F = \\empty to be the indices of the features that matter Repeat until |F| = n or |F| > max features: For each feature i = 1, \\dots, n : train a model using F \\cup \\{i\\} measure the generalization using some version of cross validation Set F to be the best feature subset found in the previous step","title":"Forward Search"},{"location":"machine_learning/model_selection/#backward-search","text":"Makes assumption that best answer is a near full set of the features Spends most time checking large subsets Cannot reinstate removed features Define feature set F = \\{1, \\dots, n \\} Repeat until |F| = \\empty : For each feature i = 1, \\dots, n : train a model using F \\setminus \\{i\\} measure the generalization using some version of cross validation Set F to be the best feature subset found in the previous step","title":"Backward Search"},{"location":"machine_learning/model_selection/#filter","text":"Computationally cheaper than wrapper method Compute a score on the data that measures the effectiveness Indepedent of the underyling learning algorithm in determining most important features Define some score S(j) which measure how informative feature j is for the output Choose the features with the largest scores This heuristic for selecting intital features can be used to select an initial set of features to use with forward and backward search Example heuristics: mutual infromation absolute value of the correletion","title":"Filter"},{"location":"machine_learning/model_selection/#mutual-information","text":"Common scoring heuristic (especially for discrete-valued features) Can be expressed as Kullback-Leibler (KL) divergence which measures the difference between two probability distributions","title":"Mutual Information"},{"location":"machine_learning/model_selection/#resources","text":"Feature Selection (ML Mastery)","title":"Resources"},{"location":"machine_learning/optimization/","text":"Optimization Techniques Gradient Descent Optimization technique (aka steepest descent) Stochastic : cost function for single datapoint before doing a model parameter update take step for each datapoint in dataset may get closer to minimum faster than batch but might never converge and just keep oscillating around the minmum (usually approximations of the minimum are good enough) adaptive learning rate that decreases over time will ensure params converge and dont oscillate around the min preferred when the training set is large Batch : cost function over the entire dataset before doing a model parameter update scan entire dataset before taking step For convex cost functions, gradient descent always converges to the global minimum assuming learning rate is not too large MSE Cost is Convex Proof Momentum- takes average of previous gradients into account this results in less zigzagged trajectory throught the parameter space over the cost function reduces changes in directions that are not useful in moving towards the min since previous gradients will all share a common direction towards the minimizer while less useful directions will tend to cancel, thus we want to generally follow the most followed direction of previous gradients we don't update parameters directly, instead we update the velocity vector very low memory (store 1 previous gradient which encapsulates the effect of gradients before it) and low computational cost but greatly helps convergence v = Bv + (1 - B) gradient_w w = w - alpha * v Gradient Ascent- maximization of a concave function (e.g. maximizing distance btw separation hyperplan and observations) w_{n+1} = w_n + \\alpha \\nabla_w f(w) can apply gradient ascent on -f to get equivalent solution to gradient descent on f using positive log-likelihood: objective function is concave therefore we use gradient ascent using negative log-likelihood: objective function is convex therefore we use gradient descent GD Ascent Coordinate Descent Wiki Advanced Techniques Conjugate Gradient BFGS- iterative method for solving unconstrained nonlinear optimization problems determines descent direction by preconditioning gradient w/ curvature information curvature from a gradually improved approximation of the hessian of the loss function (obtained from gradient evalations via a secant method L-BFGS (limited memory version of BFGS)","title":"Optimization Techniques"},{"location":"machine_learning/optimization/#optimization-techniques","text":"","title":"Optimization Techniques"},{"location":"machine_learning/optimization/#gradient-descent","text":"Optimization technique (aka steepest descent) Stochastic : cost function for single datapoint before doing a model parameter update take step for each datapoint in dataset may get closer to minimum faster than batch but might never converge and just keep oscillating around the minmum (usually approximations of the minimum are good enough) adaptive learning rate that decreases over time will ensure params converge and dont oscillate around the min preferred when the training set is large Batch : cost function over the entire dataset before doing a model parameter update scan entire dataset before taking step For convex cost functions, gradient descent always converges to the global minimum assuming learning rate is not too large MSE Cost is Convex Proof Momentum- takes average of previous gradients into account this results in less zigzagged trajectory throught the parameter space over the cost function reduces changes in directions that are not useful in moving towards the min since previous gradients will all share a common direction towards the minimizer while less useful directions will tend to cancel, thus we want to generally follow the most followed direction of previous gradients we don't update parameters directly, instead we update the velocity vector very low memory (store 1 previous gradient which encapsulates the effect of gradients before it) and low computational cost but greatly helps convergence v = Bv + (1 - B) gradient_w w = w - alpha * v Gradient Ascent- maximization of a concave function (e.g. maximizing distance btw separation hyperplan and observations) w_{n+1} = w_n + \\alpha \\nabla_w f(w) can apply gradient ascent on -f to get equivalent solution to gradient descent on f using positive log-likelihood: objective function is concave therefore we use gradient ascent using negative log-likelihood: objective function is convex therefore we use gradient descent GD Ascent","title":"Gradient Descent"},{"location":"machine_learning/optimization/#coordinate-descent","text":"Wiki","title":"Coordinate Descent"},{"location":"machine_learning/optimization/#advanced-techniques","text":"Conjugate Gradient BFGS- iterative method for solving unconstrained nonlinear optimization problems determines descent direction by preconditioning gradient w/ curvature information curvature from a gradually improved approximation of the hessian of the loss function (obtained from gradient evalations via a secant method L-BFGS (limited memory version of BFGS)","title":"Advanced Techniques"},{"location":"machine_learning/pca/","text":"Singular Value Decomposition Allows us to decompose a data matrix X \\in \\mathbb{R}^{n \\times m} into matrices U \\Sigma V^T . U and V are orothonormal matrices (unitary so UU^T = U^TU = I_{n \\times n} ) Columns of U are \"eigen\" and give us a basis (that retains the variance of X ) that we can represent the column vectors in X in \\Sigma is a diagonal matrix ( \\sigma_1 \\geq \\cdots \\geq \\sigma_m \\geq 0 ). X - each column is a sample U - left singular vectors V - right singular vectors. V^T columns give the mixture of the basis columns in U needed to get the corresponding columns in the data matrix X \\Sigma - singular values (sigmas are ordered by importance) Guaranted to exist and be unique Matlab: [u, s, v] = svd(x); Resources SVD Overview Video Principal Component Analysis Common dimensionality reduction technique Background Math Refresher Variance & Covariance Variance measures the spread of the data variance = sum of (dist from mean)^2 / n Var(X) = \\sigma_X^2 = \\frac{\\sum_{i=1}^n ( x_i - \\mu_X)^2}{n-1} (this is sample variance, population variance has n ) Std Deviation is the square root of variance \\sigma_X = \\sqrt{\\sigma_X^2} For data x \\in \\mathbb{R}^n , you can measure the variance along each dimension by calculating the variance of the components in that dimenson However, variance does not show the whole structure of how multi-dimensional data is distributed so we also use covariance to describe multidimensional data Cov(X,Y) = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])] Subtracting by the expected values of X and Y centers the data at (0,0) Can be interpreted as taking the avg of the product of coordinates We define a covariance matrix \\Sigma = <script type=\"math/tex; mode=display\">\\begin{bmatrix} cov(X,X) & cov(X,Y) \\\\cov(Y,X) & cov(Y,Y) \\\\ \\end{bmatrix} = \\begin{bmatrix} var(X) & cov(X,Y) \\\\cov(Y,X) & var(Y) \\\\ \\end{bmatrix} Symmetric since property of covariance is that cov(X,Y) = cov(Y,X) can be n \\times n where covariance of each dimension with itself for a zero-mean matrix X , we \\Sigma = \\frac{1}{n-1} XX^T Eigenvectors & Eigenvalues Eigenvectors of a matrix/linear transform are vectors who maintain the same direction before and after the transformation Eigenvalue is the scalar amount eigenvectors are stretched by imaginary eigenvalues indicate rotation Av = \\lambda v eigenvectors of A - direction of stretch of LT from A eigenvalues of A - magnitude/amt of stretch of LT from A Symmetric matrices have orthogonal eigenvectors and real eigenvalues covariance matrices are always symmetric Tieing them Together We can view the covariance matrix as a linear transformation The eigenvectors of \\Sigma indicate the directions of the variance of the data The eigenvalues of \\Sigma indicate the magnitude of the variance of the data along the direction of the eigenvectors Larger eigenvalues will indicate more variance is captured along the direction of the corresponding eigenvector Covariance Perspective Calculate the covariance matrix \\Sigma of the data Find the eigenvalues and corresponding eigenvectors of the covariance matrix Select the eigenvectors with the largest eigenvalues to be the basis of a smaller subspace to represent the data Eigenvectors w/ largest eigenvalues represent the directions in which the most variance of the data is capture In order to accurately represent the data in lower dimensional space, we want to preserve as much of the original variance as possible Can select a new basis based on desired dimension of the subspace or based on how much variance you want preserved (e.g. 80%) sum of eigenvalues of current selected eigenvectors / sum of all eigenvalues = variance preserved in current subspace sum of eigenvalues should equal the sum along the diagonal of the covariance matrix (sum of variability in dataset) SVD Perspective Statistical Interpretation of SVD Gives us a heirirachial coordiante system (based on data) want to uncover dominant combinations of features that describe the data as much as possible X - each row is a sample assumption of gaussian distribution for the data compute mean row and make a giant mean matrix X' out of it that is the same size as X subtract the mean from the data matrix to get the mean centered matrix \\Beta = X - X' centers data at the origin Covariance matrix ( \\Sigma in SVD) of rows of \\Beta : C = \\Beta^T \\Beta Takeaway: Can compute principal components and loadings from the SVD of your mean subtracted data principal components: T = U \\Sigma Optimization Perspective We want compute the major axis of variation u of a mean normalized dataset \\max_u \\frac{1}{n} \\sum_{i=1}^n (x^{(i)^T} u)^2 subject to ||u||_2 = 1 x^Tu is the distance/variation from the origin (assumes the data's mean is at the origin) so we want to maximize it for all samples, subject to the constraint of u being a unit vector the problem expands to: \\max_u \\frac{1}{n} u^T \\Sigma u subject to u^Tu = 1 Can be solved using the method of lagrange multiplier to yield \\Sigma u = \\lambda u This means the top k eigenvectors of \\Sigma \\in \\mathbb{R}^{n \\times n} are the principal components that form a new orthogonal basis for the data in \\mathbb{R}^k Given x \\in \\mathbb{R}^n where n >> k , we can represent it in \\mathbb{R}^k using the k principal components u_i \\in \\mathbb{R}^n <script type=\"math/tex; mode=display\">\\begin{bmatrix} u_1^Tx \\\\ \\vdots \\\\ u_k^T x\\end{bmatrix} \\in \\mathbb{R}^k Applications Noise Reduction Image Compression Resources PCA using SVD Video PCA Intuition Video Covariance Matrix Relationship btw SVD & PCA Tutorial on PCA CS 229 Notes","title":"PCA"},{"location":"machine_learning/pca/#singular-value-decomposition","text":"Allows us to decompose a data matrix X \\in \\mathbb{R}^{n \\times m} into matrices U \\Sigma V^T . U and V are orothonormal matrices (unitary so UU^T = U^TU = I_{n \\times n} ) Columns of U are \"eigen\" and give us a basis (that retains the variance of X ) that we can represent the column vectors in X in \\Sigma is a diagonal matrix ( \\sigma_1 \\geq \\cdots \\geq \\sigma_m \\geq 0 ). X - each column is a sample U - left singular vectors V - right singular vectors. V^T columns give the mixture of the basis columns in U needed to get the corresponding columns in the data matrix X \\Sigma - singular values (sigmas are ordered by importance) Guaranted to exist and be unique Matlab: [u, s, v] = svd(x);","title":"Singular Value Decomposition"},{"location":"machine_learning/pca/#resources","text":"SVD Overview Video","title":"Resources"},{"location":"machine_learning/pca/#principal-component-analysis","text":"Common dimensionality reduction technique","title":"Principal Component Analysis"},{"location":"machine_learning/pca/#background-math-refresher","text":"","title":"Background Math Refresher"},{"location":"machine_learning/pca/#variance-covariance","text":"Variance measures the spread of the data variance = sum of (dist from mean)^2 / n Var(X) = \\sigma_X^2 = \\frac{\\sum_{i=1}^n ( x_i - \\mu_X)^2}{n-1} (this is sample variance, population variance has n ) Std Deviation is the square root of variance \\sigma_X = \\sqrt{\\sigma_X^2} For data x \\in \\mathbb{R}^n , you can measure the variance along each dimension by calculating the variance of the components in that dimenson However, variance does not show the whole structure of how multi-dimensional data is distributed so we also use covariance to describe multidimensional data Cov(X,Y) = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])] Subtracting by the expected values of X and Y centers the data at (0,0) Can be interpreted as taking the avg of the product of coordinates We define a covariance matrix \\Sigma = <script type=\"math/tex; mode=display\">\\begin{bmatrix} cov(X,X) & cov(X,Y) \\\\cov(Y,X) & cov(Y,Y) \\\\ \\end{bmatrix} = \\begin{bmatrix} var(X) & cov(X,Y) \\\\cov(Y,X) & var(Y) \\\\ \\end{bmatrix} Symmetric since property of covariance is that cov(X,Y) = cov(Y,X) can be n \\times n where covariance of each dimension with itself for a zero-mean matrix X , we \\Sigma = \\frac{1}{n-1} XX^T","title":"Variance &amp; Covariance"},{"location":"machine_learning/pca/#eigenvectors-eigenvalues","text":"Eigenvectors of a matrix/linear transform are vectors who maintain the same direction before and after the transformation Eigenvalue is the scalar amount eigenvectors are stretched by imaginary eigenvalues indicate rotation Av = \\lambda v eigenvectors of A - direction of stretch of LT from A eigenvalues of A - magnitude/amt of stretch of LT from A Symmetric matrices have orthogonal eigenvectors and real eigenvalues covariance matrices are always symmetric","title":"Eigenvectors &amp; Eigenvalues"},{"location":"machine_learning/pca/#tieing-them-together","text":"We can view the covariance matrix as a linear transformation The eigenvectors of \\Sigma indicate the directions of the variance of the data The eigenvalues of \\Sigma indicate the magnitude of the variance of the data along the direction of the eigenvectors Larger eigenvalues will indicate more variance is captured along the direction of the corresponding eigenvector","title":"Tieing them Together"},{"location":"machine_learning/pca/#covariance-perspective","text":"Calculate the covariance matrix \\Sigma of the data Find the eigenvalues and corresponding eigenvectors of the covariance matrix Select the eigenvectors with the largest eigenvalues to be the basis of a smaller subspace to represent the data Eigenvectors w/ largest eigenvalues represent the directions in which the most variance of the data is capture In order to accurately represent the data in lower dimensional space, we want to preserve as much of the original variance as possible Can select a new basis based on desired dimension of the subspace or based on how much variance you want preserved (e.g. 80%) sum of eigenvalues of current selected eigenvectors / sum of all eigenvalues = variance preserved in current subspace sum of eigenvalues should equal the sum along the diagonal of the covariance matrix (sum of variability in dataset)","title":"Covariance Perspective"},{"location":"machine_learning/pca/#svd-perspective","text":"Statistical Interpretation of SVD Gives us a heirirachial coordiante system (based on data) want to uncover dominant combinations of features that describe the data as much as possible X - each row is a sample assumption of gaussian distribution for the data compute mean row and make a giant mean matrix X' out of it that is the same size as X subtract the mean from the data matrix to get the mean centered matrix \\Beta = X - X' centers data at the origin Covariance matrix ( \\Sigma in SVD) of rows of \\Beta : C = \\Beta^T \\Beta Takeaway: Can compute principal components and loadings from the SVD of your mean subtracted data principal components: T = U \\Sigma","title":"SVD Perspective"},{"location":"machine_learning/pca/#optimization-perspective","text":"We want compute the major axis of variation u of a mean normalized dataset \\max_u \\frac{1}{n} \\sum_{i=1}^n (x^{(i)^T} u)^2 subject to ||u||_2 = 1 x^Tu is the distance/variation from the origin (assumes the data's mean is at the origin) so we want to maximize it for all samples, subject to the constraint of u being a unit vector the problem expands to: \\max_u \\frac{1}{n} u^T \\Sigma u subject to u^Tu = 1 Can be solved using the method of lagrange multiplier to yield \\Sigma u = \\lambda u This means the top k eigenvectors of \\Sigma \\in \\mathbb{R}^{n \\times n} are the principal components that form a new orthogonal basis for the data in \\mathbb{R}^k Given x \\in \\mathbb{R}^n where n >> k , we can represent it in \\mathbb{R}^k using the k principal components u_i \\in \\mathbb{R}^n <script type=\"math/tex; mode=display\">\\begin{bmatrix} u_1^Tx \\\\ \\vdots \\\\ u_k^T x\\end{bmatrix} \\in \\mathbb{R}^k","title":"Optimization Perspective"},{"location":"machine_learning/pca/#applications","text":"Noise Reduction Image Compression","title":"Applications"},{"location":"machine_learning/pca/#resources_1","text":"PCA using SVD Video PCA Intuition Video Covariance Matrix Relationship btw SVD & PCA Tutorial on PCA CS 229 Notes","title":"Resources"},{"location":"machine_learning/regression/","text":"Regression Simple Linear Regression Probabilistic Interpretation Multiple Linear Regression Polynomial Regression Scikit Learn Provides two approaches LinearRegression object- uses ordinary least squares solver from scipy to compite the closed form solution If enough memory for the matrices and inversions, this method is faster and easier SGDRegressor object- generic implementation of stochastic gradient descent so must set loss to L2 for linear regression penality to none for linear regression or L2 for ridge regression (this is the regularization mode) behaves better if loss function can be decomposed into additive terms Locally Weighted Linear Regression For a given query point x \\in \\mathbb{R}^n , we compute the weights of all other training points and use these weights to compute the optimal parameters \\theta \\in \\mathbb{R}^n . Each training sample has a weight w^{(i)} = \\exp (-\\frac{(x^{(i)} - x)^2}{2 \\tau^2}) The weights for the each training sample are placed along the diagonal of a matrix of zeros W \\in \\mathbb{R}^{m \\times m} Optimal parameters can be calculated using the closed form equation for ordinary least squares: \\theta = (X^T W X )^{-1} X^T Wy Must be recomputed for each new query point since the weights change for each query point (computationally expensive) h_\\theta(x) = \\theta^T x = x^T \\theta where x \\in \\mathbb{R}^n Note that the intercept term of x_0 = 1 is not used in LWLR \\tau is the bandwith parameter Increasing \\tau gives more equal weighting to all training samples, resulting in more underfitting Decreasing \\tau gives more weighting to nearby training samples, resulting in more overfitting","title":"Regression"},{"location":"machine_learning/regression/#regression","text":"","title":"Regression"},{"location":"machine_learning/regression/#simple-linear-regression","text":"","title":"Simple Linear Regression"},{"location":"machine_learning/regression/#probabilistic-interpretation","text":"","title":"Probabilistic Interpretation"},{"location":"machine_learning/regression/#multiple-linear-regression","text":"","title":"Multiple Linear Regression"},{"location":"machine_learning/regression/#polynomial-regression","text":"","title":"Polynomial Regression"},{"location":"machine_learning/regression/#scikit-learn","text":"Provides two approaches LinearRegression object- uses ordinary least squares solver from scipy to compite the closed form solution If enough memory for the matrices and inversions, this method is faster and easier SGDRegressor object- generic implementation of stochastic gradient descent so must set loss to L2 for linear regression penality to none for linear regression or L2 for ridge regression (this is the regularization mode) behaves better if loss function can be decomposed into additive terms","title":"Scikit Learn"},{"location":"machine_learning/regression/#locally-weighted-linear-regression","text":"For a given query point x \\in \\mathbb{R}^n , we compute the weights of all other training points and use these weights to compute the optimal parameters \\theta \\in \\mathbb{R}^n . Each training sample has a weight w^{(i)} = \\exp (-\\frac{(x^{(i)} - x)^2}{2 \\tau^2}) The weights for the each training sample are placed along the diagonal of a matrix of zeros W \\in \\mathbb{R}^{m \\times m} Optimal parameters can be calculated using the closed form equation for ordinary least squares: \\theta = (X^T W X )^{-1} X^T Wy Must be recomputed for each new query point since the weights change for each query point (computationally expensive) h_\\theta(x) = \\theta^T x = x^T \\theta where x \\in \\mathbb{R}^n Note that the intercept term of x_0 = 1 is not used in LWLR \\tau is the bandwith parameter Increasing \\tau gives more equal weighting to all training samples, resulting in more underfitting Decreasing \\tau gives more weighting to nearby training samples, resulting in more overfitting","title":"Locally Weighted Linear Regression"},{"location":"machine_learning/regularization/","text":"Regularization Coursera ML High Bias (underfitting) High variance (overfitting) As \\lambda \\to 0 we tend to overfit the data Larger \\lambda penalizes large weights more Linear Regression Logistic Regression","title":"Regularization"},{"location":"machine_learning/regularization/#regularization","text":"Coursera ML High Bias (underfitting) High variance (overfitting) As \\lambda \\to 0 we tend to overfit the data Larger \\lambda penalizes large weights more","title":"Regularization"},{"location":"machine_learning/regularization/#linear-regression","text":"","title":"Linear Regression"},{"location":"machine_learning/regularization/#logistic-regression","text":"","title":"Logistic Regression"},{"location":"machine_learning/reinforcement_learning/","text":"Reinforcement Learning The science of learning to make decisions from interaction Differs from other machine learning paradigms in that: No supervision, only a reward signal Feedback can be delayed, not instantaneous Time matters Earlier decisions affect later interactions Sequential decision problems can be solved w/ RL RL is a framework for how to think about these problems as well as a set of algorithms that can be used to solved these problems Core Concepts: Environment Reward Signal External to the learning algorithm even if its internal to the learning system as a whole Agent containing: Agent state Policy Value function (probably) Model (optionally) Rewards A reward R_t is a scalar feedback signal Indicates how well agent is doing at step t The agent's job is to maximize cumulative reward G_t = R_{t+1} + R_{t+2} + ... RL is based on the reward hypothesis Any goal can be formalized as the outcome of maximizing a cumulative reward Dense- reward on every step Sparse- reward only when the desired event happens Can be negative or positive rewads (Carrot vs. Stick in helping train the algorithm) Values Value - the expected cumulative reward from state s States use the v function to find their value v(s) = E[G_t | S_t = s] where G_t = R_{t+1} + R_{t+2} + ... Goal is to maximize value by picking suitable actions Rewards and values define desirability of state or acton Returns and values can be defined recursively G_t = R_{t+1} + G_{t+1} Actions Goal: Select actions to maximize value Actions may have long term consequences meaning rewards may be delayed Might be better to sacrific immediate reward to gain more long-term reward Ex: Financial investments might take months to mature Blocking opponent moves might help winning chances many moves in the future Policy - a mapping from states to actions Action Values It is possible to condition the value on actions q(s,a) = E[G_t | S_t = s, A_t = a] State action pairs use the Q-function to find their value Agent Components Agent State Policy Value Function Model State Actions depend on the state of the agent Both agent and environment may have an internal state In the simplest case, there is only one shared state by the environemnt and agent Often there are multiple (sometimes infinite) different states The agent's state is usually different than the environment's state The agent might not know the full state of the environment Environment State The environment's internal state Usually not visible to the agent May contain lots of irrelevant information if visible Agent State A history is a sequence of observations, action, rewards H_t = O_0,A_0,R_1,O_1,...,O_{t-1},A_{t-1},R_t,O_t The agent state S_t can be constructed from the history and is considerd to be a function of the history S_{t+1} = f(S_t,A_t,R_{t+1},O_{t+1}) where f is a state update function Means the next agent state is dependent on the current agent state, the action taken, the reward, and the observation A simple update function can be concatinating the previous states together to create the current state (Done w/ the Atari game agents by cocatenating frames together since frames are the state) Actions depend on this state Agent state is typically much smaller than the environment state and the full history Fully Observable Environments When the agent sees the full environment state observation = environment state Agent state could just be the observation S_t = O_t = environment state Then the agent is in a Markov Decision Process Ex: Single player board games in which the player can see the entire board Smart Pong is an example of fully observable environment (VERIFY) Markov Decision Processes MDPs provide a useful mathematical framework for talking about many RL topics Easier to reason about than the full problem (long markovian) Limited b/c of markov assumption Def- A decision process is Markov if the future is independent of the past given the present H_t => S_t => H_{t+1} Once the current state ( S_t ) is known , the history ( H_t ) may be thrown away (useful for space consideration) The current state gives enough information for the next state to be predicted The environment state is typically Markov The history is Markov Partially Observable Environments The agent gets parial information A robot w/ camera vision doesn't have its absolute location A poker playing agent only observes public cards Now the observation is not Markov Formally this is a partially observable Markov decision process (POMDP) The environment state can still be markov, but the agent does not know it The state should contain enough information for good policies/value predictions Policy Defines the agent's behavior A map from agent state to action Deterministic Policy - function that outputs an action based on an input agent state Stochastic Policy - a probability of selecting each action in each state Value Function: The actual value function is the expected return V_\\pi(s)=E[G_t|S_t=s,\\pi] = E[R_{t+1}+\\gamma R_{t+2}+\\gamma^2R_{t+3}+... | S_t = s,\\pi] The return has a recursive form since G_t = R_{t+1}+\\gamma G_{t+1} Bellman Equation : V_\\pi (s) = E[R_{t+1} + \\gamma G_{t+1} | S_t = s,A_t \\sim \\pi (s)] where a \\sim \\pi (s) means action a is chosen by policy \\pi in state s Discount Factor ( \\gamma \\in [0,1] ) trades off importance of immediate vs. long-term rewards Closer to 1 means higher importane on immediate rewards (AKA discounting future rewards in favor of immediate rewards) The value depends on the policy Can be used to evaluate desirability of states Can be used to select btw actions","title":"Reinforcement Learning"},{"location":"machine_learning/reinforcement_learning/#reinforcement-learning","text":"The science of learning to make decisions from interaction Differs from other machine learning paradigms in that: No supervision, only a reward signal Feedback can be delayed, not instantaneous Time matters Earlier decisions affect later interactions Sequential decision problems can be solved w/ RL RL is a framework for how to think about these problems as well as a set of algorithms that can be used to solved these problems","title":"Reinforcement Learning"},{"location":"machine_learning/reinforcement_learning/#core-concepts","text":"Environment Reward Signal External to the learning algorithm even if its internal to the learning system as a whole Agent containing: Agent state Policy Value function (probably) Model (optionally)","title":"Core Concepts:"},{"location":"machine_learning/reinforcement_learning/#rewards","text":"A reward R_t is a scalar feedback signal Indicates how well agent is doing at step t The agent's job is to maximize cumulative reward G_t = R_{t+1} + R_{t+2} + ... RL is based on the reward hypothesis Any goal can be formalized as the outcome of maximizing a cumulative reward Dense- reward on every step Sparse- reward only when the desired event happens Can be negative or positive rewads (Carrot vs. Stick in helping train the algorithm)","title":"Rewards"},{"location":"machine_learning/reinforcement_learning/#values","text":"Value - the expected cumulative reward from state s States use the v function to find their value v(s) = E[G_t | S_t = s] where G_t = R_{t+1} + R_{t+2} + ... Goal is to maximize value by picking suitable actions Rewards and values define desirability of state or acton Returns and values can be defined recursively G_t = R_{t+1} + G_{t+1}","title":"Values"},{"location":"machine_learning/reinforcement_learning/#actions","text":"Goal: Select actions to maximize value Actions may have long term consequences meaning rewards may be delayed Might be better to sacrific immediate reward to gain more long-term reward Ex: Financial investments might take months to mature Blocking opponent moves might help winning chances many moves in the future Policy - a mapping from states to actions","title":"Actions"},{"location":"machine_learning/reinforcement_learning/#action-values","text":"It is possible to condition the value on actions q(s,a) = E[G_t | S_t = s, A_t = a] State action pairs use the Q-function to find their value","title":"Action Values"},{"location":"machine_learning/reinforcement_learning/#agent-components","text":"Agent State Policy Value Function Model","title":"Agent Components"},{"location":"machine_learning/reinforcement_learning/#state","text":"Actions depend on the state of the agent Both agent and environment may have an internal state In the simplest case, there is only one shared state by the environemnt and agent Often there are multiple (sometimes infinite) different states The agent's state is usually different than the environment's state The agent might not know the full state of the environment","title":"State"},{"location":"machine_learning/reinforcement_learning/#environment-state","text":"The environment's internal state Usually not visible to the agent May contain lots of irrelevant information if visible","title":"Environment State"},{"location":"machine_learning/reinforcement_learning/#agent-state","text":"A history is a sequence of observations, action, rewards H_t = O_0,A_0,R_1,O_1,...,O_{t-1},A_{t-1},R_t,O_t The agent state S_t can be constructed from the history and is considerd to be a function of the history S_{t+1} = f(S_t,A_t,R_{t+1},O_{t+1}) where f is a state update function Means the next agent state is dependent on the current agent state, the action taken, the reward, and the observation A simple update function can be concatinating the previous states together to create the current state (Done w/ the Atari game agents by cocatenating frames together since frames are the state) Actions depend on this state Agent state is typically much smaller than the environment state and the full history","title":"Agent State"},{"location":"machine_learning/reinforcement_learning/#fully-observable-environments","text":"When the agent sees the full environment state observation = environment state Agent state could just be the observation S_t = O_t = environment state Then the agent is in a Markov Decision Process Ex: Single player board games in which the player can see the entire board Smart Pong is an example of fully observable environment (VERIFY)","title":"Fully Observable Environments"},{"location":"machine_learning/reinforcement_learning/#markov-decision-processes","text":"MDPs provide a useful mathematical framework for talking about many RL topics Easier to reason about than the full problem (long markovian) Limited b/c of markov assumption Def- A decision process is Markov if the future is independent of the past given the present H_t => S_t => H_{t+1} Once the current state ( S_t ) is known , the history ( H_t ) may be thrown away (useful for space consideration) The current state gives enough information for the next state to be predicted The environment state is typically Markov The history is Markov","title":"Markov Decision Processes"},{"location":"machine_learning/reinforcement_learning/#partially-observable-environments","text":"The agent gets parial information A robot w/ camera vision doesn't have its absolute location A poker playing agent only observes public cards Now the observation is not Markov Formally this is a partially observable Markov decision process (POMDP) The environment state can still be markov, but the agent does not know it The state should contain enough information for good policies/value predictions","title":"Partially Observable Environments"},{"location":"machine_learning/reinforcement_learning/#policy","text":"Defines the agent's behavior A map from agent state to action Deterministic Policy - function that outputs an action based on an input agent state Stochastic Policy - a probability of selecting each action in each state","title":"Policy"},{"location":"machine_learning/reinforcement_learning/#value-function","text":"The actual value function is the expected return V_\\pi(s)=E[G_t|S_t=s,\\pi] = E[R_{t+1}+\\gamma R_{t+2}+\\gamma^2R_{t+3}+... | S_t = s,\\pi] The return has a recursive form since G_t = R_{t+1}+\\gamma G_{t+1} Bellman Equation : V_\\pi (s) = E[R_{t+1} + \\gamma G_{t+1} | S_t = s,A_t \\sim \\pi (s)] where a \\sim \\pi (s) means action a is chosen by policy \\pi in state s Discount Factor ( \\gamma \\in [0,1] ) trades off importance of immediate vs. long-term rewards Closer to 1 means higher importane on immediate rewards (AKA discounting future rewards in favor of immediate rewards) The value depends on the policy Can be used to evaluate desirability of states Can be used to select btw actions","title":"Value Function:"},{"location":"machine_learning/statistical_learning/","text":"Statistical Learning Supervised Learning- building a statistical model for prediting/estimating an ouput based on some inputs. Basically is function approximation Regression- predicting continuous or quantitative outputs Least squares was the earliest form Classifiction- predicting discrete/categorial or qualitative outputs Linear discriminant analysis was the earliest form Unsupervised Learning- learn the relationships and strucuture from input data Clustering- want to group datapoints according to similarity Generalized Linear Model- describes a class of stat learning methods including both linear and logistic regression Only linear methods were used till the 80s becuase non-linear relationships were too computationally expensive to fit at the time In the 80s, computational power was sufficient for nonlinear methods to flourish Classification and Regression trees Generalized additive models Neural Networks Support Vector Machines (90s) In stat learning, inputs are referred to as predictors or independent vars (inputs/features in ML/CS) outputs are responses or dependent variables (ouputs/target in ML) Fitting Linear Models Let there be n datapoints where each datapoint has m features. We want to find the optimal parameters for f(x) = a_0 + \\sum_{i=1}^m a_i x_i . Linear regression of single var: model for making predictions is a line in 2D of 2 vars: model is a plane in 3D (for nonlinear models it would be a mesh/surface) of m vars: model is a hyperplane in the m+1 dimensional input-output space Least Squares Makes huge assumptions about structure of data and yields stable but possibly inaccurate predictions relies heavily on assumption that a linear model/decision boundary is appropriate low variance, high bias We fit a linear model to a set of training data by minimizing a cost function defined by the residual sum of squares RSS RSS(\\theta) = \\sum_{i=1}^n(y_i - x_i^T\\theta)^2 Vectorized: We can take the gradient of the cost function, set equal to zero and then solve for the parameters This way of computing the optimal paramters for a linear model is known as the normal equation Instead of the normal equation, could use gradient descent algorithm to iteratively find the minimum of the quadratic cost function k-nearest neighbor Makes very mild assumptions about structure of data and yields unstable but accurate predictions no assumptions on the underlying data like in least squares resulting in wiggly, unstable decision boundaries that depend on only a handful of neighboring points high variance, low bias Uses observations in the training set closest to the input to create predictions h(x) = \\frac{1}{k}\\sum_{x_i \\in N_k(x)} y_i N_k(x) is the neighborhood of x defined by the closest k points x_i in the training set Prediction is just the average of the k-closest datapoints in the training set How do we define closeness in building the neighborhood? Euclidean distance is nice Summary these two techniques are very popular (e.g. 1-nearest-neighbor is most used technique in low-dim problems) advancements/improvements: kernel methods use weights that decrease smoothly to zero as the distance increases from the target point (rather than the abrupt 0/1 used by k-nearest neighbors) distance kernels are modified to emphasize some variables more than others in high dimensional spaces Local regression fits linear models by locally weighted least squares Linear models fit to an expanded basis of the original inputs allow arbitrarily complex models idk why you would ever want this neural networks constist of sums of nonlinearly transformed linear models model selection & cross validation Given a good model for our training data, is the prediction's error on unseen data good or bad? k-fold cross validation better than simple train test split for estimating model prediction error divide dataset into k-sets For each subset: use it once as a test set and the other k-1 sets for training and record the results discard the model and start over again End result is k performance measures which can be averaged to get the models overall performance gives insight into which d is best for polynomial fit Common k = \\{2, 5, 10\\} better than hold-out leave one out cross validation (k = m) useful when m is small k = 5 or 10 is a good compromise select d based on training and validation best performer (hold out the test data) train final model on training + validation data and then measure performance on testing data Training Set: fit model to best parameters Validation Set: evaluate model with MSE At the end, retrain and measure performance with test set split into train and test set hold out test set for final evaluation (never used for training/model updates) variety of models with diff hyperparams train on training data and evaluate with validation data once the best model hyperparams have been selected, train on the training and validation data evaluate final model with test data that has never been seen before train, val, test = 60%, 20%, 20% of dataset as you increase d meaning the polynomial becomes more complex, it is able to fit the data better so the training loss will continue going down as d increses since essentially you reach a point where the curve is overfitting the data comparing the train loss to the validation loss and picking the d with the minimum validation loss is the best","title":"Statistical Learning"},{"location":"machine_learning/statistical_learning/#statistical-learning","text":"Supervised Learning- building a statistical model for prediting/estimating an ouput based on some inputs. Basically is function approximation Regression- predicting continuous or quantitative outputs Least squares was the earliest form Classifiction- predicting discrete/categorial or qualitative outputs Linear discriminant analysis was the earliest form Unsupervised Learning- learn the relationships and strucuture from input data Clustering- want to group datapoints according to similarity Generalized Linear Model- describes a class of stat learning methods including both linear and logistic regression Only linear methods were used till the 80s becuase non-linear relationships were too computationally expensive to fit at the time In the 80s, computational power was sufficient for nonlinear methods to flourish Classification and Regression trees Generalized additive models Neural Networks Support Vector Machines (90s) In stat learning, inputs are referred to as predictors or independent vars (inputs/features in ML/CS) outputs are responses or dependent variables (ouputs/target in ML)","title":"Statistical Learning"},{"location":"machine_learning/statistical_learning/#fitting-linear-models","text":"Let there be n datapoints where each datapoint has m features. We want to find the optimal parameters for f(x) = a_0 + \\sum_{i=1}^m a_i x_i . Linear regression of single var: model for making predictions is a line in 2D of 2 vars: model is a plane in 3D (for nonlinear models it would be a mesh/surface) of m vars: model is a hyperplane in the m+1 dimensional input-output space","title":"Fitting Linear Models"},{"location":"machine_learning/statistical_learning/#least-squares","text":"Makes huge assumptions about structure of data and yields stable but possibly inaccurate predictions relies heavily on assumption that a linear model/decision boundary is appropriate low variance, high bias We fit a linear model to a set of training data by minimizing a cost function defined by the residual sum of squares RSS RSS(\\theta) = \\sum_{i=1}^n(y_i - x_i^T\\theta)^2 Vectorized: We can take the gradient of the cost function, set equal to zero and then solve for the parameters This way of computing the optimal paramters for a linear model is known as the normal equation Instead of the normal equation, could use gradient descent algorithm to iteratively find the minimum of the quadratic cost function","title":"Least Squares"},{"location":"machine_learning/statistical_learning/#k-nearest-neighbor","text":"Makes very mild assumptions about structure of data and yields unstable but accurate predictions no assumptions on the underlying data like in least squares resulting in wiggly, unstable decision boundaries that depend on only a handful of neighboring points high variance, low bias Uses observations in the training set closest to the input to create predictions h(x) = \\frac{1}{k}\\sum_{x_i \\in N_k(x)} y_i N_k(x) is the neighborhood of x defined by the closest k points x_i in the training set Prediction is just the average of the k-closest datapoints in the training set How do we define closeness in building the neighborhood? Euclidean distance is nice","title":"k-nearest neighbor"},{"location":"machine_learning/statistical_learning/#summary","text":"these two techniques are very popular (e.g. 1-nearest-neighbor is most used technique in low-dim problems) advancements/improvements: kernel methods use weights that decrease smoothly to zero as the distance increases from the target point (rather than the abrupt 0/1 used by k-nearest neighbors) distance kernels are modified to emphasize some variables more than others in high dimensional spaces Local regression fits linear models by locally weighted least squares Linear models fit to an expanded basis of the original inputs allow arbitrarily complex models idk why you would ever want this neural networks constist of sums of nonlinearly transformed linear models","title":"Summary"},{"location":"machine_learning/statistical_learning/#model-selection-cross-validation","text":"Given a good model for our training data, is the prediction's error on unseen data good or bad? k-fold cross validation better than simple train test split for estimating model prediction error divide dataset into k-sets For each subset: use it once as a test set and the other k-1 sets for training and record the results discard the model and start over again End result is k performance measures which can be averaged to get the models overall performance gives insight into which d is best for polynomial fit Common k = \\{2, 5, 10\\} better than hold-out leave one out cross validation (k = m) useful when m is small k = 5 or 10 is a good compromise select d based on training and validation best performer (hold out the test data) train final model on training + validation data and then measure performance on testing data Training Set: fit model to best parameters Validation Set: evaluate model with MSE At the end, retrain and measure performance with test set split into train and test set hold out test set for final evaluation (never used for training/model updates) variety of models with diff hyperparams train on training data and evaluate with validation data once the best model hyperparams have been selected, train on the training and validation data evaluate final model with test data that has never been seen before train, val, test = 60%, 20%, 20% of dataset as you increase d meaning the polynomial becomes more complex, it is able to fit the data better so the training loss will continue going down as d increses since essentially you reach a point where the curve is overfitting the data comparing the train loss to the validation loss and picking the d with the minimum validation loss is the best","title":"model selection &amp; cross validation"},{"location":"machine_learning/supervised_learning/","text":"Supervised Learning Regression- predicting continuous values Classification- predicting discrete values Function Approximation Function approximation is a technique for estimating an unknown underyling function using observations from the domain and their resulting image. Given a dataset of inputs and outputs, we can assume there is an unkwnown underyling function that maps the inputs to the outputs. We can use supervised learning techniques to approximate this function. Supervised learning achieves this by minimizing the error between predicted outputs and expected outputs from the domain by changing its parameters (the weights of the network). Thus, simple feedforward neural networks can be better thought of as function approximators rather than models of how the brain works. The true function mapping inputs to outputs is referred to as the target function since it is the target of the learning process. Thus, intuitively we can better get a sense of what the function is by observing more input-output data points. The universal approximation theorem states a feedforward NN with a linear output layer and at least 1 hidden layer with a nonlinear activation function can approximate any function mapping between two finite dimensional spaces with any desired non-zero amount of error (provided enough hidden units). Below is an example of y=x^2 being approximated by a simple feedforward NN with 2 hidden layers of 10 nodes with relu activation trained on 100 observations: . Resources Function Approximators","title":"Supervised Learning"},{"location":"machine_learning/supervised_learning/#supervised-learning","text":"Regression- predicting continuous values Classification- predicting discrete values","title":"Supervised Learning"},{"location":"machine_learning/supervised_learning/#function-approximation","text":"Function approximation is a technique for estimating an unknown underyling function using observations from the domain and their resulting image. Given a dataset of inputs and outputs, we can assume there is an unkwnown underyling function that maps the inputs to the outputs. We can use supervised learning techniques to approximate this function. Supervised learning achieves this by minimizing the error between predicted outputs and expected outputs from the domain by changing its parameters (the weights of the network). Thus, simple feedforward neural networks can be better thought of as function approximators rather than models of how the brain works. The true function mapping inputs to outputs is referred to as the target function since it is the target of the learning process. Thus, intuitively we can better get a sense of what the function is by observing more input-output data points. The universal approximation theorem states a feedforward NN with a linear output layer and at least 1 hidden layer with a nonlinear activation function can approximate any function mapping between two finite dimensional spaces with any desired non-zero amount of error (provided enough hidden units). Below is an example of y=x^2 being approximated by a simple feedforward NN with 2 hidden layers of 10 nodes with relu activation trained on 100 observations: .","title":"Function Approximation"},{"location":"machine_learning/supervised_learning/#resources","text":"Function Approximators","title":"Resources"},{"location":"machine_learning/svms/","text":"Kernel Methods Attributes - original input values Features - new set of quantities from the attributes A feature map \\phi maps attributes to features GD update becomes computationally expensive when the features are high dimensional since its computing \\theta := \\theta - \\alpha \\sum_{i=1}^n (y^{(i)} - \\theta^T \\phi(x^{(i)}))\\phi(x^{(i)}) Kernel Trick Avoids computing the explicit mapping needed to allow linear learning algorithms to learn a nonlinear decision boundary Dot products in the high-dimensional feature space can be computed as a kernel function K(x, x') = \\phi (x)^T \\phi(x') thus avoiding the need to actually compute \\phi (x) Kernels Dot products measure similarity of two vectors (tells us how parallel two vectors are) dot product projects a vector onto another and is the length of that projected vector Since kernels replace dot products, they are a similarity measure They are a class of functions (defined by mercer's theorem) that provably correspond to the dot product of some higher dimensional mapping Allow us to replace all dot products in the dual SVM computation with the kernel function that represents the data in a higher dimensional (and linearly separable) feature space Linear Just the basic dot product (no feature mapping) Can be used when the data is already linearly separable Polynomial K(x, x') = (x^Tx' + c )^d where c \\geq 0 and d \\geq 1 are parameters of the kernel. When c = 0 the kernel is called homogeneous Commonly applied to NLP problems with d = 2 (higher d usually results in overfitting on NLP problems) The mapped to feature space is equivalent to that of polynomial regression however the number of parameters to learn is decreased drastically Radial Basis Function / Gaussian K(x, x') = \\exp(-\\gamma || x - x'||^2) where \\gamma = \\frac{1}{2\\sigma^2} is a parameter of the kernel that controls the spread. The RBF kernel projects features into an infinite dimensional Euclidean space which is where it is able to linearly separate any originally nonlinearly seperable data (this is why it performs so well and is so often used) RBF Kernel as Projection into Infinite Dimensional Space Need to do feature scaling before using gaussian kernel Other Specialized to data type: String, Graph, Tree, Wavelet, etc. chi-square cosine similarity Sigmoid: K(x, x') = \\tanh (\\beta x^T x' + a) Support Vector Machines Formulated as an optimization problem to find the optimal parameters The dual form of the optimization problem depends only on the dot product of datapoints (similarity of the training examples) Replacing this dot product with a kernel function allows the algorithm to learn a nonlinear decision boundary in the original input space by projecting the low dimensional features into a higher dimensional feature space where they are linearly separable Using the kernel trick allowsus to leverage the advantages of the feature mapping without the computational costs of computing the mapping for each datapoint The cost function is convex so it guarantees the existance of a global extremum Resources AI Master Kernel SVM MIT Lecture (Great- builds up from basics!) svms want a decision boundary represented by a hyperplane for 2d, our hyperplane will be <w,x> + b = 0 ( w is the normal vector to the hyperplane and b is the bias so our hyperplanes don't have to pass through the origin) thus the decision rule will be h_w(z) = \\text{sign}(<w,x> + b) many possible separating hyperplanes for a given labeled training dataset we want to find the hyperplan that generalzies the best to new data we theorize the hyperplane that is as far away from any training point as possible is the one that generalizes best to the new data we define the margin of a datapoint for a given hyperplane defined by (w,b) to be <x , w> since x 's projection onto the normal of the hyperplane is simply its distance from the hyperplane the geometric margin of a hyperplane (w,b) wrt to a dataset D is the smallest distance from a training point x_i to the hyperplane linearizing the constrinats we want \\text{sign}(<x_i, w> +b ) = \\text{sign}(y_i) so we can multiply them together (if the signs agree, result is positive, otherwise negative) so the constraint for each training point becomes (<x_i, w> + b)y_i \\geq 0 linear constraint since y_i is a constant LHS is called the functional margin sign of the inner product is independent of w 's scaling so it doesn't have to be a unit vector","title":"SVMs"},{"location":"machine_learning/svms/#kernel-methods","text":"Attributes - original input values Features - new set of quantities from the attributes A feature map \\phi maps attributes to features GD update becomes computationally expensive when the features are high dimensional since its computing \\theta := \\theta - \\alpha \\sum_{i=1}^n (y^{(i)} - \\theta^T \\phi(x^{(i)}))\\phi(x^{(i)})","title":"Kernel Methods"},{"location":"machine_learning/svms/#kernel-trick","text":"Avoids computing the explicit mapping needed to allow linear learning algorithms to learn a nonlinear decision boundary Dot products in the high-dimensional feature space can be computed as a kernel function K(x, x') = \\phi (x)^T \\phi(x') thus avoiding the need to actually compute \\phi (x)","title":"Kernel Trick"},{"location":"machine_learning/svms/#kernels","text":"Dot products measure similarity of two vectors (tells us how parallel two vectors are) dot product projects a vector onto another and is the length of that projected vector Since kernels replace dot products, they are a similarity measure They are a class of functions (defined by mercer's theorem) that provably correspond to the dot product of some higher dimensional mapping Allow us to replace all dot products in the dual SVM computation with the kernel function that represents the data in a higher dimensional (and linearly separable) feature space","title":"Kernels"},{"location":"machine_learning/svms/#linear","text":"Just the basic dot product (no feature mapping) Can be used when the data is already linearly separable","title":"Linear"},{"location":"machine_learning/svms/#polynomial","text":"K(x, x') = (x^Tx' + c )^d where c \\geq 0 and d \\geq 1 are parameters of the kernel. When c = 0 the kernel is called homogeneous Commonly applied to NLP problems with d = 2 (higher d usually results in overfitting on NLP problems) The mapped to feature space is equivalent to that of polynomial regression however the number of parameters to learn is decreased drastically","title":"Polynomial"},{"location":"machine_learning/svms/#radial-basis-function-gaussian","text":"K(x, x') = \\exp(-\\gamma || x - x'||^2) where \\gamma = \\frac{1}{2\\sigma^2} is a parameter of the kernel that controls the spread. The RBF kernel projects features into an infinite dimensional Euclidean space which is where it is able to linearly separate any originally nonlinearly seperable data (this is why it performs so well and is so often used) RBF Kernel as Projection into Infinite Dimensional Space Need to do feature scaling before using gaussian kernel","title":"Radial Basis Function / Gaussian"},{"location":"machine_learning/svms/#other","text":"Specialized to data type: String, Graph, Tree, Wavelet, etc. chi-square cosine similarity Sigmoid: K(x, x') = \\tanh (\\beta x^T x' + a)","title":"Other"},{"location":"machine_learning/svms/#support-vector-machines","text":"Formulated as an optimization problem to find the optimal parameters The dual form of the optimization problem depends only on the dot product of datapoints (similarity of the training examples) Replacing this dot product with a kernel function allows the algorithm to learn a nonlinear decision boundary in the original input space by projecting the low dimensional features into a higher dimensional feature space where they are linearly separable Using the kernel trick allowsus to leverage the advantages of the feature mapping without the computational costs of computing the mapping for each datapoint The cost function is convex so it guarantees the existance of a global extremum","title":"Support Vector Machines"},{"location":"machine_learning/svms/#resources","text":"AI Master Kernel SVM MIT Lecture (Great- builds up from basics!)","title":"Resources"},{"location":"machine_learning/svms/#svms","text":"want a decision boundary represented by a hyperplane for 2d, our hyperplane will be <w,x> + b = 0 ( w is the normal vector to the hyperplane and b is the bias so our hyperplanes don't have to pass through the origin) thus the decision rule will be h_w(z) = \\text{sign}(<w,x> + b) many possible separating hyperplanes for a given labeled training dataset we want to find the hyperplan that generalzies the best to new data we theorize the hyperplane that is as far away from any training point as possible is the one that generalizes best to the new data we define the margin of a datapoint for a given hyperplane defined by (w,b) to be <x , w> since x 's projection onto the normal of the hyperplane is simply its distance from the hyperplane the geometric margin of a hyperplane (w,b) wrt to a dataset D is the smallest distance from a training point x_i to the hyperplane linearizing the constrinats we want \\text{sign}(<x_i, w> +b ) = \\text{sign}(y_i) so we can multiply them together (if the signs agree, result is positive, otherwise negative) so the constraint for each training point becomes (<x_i, w> + b)y_i \\geq 0 linear constraint since y_i is a constant LHS is called the functional margin sign of the inner product is independent of w 's scaling so it doesn't have to be a unit vector","title":"svms"},{"location":"machine_learning/nn/activation_functions/","text":"Activation Functions Mathematical equations that determine the output of a node/neuron in a neural network Determines whether each neuron should be activated based on the neuron's input and the specific activation function Normalize the output of a neuron to a range: [0,1] or [-1,1] Must be computationally efficient b/c they are calculated across thousands or millions of neurons for each pass Having a computationally efficient derivative is good too for backpropagation Can be as simple as a step function that turns the neuron output on or off (e.g. 0 or 1) Binary Step Function (Doesn't allow multi-value outputs so it can't do multi-class classification) Can be a trasnformation that maps input signals to output signals Linear activations functions collapse a neural network into one layer b/c a linear combination of linear functions is still a linear function (not very useful) Can't use backpropagation with linear activation functions b/c the derivative is a constant so its impossible to see the effect of changing weights Non-linear activation functions allow the neural network to learn more complex data like images, videos, and audio Allow backpropagation b/c the derivative function is related to the inputs Allow sacking of multiple hidden layers unlike linear activation functions Most all processes can be modeled by a neural network w/ nonlinear activation functions Why do we need nonlinear activation functions? Matrix multiplication and vector addition are simply linear transformations so in order to learn any nonlinear representations of data, we need to introduce a non-linear function or else the most complex classification boundary it can learn will be linear (e.g. line, plane, etc. ) which isn't that useful for interesting problems. If the activation function was linear (e.g. identity function), then the network would essentially just be a linear regression. In order to introduce non-linearity (otherwise matrix ops will result in only linear transformations), we apply non-linear activation functions pointwise to the results of the linear transformations at each layer. Sigmoid/Logistic \\sigma(x) = \\dfrac{1}{1 + e^{-x}} \\dfrac{d}{dx}\\sigma(x) = \\sigma(x)(1 - \\sigma(x)) Pros: Smooth Gradient prevents jumps in the output values Output vaues bound btw 0 and 1, normalizing the each neuron's output can be interpreted as a probability Clear predictions - any values above 2 or below -2 are very close to the edge of the curve Cons: Vanishing Gradient- almost no change in the prediction for very high and very low input values Very large input values to sigmoid function result in a tiny gradient since we are at a nearly flat point of the function This makes it hard to update the parameters earlier in the network Can result in the network refusing to learn further or being too slow to reach an accurate prediction Works well in linear regime but work poorly in the saturating regimes Outputs not zero centered Computationally expensive Detailed Derivation: \\begin{align}\\dfrac{d}{dx} \\sigma(x) &= \\dfrac{d}{dx} \\left[ \\dfrac{1}{1 + e^{-x}} \\right] \\\\ &= \\dfrac{d}{dx} \\left( 1 + \\mathrm{e}^{-x} \\right)^{-1} \\\\ &= -(1 + e^{-x})^{-2}(-e^{-x}) \\\\ &= \\dfrac{e^{-x}}{\\left(1 + e^{-x}\\right)^2} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{e^{-x}}{1 + e^{-x}} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{(1 + e^{-x}) - 1}{1 + e^{-x}} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( \\dfrac{1 + e^{-x}}{1 + e^{-x}} - \\dfrac{1}{1 + e^{-x}} \\right) \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( 1 - \\dfrac{1}{1 + e^{-x}} \\right) \\\\ &= \\sigma(x) \\cdot (1 - \\sigma(x)) \\end{align} TanH (Hyperbolic Tangent) \\text{tanh}(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}} \\text{tanh}'(z) = 1 - \\text{tanh}(z)^2 Similar in advantages and disadvantages to sigmoid except it has the advantage of being zero centered (i.e. output range is [-1, 1] ), making it easier to models input that have strongly negative, neutral, and strongly positive values (e.g. reward for deep RL model) ReLU (Rectified Linear Unit) \\text{ReLU}(z) = \\begin{cases}z & \\text{if } z \\geq 0 \\\\ 0 & \\text{if } z < 0\\end{cases} \\text{ReLU}'(z) = \\begin{cases}1 & \\text{if } z \\geq 0 \\\\ 0 & \\text{if } z < 0\\end{cases} Pros: Computationally efficient - allows the network to converge quickly Non-linear - although it seems like a lienar function, its derivative allows for backpropagation Cons: The Dying ReLU problem - as inputs approach zero or are negative, the gradient becomes zero, so the network can't perform backpropagation and therefore cannot learn Leaky ReLU Pros: Prevents dying ReLU problem - the smallpositive slope in the negative area enables backpropagation for negative inputs Otherwise like ReLU Cons: Inconsistent Results - does't provide consistent predicitions for negative input values https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/","title":"Activation Functions"},{"location":"machine_learning/nn/activation_functions/#activation-functions","text":"Mathematical equations that determine the output of a node/neuron in a neural network Determines whether each neuron should be activated based on the neuron's input and the specific activation function Normalize the output of a neuron to a range: [0,1] or [-1,1] Must be computationally efficient b/c they are calculated across thousands or millions of neurons for each pass Having a computationally efficient derivative is good too for backpropagation Can be as simple as a step function that turns the neuron output on or off (e.g. 0 or 1) Binary Step Function (Doesn't allow multi-value outputs so it can't do multi-class classification) Can be a trasnformation that maps input signals to output signals Linear activations functions collapse a neural network into one layer b/c a linear combination of linear functions is still a linear function (not very useful) Can't use backpropagation with linear activation functions b/c the derivative is a constant so its impossible to see the effect of changing weights Non-linear activation functions allow the neural network to learn more complex data like images, videos, and audio Allow backpropagation b/c the derivative function is related to the inputs Allow sacking of multiple hidden layers unlike linear activation functions Most all processes can be modeled by a neural network w/ nonlinear activation functions","title":"Activation Functions"},{"location":"machine_learning/nn/activation_functions/#why-do-we-need-nonlinear-activation-functions","text":"Matrix multiplication and vector addition are simply linear transformations so in order to learn any nonlinear representations of data, we need to introduce a non-linear function or else the most complex classification boundary it can learn will be linear (e.g. line, plane, etc. ) which isn't that useful for interesting problems. If the activation function was linear (e.g. identity function), then the network would essentially just be a linear regression. In order to introduce non-linearity (otherwise matrix ops will result in only linear transformations), we apply non-linear activation functions pointwise to the results of the linear transformations at each layer.","title":"Why do we need nonlinear activation functions?"},{"location":"machine_learning/nn/activation_functions/#sigmoidlogistic","text":"\\sigma(x) = \\dfrac{1}{1 + e^{-x}} \\dfrac{d}{dx}\\sigma(x) = \\sigma(x)(1 - \\sigma(x)) Pros: Smooth Gradient prevents jumps in the output values Output vaues bound btw 0 and 1, normalizing the each neuron's output can be interpreted as a probability Clear predictions - any values above 2 or below -2 are very close to the edge of the curve Cons: Vanishing Gradient- almost no change in the prediction for very high and very low input values Very large input values to sigmoid function result in a tiny gradient since we are at a nearly flat point of the function This makes it hard to update the parameters earlier in the network Can result in the network refusing to learn further or being too slow to reach an accurate prediction Works well in linear regime but work poorly in the saturating regimes Outputs not zero centered Computationally expensive Detailed Derivation: \\begin{align}\\dfrac{d}{dx} \\sigma(x) &= \\dfrac{d}{dx} \\left[ \\dfrac{1}{1 + e^{-x}} \\right] \\\\ &= \\dfrac{d}{dx} \\left( 1 + \\mathrm{e}^{-x} \\right)^{-1} \\\\ &= -(1 + e^{-x})^{-2}(-e^{-x}) \\\\ &= \\dfrac{e^{-x}}{\\left(1 + e^{-x}\\right)^2} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{e^{-x}}{1 + e^{-x}} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{(1 + e^{-x}) - 1}{1 + e^{-x}} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( \\dfrac{1 + e^{-x}}{1 + e^{-x}} - \\dfrac{1}{1 + e^{-x}} \\right) \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( 1 - \\dfrac{1}{1 + e^{-x}} \\right) \\\\ &= \\sigma(x) \\cdot (1 - \\sigma(x)) \\end{align}","title":"Sigmoid/Logistic"},{"location":"machine_learning/nn/activation_functions/#tanh-hyperbolic-tangent","text":"\\text{tanh}(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}} \\text{tanh}'(z) = 1 - \\text{tanh}(z)^2 Similar in advantages and disadvantages to sigmoid except it has the advantage of being zero centered (i.e. output range is [-1, 1] ), making it easier to models input that have strongly negative, neutral, and strongly positive values (e.g. reward for deep RL model)","title":"TanH (Hyperbolic Tangent)"},{"location":"machine_learning/nn/activation_functions/#relu-rectified-linear-unit","text":"\\text{ReLU}(z) = \\begin{cases}z & \\text{if } z \\geq 0 \\\\ 0 & \\text{if } z < 0\\end{cases} \\text{ReLU}'(z) = \\begin{cases}1 & \\text{if } z \\geq 0 \\\\ 0 & \\text{if } z < 0\\end{cases} Pros: Computationally efficient - allows the network to converge quickly Non-linear - although it seems like a lienar function, its derivative allows for backpropagation Cons: The Dying ReLU problem - as inputs approach zero or are negative, the gradient becomes zero, so the network can't perform backpropagation and therefore cannot learn","title":"ReLU (Rectified Linear Unit)"},{"location":"machine_learning/nn/activation_functions/#leaky-relu","text":"Pros: Prevents dying ReLU problem - the smallpositive slope in the negative area enables backpropagation for negative inputs Otherwise like ReLU Cons: Inconsistent Results - does't provide consistent predicitions for negative input values https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/","title":"Leaky ReLU"},{"location":"machine_learning/nn/auto_encoders/","text":"Auto Encoders Takes high dim input and tries to compress it to a smaller representation Two Principle Components: Encoder- series of layers that compress input to smaller layer callde the bottleneck Decoder-","title":"AutoEncoders"},{"location":"machine_learning/nn/auto_encoders/#auto-encoders","text":"Takes high dim input and tries to compress it to a smaller representation Two Principle Components: Encoder- series of layers that compress input to smaller layer callde the bottleneck Decoder-","title":"Auto Encoders"},{"location":"machine_learning/nn/embeddings/","text":"Embeddings Map items (e.g. movies, text, imgs, etc.) to low-dimensional real vectors such that similar items are close to each other Can be applied to dense data like audio to create a meaningful similarity metric Can jointly embedd multiple data types (e.g. text and images) to define a similarity between them embedding layer in a neural network is just a single hidden layer with one neuron for each dimension (usually d dimensions) d \\approx \\text{possible vals}^ {1/4} rule of thumb, should be cross validated for particular datasets Continuous Retrieval Classical retrieval uses inverted index oftentimes on word Continuous retrieval represents objects as vectors in a vector space where similar objects are placed closer together 3 Requirements: learn to represent objects as contiuous vectors (place everything in a learned space) learn to place similar objects close together learn to retrieve neighboring objects really fast (given a new query, encode it in the learned space and find its nearest neighbors) Resources PyTorch Embedding Module Video Google Devs Embeddings Lecture Word Embeddings","title":"Embeddings"},{"location":"machine_learning/nn/embeddings/#embeddings","text":"Map items (e.g. movies, text, imgs, etc.) to low-dimensional real vectors such that similar items are close to each other Can be applied to dense data like audio to create a meaningful similarity metric Can jointly embedd multiple data types (e.g. text and images) to define a similarity between them embedding layer in a neural network is just a single hidden layer with one neuron for each dimension (usually d dimensions) d \\approx \\text{possible vals}^ {1/4} rule of thumb, should be cross validated for particular datasets","title":"Embeddings"},{"location":"machine_learning/nn/embeddings/#continuous-retrieval","text":"Classical retrieval uses inverted index oftentimes on word Continuous retrieval represents objects as vectors in a vector space where similar objects are placed closer together 3 Requirements: learn to represent objects as contiuous vectors (place everything in a learned space) learn to place similar objects close together learn to retrieve neighboring objects really fast (given a new query, encode it in the learned space and find its nearest neighbors)","title":"Continuous Retrieval"},{"location":"machine_learning/nn/embeddings/#resources","text":"PyTorch Embedding Module Video Google Devs Embeddings Lecture Word Embeddings","title":"Resources"},{"location":"machine_learning/nn/overview/","text":"Neural Networks network can be viewed as a function we want to learn that perfectly maps from inputs to outputs given a dataset, how we define our loss function determines how well we learn the mapping (how well we approximate the function) logistic regression can be seen as a special instance of a neural network A neuron is an operation that is a composite of a linear operation with an activation/thresholding operation Classification Tasks: Neurons in early layers of a network become feature detectors for things such as edges 1st layer can only detect edges or similar basic features since it is given only pixel information in a simple fully connected network later layers can detect more complex features by combining information from earlier feature detectors indicating the presence/orientation of certain edges or other simple features Neurons in the later layers build up more complex feature detectors using the feature detector neurons in the previous layer and the weights connecting the layers Regression Tasks: Layers learn more complex features from the input features (e.g. relationships between features, the weighting of importance of certain features) The learned features are often better than hand engineered features Often difficult to understand what features are being learned and that's why NN's are often considered black boxes A model is an architecture with parameters end-to-end learning- we train based on inputs and outputs and don't care about whats happening on the inside (blackbox model) Loss Function \\mathcal{L} - for 1 example Cost Function J - for multiple examples J = \\frac{1}{m}\\sum_{i=1}^m \\mathcal{L}^{(i)} Can find the derivative by just computing derivative of \\mathcal{L} since summation is a linear operation so we can just compute the derivative inside the summation Normalizing input to neural networks can help it learn better since certain features will not have outsized influence due to their scaling/units normalize = center (subtract by mean) and standardize (divide by std deviation) this results in circular contours to the cost function which result in a smoother descent for gradient descent compared to the unnormalized input which will have ellipsoidal contours mean and standard deviation from the train set must also be applied to test set Smaller the batch, the more noisy the convergence Full batch is the best approximation of the true cost function Smaller batches serve as good enough approximations of the true cost function that are faster to compute, allowing more updates to the parameters in a given amount of time More (less accurate) updates are typically preferred over fewer (very accurate) updates the most accurate updates computed over the whole batch will have the smoothest convergence Some research suggests the stochastity of smaller batches/single samples in gradient descent can help it escape local minima in the cost function that full batch gradient might get caught in Weight Initialization One way to avoid vanishing and exploding gradients is by initializing the weights to be near the ideal values The larger the number of input features to a layer, the smaller we want each individual weight to be so that the dot product of w^Tx does not explode/become too lare For sigmoid activation an initialization that works well: w^{[\\ell] } = \\text{random}(shape) \\times \\frac{1}{\\sqrt{n^{[\\ell - 1]}}} For ReLU activation an initialization that works well: w^{[\\ell] } = \\text{random}(shape) \\times \\frac{2}{\\sqrt{n^{[\\ell - 1]}}} Xavier Initialization used for sigmoid and tanh activation Read More He Initialization: used for relu activation takes into account number of inputs and outputs b/c of the back propagation step Weights need to be randomly initialized or else network will suffer from the symmetry problem where all neurons start learning the same thing (need to be able to evolve independently)","title":"Overview"},{"location":"machine_learning/nn/overview/#neural-networks","text":"network can be viewed as a function we want to learn that perfectly maps from inputs to outputs given a dataset, how we define our loss function determines how well we learn the mapping (how well we approximate the function) logistic regression can be seen as a special instance of a neural network A neuron is an operation that is a composite of a linear operation with an activation/thresholding operation Classification Tasks: Neurons in early layers of a network become feature detectors for things such as edges 1st layer can only detect edges or similar basic features since it is given only pixel information in a simple fully connected network later layers can detect more complex features by combining information from earlier feature detectors indicating the presence/orientation of certain edges or other simple features Neurons in the later layers build up more complex feature detectors using the feature detector neurons in the previous layer and the weights connecting the layers Regression Tasks: Layers learn more complex features from the input features (e.g. relationships between features, the weighting of importance of certain features) The learned features are often better than hand engineered features Often difficult to understand what features are being learned and that's why NN's are often considered black boxes A model is an architecture with parameters end-to-end learning- we train based on inputs and outputs and don't care about whats happening on the inside (blackbox model) Loss Function \\mathcal{L} - for 1 example Cost Function J - for multiple examples J = \\frac{1}{m}\\sum_{i=1}^m \\mathcal{L}^{(i)} Can find the derivative by just computing derivative of \\mathcal{L} since summation is a linear operation so we can just compute the derivative inside the summation Normalizing input to neural networks can help it learn better since certain features will not have outsized influence due to their scaling/units normalize = center (subtract by mean) and standardize (divide by std deviation) this results in circular contours to the cost function which result in a smoother descent for gradient descent compared to the unnormalized input which will have ellipsoidal contours mean and standard deviation from the train set must also be applied to test set Smaller the batch, the more noisy the convergence Full batch is the best approximation of the true cost function Smaller batches serve as good enough approximations of the true cost function that are faster to compute, allowing more updates to the parameters in a given amount of time More (less accurate) updates are typically preferred over fewer (very accurate) updates the most accurate updates computed over the whole batch will have the smoothest convergence Some research suggests the stochastity of smaller batches/single samples in gradient descent can help it escape local minima in the cost function that full batch gradient might get caught in","title":"Neural Networks"},{"location":"machine_learning/nn/overview/#weight-initialization","text":"One way to avoid vanishing and exploding gradients is by initializing the weights to be near the ideal values The larger the number of input features to a layer, the smaller we want each individual weight to be so that the dot product of w^Tx does not explode/become too lare For sigmoid activation an initialization that works well: w^{[\\ell] } = \\text{random}(shape) \\times \\frac{1}{\\sqrt{n^{[\\ell - 1]}}} For ReLU activation an initialization that works well: w^{[\\ell] } = \\text{random}(shape) \\times \\frac{2}{\\sqrt{n^{[\\ell - 1]}}} Xavier Initialization used for sigmoid and tanh activation Read More He Initialization: used for relu activation takes into account number of inputs and outputs b/c of the back propagation step Weights need to be randomly initialized or else network will suffer from the symmetry problem where all neurons start learning the same thing (need to be able to evolve independently)","title":"Weight Initialization"},{"location":"machine_learning/nn/pytorch/","text":"PyTorch Blitz autograd- calculates and stores the gradients for each model param in each parameter's .grad attribute simple training loop: import torch model = torchvision.models.resnet18(pretrained=True) data = torch.rand(1,3,64,64) # 64 x 64 img with 3 channels labels = torch.rand(1,1000) prediction = model(data) # Forward pass loss = (prediction - labels).sum() loss.backward() # backward pass (backprop) # register all the model params in the optimizer optimizer = torch.optim.SGD(model.parameters(),lr=1e-2,momentum=0.9) optimizer.step() # initiate gradient descent if a parameter in a NN does not requires_grad then these params are known as frozen meaning the gradients won't be recomputed. NOTE: torch.no_grad also does same thing ( read more ). also useful for finetuning pretrained networks where you only want to modify the params in the classifer layers to make predictions on the new labels: model = torchvision.models.resnet18(pretrained=True) # Freeze all the parameters in the network for param in model.parameters(): param.requires_grad = False # replace last linear layer (the classifier) with new unfrozen classification layer model.fc = nn.Linear(512, 10) torch.nn pkg nn.Module contains layers and a forward(input) method that returns the output. Can build modules of networks that can be used in other networks. Only have to define the forward function and the backward function will be automatically defined using autograd (relies on autograd parsing the operations in the forward function and creating the appropriate computational graph of all the derivatives) net.parameters() returns the learnable parameters of a nn module only supports mini-batch inputs, no single input samples import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = torch.flatten(x, 1) # flatten all dimensions except batch x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target. Many are provided by the nn pkg such as nn.MSELoss but you can also define your own In order to backpropagate the error/loss, we need to clear the existing gradients ( net.zero_grad() ) and then call loss.backwards() https://pytorch.org/docs/stable/nn.html updating the weights can be done using the below code since weights = weights - learning rate * gradient for SGD: learning_rate = 0.01 for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) using the torch.optim module lets you easily use other update rules like SGD, Adam, RMSProp, etc. import torch.optim as optim # create your optimizer optimizer = optim.SGD(net.parameters(), lr=0.01) # in your training loop: optimizer.zero_grad() # zero the gradient buffers output = net(input) loss = criterion(output, target) loss.backward() optimizer.step() # Does the update recap torch.Tensor - A multi-dimensional array with support for autograd operations like backward() . Also holds the gradient w.r.t. the tensor. nn.Module - Neural network module. Convenient way of encapsulating parameters , with helpers for moving them to GPU, exporting, loading, etc. nn.Parameter - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module . autograd.Function - Implements forward and backward definitions of an autograd operation . Every Tensor operation creates at least a single Function node that connects to functions that created a Tensor and encodes its history . gradient clipping optimizer.zero_grad() loss.backward() # by value torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1) # by norm # torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm=1) optimizer.step() Note that it stacks all the paremeters into a single vector then performs the clipping we want to just clip the gradients from th hooks tensor.backward() starts the backward pass on the computational graph with a default starting gradient value of 1.0 allow us to inspect (and possibly change) gradients as they flow backwards through the graph hooks get called on tensors in the order they were added .retain_grad() stores the grad on non-leaf/intermediate nodes in the computational graph when adding hooks to a intermediate node in the forward graph (stored in the backward_hooks dict), the function will also be added as a pre-hook to the corresponding node in the backwards graph to be run on the gradient before the node does its thing def fn(grad): print(grad) return grad + 2 # if you return nothing, the same gradient as before will be used c.register_hook(fn)","title":"PyTorch"},{"location":"machine_learning/nn/pytorch/#pytorch-blitz","text":"autograd- calculates and stores the gradients for each model param in each parameter's .grad attribute simple training loop: import torch model = torchvision.models.resnet18(pretrained=True) data = torch.rand(1,3,64,64) # 64 x 64 img with 3 channels labels = torch.rand(1,1000) prediction = model(data) # Forward pass loss = (prediction - labels).sum() loss.backward() # backward pass (backprop) # register all the model params in the optimizer optimizer = torch.optim.SGD(model.parameters(),lr=1e-2,momentum=0.9) optimizer.step() # initiate gradient descent if a parameter in a NN does not requires_grad then these params are known as frozen meaning the gradients won't be recomputed. NOTE: torch.no_grad also does same thing ( read more ). also useful for finetuning pretrained networks where you only want to modify the params in the classifer layers to make predictions on the new labels: model = torchvision.models.resnet18(pretrained=True) # Freeze all the parameters in the network for param in model.parameters(): param.requires_grad = False # replace last linear layer (the classifier) with new unfrozen classification layer model.fc = nn.Linear(512, 10)","title":"PyTorch Blitz"},{"location":"machine_learning/nn/pytorch/#torchnn-pkg","text":"nn.Module contains layers and a forward(input) method that returns the output. Can build modules of networks that can be used in other networks. Only have to define the forward function and the backward function will be automatically defined using autograd (relies on autograd parsing the operations in the forward function and creating the appropriate computational graph of all the derivatives) net.parameters() returns the learnable parameters of a nn module only supports mini-batch inputs, no single input samples import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = torch.flatten(x, 1) # flatten all dimensions except batch x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target. Many are provided by the nn pkg such as nn.MSELoss but you can also define your own In order to backpropagate the error/loss, we need to clear the existing gradients ( net.zero_grad() ) and then call loss.backwards() https://pytorch.org/docs/stable/nn.html updating the weights can be done using the below code since weights = weights - learning rate * gradient for SGD: learning_rate = 0.01 for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) using the torch.optim module lets you easily use other update rules like SGD, Adam, RMSProp, etc. import torch.optim as optim # create your optimizer optimizer = optim.SGD(net.parameters(), lr=0.01) # in your training loop: optimizer.zero_grad() # zero the gradient buffers output = net(input) loss = criterion(output, target) loss.backward() optimizer.step() # Does the update","title":"torch.nn pkg"},{"location":"machine_learning/nn/pytorch/#recap","text":"torch.Tensor - A multi-dimensional array with support for autograd operations like backward() . Also holds the gradient w.r.t. the tensor. nn.Module - Neural network module. Convenient way of encapsulating parameters , with helpers for moving them to GPU, exporting, loading, etc. nn.Parameter - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module . autograd.Function - Implements forward and backward definitions of an autograd operation . Every Tensor operation creates at least a single Function node that connects to functions that created a Tensor and encodes its history .","title":"recap"},{"location":"machine_learning/nn/pytorch/#gradient-clipping","text":"optimizer.zero_grad() loss.backward() # by value torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1) # by norm # torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm=1) optimizer.step() Note that it stacks all the paremeters into a single vector then performs the clipping we want to just clip the gradients from th","title":"gradient clipping"},{"location":"machine_learning/nn/pytorch/#hooks","text":"tensor.backward() starts the backward pass on the computational graph with a default starting gradient value of 1.0 allow us to inspect (and possibly change) gradients as they flow backwards through the graph hooks get called on tensors in the order they were added .retain_grad() stores the grad on non-leaf/intermediate nodes in the computational graph when adding hooks to a intermediate node in the forward graph (stored in the backward_hooks dict), the function will also be added as a pre-hook to the corresponding node in the backwards graph to be run on the gradient before the node does its thing def fn(grad): print(grad) return grad + 2 # if you return nothing, the same gradient as before will be used c.register_hook(fn)","title":"hooks"},{"location":"machine_learning/nn/ssl/","text":"Self Supervised Learning By forcing the model to learn a task defined in terms of input images, it forces the network to learn features which will be useful for downstream general visual recognition tasks later Weakenss of Supervised Learning Annotations: costly (e.g. medical imaging data) ambiguous (set of labels not always obvious ) biased (bias in selecting data and/or annotator) privacy concerns (e.g. medical imaging data) limits learning to predefined categories e.g. all dogs are one category but later we might want to seperate dog breeds with transfer learning but this isnt great for that SSL Model Capacity Compared to SL, SSL benefits more from deeper models also benefits from abundance of unlabeled data Can benefit from training a very deep model (teacher model) and then compressing it into a smaller model (student model) this is done for categorical outputs from the teacher by training the students to mimic the output of the teacher network using something like KL divergence loss when the teacher outputs non categorical outputs such as embeddings other loss functions need to be used such as MSE (performs not that well) or clustering with cross entropy Pretext Tasks aka pseudo or proxy task Turn input to grayscale and make network colorize the image learns features detectors for things such as grass and sky that are often the same color Contrastive Learning Recent SSL technique Moco- Momentum Contrast Learning (CVPR 20) From a query image it makes two augmented ones (e.g. two different crops) The augmented ones are passed through two copies of the network (shared weights) to create two embeddings The network is trained to pull together these augmented pairs (positive pairs) In order to prevent the network from learning trivial embeddings, random images are passed through the network and the loss function is structured to push negative pairs away from the positive pairs A memory bank that remembers other negatives also helps","title":"Self-Supervised Learning"},{"location":"machine_learning/nn/ssl/#self-supervised-learning","text":"By forcing the model to learn a task defined in terms of input images, it forces the network to learn features which will be useful for downstream general visual recognition tasks later","title":"Self Supervised Learning"},{"location":"machine_learning/nn/ssl/#weakenss-of-supervised-learning","text":"Annotations: costly (e.g. medical imaging data) ambiguous (set of labels not always obvious ) biased (bias in selecting data and/or annotator) privacy concerns (e.g. medical imaging data) limits learning to predefined categories e.g. all dogs are one category but later we might want to seperate dog breeds with transfer learning but this isnt great for that","title":"Weakenss of Supervised Learning"},{"location":"machine_learning/nn/ssl/#ssl-model-capacity","text":"Compared to SL, SSL benefits more from deeper models also benefits from abundance of unlabeled data Can benefit from training a very deep model (teacher model) and then compressing it into a smaller model (student model) this is done for categorical outputs from the teacher by training the students to mimic the output of the teacher network using something like KL divergence loss when the teacher outputs non categorical outputs such as embeddings other loss functions need to be used such as MSE (performs not that well) or clustering with cross entropy","title":"SSL Model Capacity"},{"location":"machine_learning/nn/ssl/#pretext-tasks","text":"aka pseudo or proxy task Turn input to grayscale and make network colorize the image learns features detectors for things such as grass and sky that are often the same color","title":"Pretext Tasks"},{"location":"machine_learning/nn/ssl/#contrastive-learning","text":"Recent SSL technique Moco- Momentum Contrast Learning (CVPR 20) From a query image it makes two augmented ones (e.g. two different crops) The augmented ones are passed through two copies of the network (shared weights) to create two embeddings The network is trained to pull together these augmented pairs (positive pairs) In order to prevent the network from learning trivial embeddings, random images are passed through the network and the loss function is structured to push negative pairs away from the positive pairs A memory bank that remembers other negatives also helps","title":"Contrastive Learning"},{"location":"math/factorization/","text":"Matrix Factorization/Decomposition Method for reducing a matrix into its constituent parts Can simplify complex matrix operations to make more efficient on computers Intro for ML LU Decomposition For a square matrix A \\in \\mathbb{R}^{n \\times n} , A = LU L is the lower triangle matrix U is the upper triangle matrix Other variations exist like LUP decomposition that are more numerically stable and work for more matrices","title":"Matrix Factorization"},{"location":"math/factorization/#matrix-factorizationdecomposition","text":"Method for reducing a matrix into its constituent parts Can simplify complex matrix operations to make more efficient on computers Intro for ML","title":"Matrix Factorization/Decomposition"},{"location":"math/factorization/#lu-decomposition","text":"For a square matrix A \\in \\mathbb{R}^{n \\times n} , A = LU L is the lower triangle matrix U is the upper triangle matrix Other variations exist like LUP decomposition that are more numerically stable and work for more matrices","title":"LU Decomposition"},{"location":"math/math/","text":"Math for ML Inner Products aka dot products Given a euclidean plane with a line L passing through the origin and a unit vector w perpendicular to L (the normal to the line) If you take any vector x , then the dot product x \\cdot w > 0 if x is on the same side of L as w and negative otherwise ( =0 if its on the line) Inner product is a linear function (sum of scalar multiplication) so it can be optimized easily The dot product as a decision rule in any n dimensional space is useful for the Kernel Trick where nonlinearly separable data is trasnformed into a linearly separable space with no additional computational cost since all that is needed in the formulas to make a decision is the dot product Inner Product as Decision Rule Positive Definite Positive definition matrix A has all positive eigenvalues a positive definite matrix means that for a vector x in the first quadrant, applying the transformation to it Ax , would result in a vector still in the first quadrant in other words the maximum angle between the two transformed vectors is < 90 degrees ( \\pi /2 rad)","title":"Important Ideas"},{"location":"math/math/#math-for-ml","text":"","title":"Math for ML"},{"location":"math/math/#inner-products","text":"aka dot products Given a euclidean plane with a line L passing through the origin and a unit vector w perpendicular to L (the normal to the line) If you take any vector x , then the dot product x \\cdot w > 0 if x is on the same side of L as w and negative otherwise ( =0 if its on the line) Inner product is a linear function (sum of scalar multiplication) so it can be optimized easily The dot product as a decision rule in any n dimensional space is useful for the Kernel Trick where nonlinearly separable data is trasnformed into a linearly separable space with no additional computational cost since all that is needed in the formulas to make a decision is the dot product Inner Product as Decision Rule","title":"Inner Products"},{"location":"math/math/#positive-definite","text":"Positive definition matrix A has all positive eigenvalues a positive definite matrix means that for a vector x in the first quadrant, applying the transformation to it Ax , would result in a vector still in the first quadrant in other words the maximum angle between the two transformed vectors is < 90 degrees ( \\pi /2 rad)","title":"Positive Definite"},{"location":"optimization/convergence/","text":"Convergence Analysis We are interested in how fast the error goes to zero for a given iterative optimization method where \\lim_{k \\to \\infty} x_k = x^* . Define the error as e_k = ||x_k - x^*|| . Q-Rate Convergence Q-rate tells us how fast a sequence convergences to a value. x_k \\to x^* with q-rate r if there exists a constant c such that for any sufficiently large k : ||x_{k+1} - x^*|| \\leq c ||x_{k+1} -x_k||^r Also can be represented as ||e_{k+1}|| \\leq c||e_k||^r for sufficiently large k where e is the error btw the current point and optimal point at a given iteration. We can alternatively write that ||e_{k_1}|| = O(e_k) using big-o notation to represent asymptotic growth. Taking the negative logarithm of the error term will give us the number of decimal places of accuracy for that given iteration's approximation. For instance, let e_k = 10^{-6} , then the number of decimal places of accuracy is -\\log_{10}10^{-6} = 6 . We can take the negative logarithm of both sides of the above equation to get: -\\log_{10} e_{k+1} \\geq r (- \\log_{10} e_k) - \\log_{10} c This expression tells us how fast the number of decimal places of accuracy is increasing. Linear rate r = 1 and c < 1 Ex: 1, 1/2 1/4, 1/8, ... -> 0 for r = 1 and c = \\frac{1}{2} Superlinear r = 1 The constant c changes over time: c_k \\to 0 as k \\to 0 Quadratic r = 2 r is large enough that it swamps the effect of c no matter its value Ex: 10^{-1}, 10^{-2}, 10^{-4}, 10^{-8}, \\cdots \\to 0 for r= 2 and c = 1 Q-rate Limit Lemma Suppose r \\geq 1 and L exists where ( L < 1 if r = 1 ) L = \\lim_{k \\to \\infty} \\frac{||x_{k+1} - x^* ||}{||x_k - x^* ||^r} Then x_k \\to x^* with Q-rate r and a constant C > L R-Rate Convergence Tells us how fast the bounds on the error converges to 0. The error has a bounds that has a q-rate convergence x_k \\to x^* with r-rate r if ||e_k || \\leq b_k for all k where b_k \\to 0 with q-rate r Summary Linear: moderate to large c is slow small c is fast Superlinear: fast Quadratic: very fast","title":"Convergence"},{"location":"optimization/convergence/#convergence-analysis","text":"We are interested in how fast the error goes to zero for a given iterative optimization method where \\lim_{k \\to \\infty} x_k = x^* . Define the error as e_k = ||x_k - x^*|| .","title":"Convergence Analysis"},{"location":"optimization/convergence/#q-rate-convergence","text":"Q-rate tells us how fast a sequence convergences to a value. x_k \\to x^* with q-rate r if there exists a constant c such that for any sufficiently large k : ||x_{k+1} - x^*|| \\leq c ||x_{k+1} -x_k||^r Also can be represented as ||e_{k+1}|| \\leq c||e_k||^r for sufficiently large k where e is the error btw the current point and optimal point at a given iteration. We can alternatively write that ||e_{k_1}|| = O(e_k) using big-o notation to represent asymptotic growth. Taking the negative logarithm of the error term will give us the number of decimal places of accuracy for that given iteration's approximation. For instance, let e_k = 10^{-6} , then the number of decimal places of accuracy is -\\log_{10}10^{-6} = 6 . We can take the negative logarithm of both sides of the above equation to get: -\\log_{10} e_{k+1} \\geq r (- \\log_{10} e_k) - \\log_{10} c This expression tells us how fast the number of decimal places of accuracy is increasing.","title":"Q-Rate Convergence"},{"location":"optimization/convergence/#linear","text":"rate r = 1 and c < 1 Ex: 1, 1/2 1/4, 1/8, ... -> 0 for r = 1 and c = \\frac{1}{2}","title":"Linear"},{"location":"optimization/convergence/#superlinear","text":"r = 1 The constant c changes over time: c_k \\to 0 as k \\to 0","title":"Superlinear"},{"location":"optimization/convergence/#quadratic","text":"r = 2 r is large enough that it swamps the effect of c no matter its value Ex: 10^{-1}, 10^{-2}, 10^{-4}, 10^{-8}, \\cdots \\to 0 for r= 2 and c = 1","title":"Quadratic"},{"location":"optimization/convergence/#q-rate-limit-lemma","text":"Suppose r \\geq 1 and L exists where ( L < 1 if r = 1 ) L = \\lim_{k \\to \\infty} \\frac{||x_{k+1} - x^* ||}{||x_k - x^* ||^r} Then x_k \\to x^* with Q-rate r and a constant C > L","title":"Q-rate Limit Lemma"},{"location":"optimization/convergence/#r-rate-convergence","text":"Tells us how fast the bounds on the error converges to 0. The error has a bounds that has a q-rate convergence x_k \\to x^* with r-rate r if ||e_k || \\leq b_k for all k where b_k \\to 0 with q-rate r","title":"R-Rate Convergence"},{"location":"optimization/convergence/#summary","text":"Linear: moderate to large c is slow small c is fast Superlinear: fast Quadratic: very fast","title":"Summary"},{"location":"optimization/method-template/","text":"Method A brief overview including what info of the function is used (1st, 2nd, 3rd derivs, ) and any conditions Method Pros Cons","title":"Method"},{"location":"optimization/method-template/#method","text":"A brief overview including what info of the function is used (1st, 2nd, 3rd derivs, ) and any conditions","title":"Method"},{"location":"optimization/method-template/#method_1","text":"","title":"Method"},{"location":"optimization/method-template/#pros","text":"","title":"Pros"},{"location":"optimization/method-template/#cons","text":"","title":"Cons"},{"location":"optimization/mpc/","text":"Model Predictive Control Relies on a dynamic model to make predictions for future values of the controlled variables of the system. It then solves a contrained optimization problem to calculate the optimal control action that minimizes the difference between the predicted controlled values and the desired/set control values. Main limitation: High computational cost since this optimization problem must be solved at each time step The computational cost increases even more as the time horizon over which to predict future control values increases. The computational cost becomes intractable to run realtime as the number of states and contraints grows (NN approximation could fix this) Explicit MPC (eMPC) and learning-based NN approaches have been proposed to overcome this Takes on many forms: one-norm, infinite norm, PWA model, tracking Explicit MPC Goal is to rewrite the optimization problem as a multi-parametric quadtratic programming (mpQP) This is possible for linear systems but cannot be extended to nonlinear systems An explicit solution to the mpQP has been demonstrated and it is of the form of a PWA function The PWA function defined over the state space will act as a fast prediction/lookup function for control policies given a input state meaning there is no need to solve the optimization problem online! This result was achieved by Bemporad et al. (2002) Resources: ( Brief mention ) Optimality: Not always feasible to run the optimization process to complete convergence for the most optimal control output (especially when running online) Therefore, optimality is sometimes relaxed to only require a sufficient decrease in the cost/objective function Then the optimization problem would be terminated at a feasible but sub-optimal stage Recursive Feasability: MPC controllers are not gauranteed to be stabilizing (linear quadratic control is gauranted to be stabilizing) Stability must be built in when defining the optimization problem The optimization problem\u2019s feasbility can be lost when the state is driven to a region in which the optimization problem has no solution typical approach to solve this is to append constrainst to the optimization problem to guarantee that loss of feasability cannot occur (that is what was done in the papers I have read so far) When this is done, an MPC controller is said to be recursively feasible meaning it cannot put the state in a place with no solution to the optimization problem Constructing an MPC controller with the theoretical (a-priori) guarantee of recursive feasability is not always desirable or easy to do Sometimes we might have a situation where given an MPC controller and the goal is to determine if it is recursively feasbile The goal is primarly to invalidate the controller by detecting problematic states where the recursive feasability is lost Second objective is to find certificates of guaranteed recursive feasability DEF: The MPC controller is recursively feasible if and only if for all ini- tially feasible x0 and for all optimal sequences of control inputs the MPC optimization problem remains feasible for all time. DEF: The MPC controller is strongly recursively feasible if and only if for all initially feasible x0 and for all sequences of feasible control inputs the MPC optimization problem remains feasible for all time. Resource (analysis tool for determining recursive feasability of an MPC controller)","title":"Model Predictive Control"},{"location":"optimization/mpc/#model-predictive-control","text":"Relies on a dynamic model to make predictions for future values of the controlled variables of the system. It then solves a contrained optimization problem to calculate the optimal control action that minimizes the difference between the predicted controlled values and the desired/set control values. Main limitation: High computational cost since this optimization problem must be solved at each time step The computational cost increases even more as the time horizon over which to predict future control values increases. The computational cost becomes intractable to run realtime as the number of states and contraints grows (NN approximation could fix this) Explicit MPC (eMPC) and learning-based NN approaches have been proposed to overcome this Takes on many forms: one-norm, infinite norm, PWA model, tracking","title":"Model Predictive Control"},{"location":"optimization/mpc/#explicit-mpc","text":"Goal is to rewrite the optimization problem as a multi-parametric quadtratic programming (mpQP) This is possible for linear systems but cannot be extended to nonlinear systems An explicit solution to the mpQP has been demonstrated and it is of the form of a PWA function The PWA function defined over the state space will act as a fast prediction/lookup function for control policies given a input state meaning there is no need to solve the optimization problem online! This result was achieved by Bemporad et al. (2002) Resources: ( Brief mention )","title":"Explicit MPC"},{"location":"optimization/mpc/#optimality","text":"Not always feasible to run the optimization process to complete convergence for the most optimal control output (especially when running online) Therefore, optimality is sometimes relaxed to only require a sufficient decrease in the cost/objective function Then the optimization problem would be terminated at a feasible but sub-optimal stage","title":"Optimality:"},{"location":"optimization/mpc/#recursive-feasability","text":"MPC controllers are not gauranteed to be stabilizing (linear quadratic control is gauranted to be stabilizing) Stability must be built in when defining the optimization problem The optimization problem\u2019s feasbility can be lost when the state is driven to a region in which the optimization problem has no solution typical approach to solve this is to append constrainst to the optimization problem to guarantee that loss of feasability cannot occur (that is what was done in the papers I have read so far) When this is done, an MPC controller is said to be recursively feasible meaning it cannot put the state in a place with no solution to the optimization problem Constructing an MPC controller with the theoretical (a-priori) guarantee of recursive feasability is not always desirable or easy to do Sometimes we might have a situation where given an MPC controller and the goal is to determine if it is recursively feasbile The goal is primarly to invalidate the controller by detecting problematic states where the recursive feasability is lost Second objective is to find certificates of guaranteed recursive feasability DEF: The MPC controller is recursively feasible if and only if for all ini- tially feasible x0 and for all optimal sequences of control inputs the MPC optimization problem remains feasible for all time. DEF: The MPC controller is strongly recursively feasible if and only if for all initially feasible x0 and for all sequences of feasible control inputs the MPC optimization problem remains feasible for all time. Resource (analysis tool for determining recursive feasability of an MPC controller)","title":"Recursive Feasability:"},{"location":"optimization/overview/","text":"Overview Optimization is a huge field with a tremendous number of applications. It is also intertwined in ML so that is why its include in the ML notebook. Here is where I will try to keep a working glossary of areas I am being introduced to with a brief overview of what they are. Being able to map out how many pieces of a field fit together greatly helps me when diving deeper into any one area. What is Optimization? Optimizing simply stands for finding the maximum or minimum of a function. This max or min is often subject to a series of constraints. The form of the function being optimized and the constraints being adhered to (e.g. linear, nonlinear, continuous, etc.) determines the type of problem and thus the technique used to solve them. Here is the mathematical definition of an optimization problem: \\min f_0(x) subject to the contraints: f_i(x) \\leq b_i Here is the textbook on convex optimization used for reference. Linear Programming (LP) Optimizing a linear objective/cost function subject to linear equality/inequality constraints Nonlinear Programming (NLP) Optimizing an objective/cost function over a set of real variables subject to system of equations called constraints (more general class that includes LP) Quadtratic Programming (QP) Methods for optimizing a quadtratic objective function subject to linear constraints Sequential Programming Sequential Quadtratic Programming (SQP) Involves solving a series of subproblems where each is a quadratic program Sequential Linear Quadtratic Programming (SLQP) Involves solving a linear program and equality constrained quadratic program at each step Dynamic Programming (DP) Involves breaking down the main optimization problem into simpler sub-problems. The Bellman Equation describes the relationship between the value of the larger problem and the values of the sub-problems Piece Wise Affine Functions (PWAs) Approximates a function using a series of lines in 2D, planes in 3D, & hyperplanes in higher dimensional spaces. While the picture above shows the PWA given a known function, PWAs are most often used when there is a set of datapoints representing the functions inputs and outputs. The task is then to construct a PWA from the data such that it best approximates the true function that is mapping the inputs to the outputs. Once you have a PWA made from your dataset, you can then make predictions on new inputs as to what the true function would map to. Constraints A constraint is a condition of an optimization problem that the solution must satisfy Feasible Set - is the set of candidate solutions that satisfy all constraints A point is infeasible if it does not satisfy a constraint A constraint is binding if an inequality constraint holds with equality at the optimal point i.e. the point is at the edge of the feasible set and cannot move anymore in the direction of the constraint, even if doing so would improve the value of the objective function, since it would move outside the feasible set A constraint is non-binding if the point could be moved in the direction of the constraint i.e. the point is not at the edge of the feasible set and can be moved more in the direction of the constraint, however doing so would not be optimal This can mean under certain conditions that the optimization problem would have the same solution in the absence of the constraint since the objective function's minimum does not push up against the edge of the feasible set Slack Variable A variable added to an inequality constraint to transform it into an equality If the slack variable for a constraint is zero at a point, the constraint is binding there If the slack variable for a constraint is positive at a point, the constraint is non-binding there If the slack variable for a constraint is negative at a point, the point is infeasible as it does not satisfy the constraint (can be thought of as outside the feasible set) Slack variables can never be negative for most interior point and simplex solvers For example, introducing the slack variable y \\geq 0 , the inequality constraint of Ax \\leq b can be transformed into an equality of Ax + y = b Intuition: slack can be thought of like a distance from a candidate solution to the constraint boundary ( y = Ax - b ) A constraint of Ax > b will be transformed into two constraints of s = Ax - b and s > 0 that is used by most solvers Lagrange Multipliers KKT conditions generalize the method of lagrange multipliers and allow inequality constraints","title":"Overview"},{"location":"optimization/overview/#overview","text":"Optimization is a huge field with a tremendous number of applications. It is also intertwined in ML so that is why its include in the ML notebook. Here is where I will try to keep a working glossary of areas I am being introduced to with a brief overview of what they are. Being able to map out how many pieces of a field fit together greatly helps me when diving deeper into any one area.","title":"Overview"},{"location":"optimization/overview/#what-is-optimization","text":"Optimizing simply stands for finding the maximum or minimum of a function. This max or min is often subject to a series of constraints. The form of the function being optimized and the constraints being adhered to (e.g. linear, nonlinear, continuous, etc.) determines the type of problem and thus the technique used to solve them. Here is the mathematical definition of an optimization problem: \\min f_0(x) subject to the contraints: f_i(x) \\leq b_i Here is the textbook on convex optimization used for reference.","title":"What is Optimization?"},{"location":"optimization/overview/#linear-programming-lp","text":"Optimizing a linear objective/cost function subject to linear equality/inequality constraints","title":"Linear Programming (LP)"},{"location":"optimization/overview/#nonlinear-programming-nlp","text":"Optimizing an objective/cost function over a set of real variables subject to system of equations called constraints (more general class that includes LP)","title":"Nonlinear Programming (NLP)"},{"location":"optimization/overview/#quadtratic-programming-qp","text":"Methods for optimizing a quadtratic objective function subject to linear constraints","title":"Quadtratic Programming (QP)"},{"location":"optimization/overview/#sequential-programming","text":"","title":"Sequential Programming"},{"location":"optimization/overview/#sequential-quadtratic-programming-sqp","text":"Involves solving a series of subproblems where each is a quadratic program","title":"Sequential Quadtratic Programming (SQP)"},{"location":"optimization/overview/#sequential-linear-quadtratic-programming-slqp","text":"Involves solving a linear program and equality constrained quadratic program at each step","title":"Sequential Linear Quadtratic Programming (SLQP)"},{"location":"optimization/overview/#dynamic-programming-dp","text":"Involves breaking down the main optimization problem into simpler sub-problems. The Bellman Equation describes the relationship between the value of the larger problem and the values of the sub-problems","title":"Dynamic Programming (DP)"},{"location":"optimization/overview/#piece-wise-affine-functions-pwas","text":"Approximates a function using a series of lines in 2D, planes in 3D, & hyperplanes in higher dimensional spaces. While the picture above shows the PWA given a known function, PWAs are most often used when there is a set of datapoints representing the functions inputs and outputs. The task is then to construct a PWA from the data such that it best approximates the true function that is mapping the inputs to the outputs. Once you have a PWA made from your dataset, you can then make predictions on new inputs as to what the true function would map to.","title":"Piece Wise Affine Functions (PWAs)"},{"location":"optimization/overview/#constraints","text":"A constraint is a condition of an optimization problem that the solution must satisfy Feasible Set - is the set of candidate solutions that satisfy all constraints A point is infeasible if it does not satisfy a constraint A constraint is binding if an inequality constraint holds with equality at the optimal point i.e. the point is at the edge of the feasible set and cannot move anymore in the direction of the constraint, even if doing so would improve the value of the objective function, since it would move outside the feasible set A constraint is non-binding if the point could be moved in the direction of the constraint i.e. the point is not at the edge of the feasible set and can be moved more in the direction of the constraint, however doing so would not be optimal This can mean under certain conditions that the optimization problem would have the same solution in the absence of the constraint since the objective function's minimum does not push up against the edge of the feasible set","title":"Constraints"},{"location":"optimization/overview/#slack-variable","text":"A variable added to an inequality constraint to transform it into an equality If the slack variable for a constraint is zero at a point, the constraint is binding there If the slack variable for a constraint is positive at a point, the constraint is non-binding there If the slack variable for a constraint is negative at a point, the point is infeasible as it does not satisfy the constraint (can be thought of as outside the feasible set) Slack variables can never be negative for most interior point and simplex solvers For example, introducing the slack variable y \\geq 0 , the inequality constraint of Ax \\leq b can be transformed into an equality of Ax + y = b Intuition: slack can be thought of like a distance from a candidate solution to the constraint boundary ( y = Ax - b ) A constraint of Ax > b will be transformed into two constraints of s = Ax - b and s > 0 that is used by most solvers","title":"Slack Variable"},{"location":"optimization/overview/#lagrange-multipliers","text":"KKT conditions generalize the method of lagrange multipliers and allow inequality constraints","title":"Lagrange Multipliers"},{"location":"optimization/steepest-descent/","text":"Steepest Descent Used to find the minimizer of a function f Method Pros Cons","title":"Steepest Descent"},{"location":"optimization/steepest-descent/#steepest-descent","text":"Used to find the minimizer of a function f","title":"Steepest Descent"},{"location":"optimization/steepest-descent/#method","text":"","title":"Method"},{"location":"optimization/steepest-descent/#pros","text":"","title":"Pros"},{"location":"optimization/steepest-descent/#cons","text":"","title":"Cons"},{"location":"optimization/root-methods/bisection/","text":"Bisection Method Uses only g Uses only g = f' where f is an objective function in an optimization problem Roughly equivalent to binary search over a continuous domain instead of over a discerete list Method Choose a starting interval [a_0, b_0] such that g(a_0) g(b_0) < 0 (alternatively x^* \\in [a_0, b_0] where g(x^*) = 0 ) interval should contain the root different signs at the endpoints of the interval implies by the intermediate value theorem that a root exists in between them due to continuity of g Compute g(c_k) where c_k = \\frac{a_k + b_k}{2} (i.e. the midpoint of the interval) Determine the next subinterval [a_{k+1}, b_{k+1}] : If g(a_k) g(c_k) < 0 then a_{k+1} = a_k and b_{k+1} = c_k If g(b_k) g(c_k) < 0 then a_{k+1} = c_k and b_{k+1} = b_k Convergence Actual error is not gauranteed to go down in any iteration Only the bound on error is gauranteed to go down Therefore, we have R-linear convergence with c = 1/2 since the bound on the error is cut in half at each iteration Pros Requires no derivative information of g (only need f' of objective function) Reliable- guaranteed to converge in a predictable way given an initial interval that contains a root Can calculate the min number of iterations required to guarantee an error less than \\epsilon > 0 ( Link ) Cons Slow","title":"Bisection"},{"location":"optimization/root-methods/bisection/#bisection-method","text":"Uses only g Uses only g = f' where f is an objective function in an optimization problem Roughly equivalent to binary search over a continuous domain instead of over a discerete list","title":"Bisection Method"},{"location":"optimization/root-methods/bisection/#method","text":"Choose a starting interval [a_0, b_0] such that g(a_0) g(b_0) < 0 (alternatively x^* \\in [a_0, b_0] where g(x^*) = 0 ) interval should contain the root different signs at the endpoints of the interval implies by the intermediate value theorem that a root exists in between them due to continuity of g Compute g(c_k) where c_k = \\frac{a_k + b_k}{2} (i.e. the midpoint of the interval) Determine the next subinterval [a_{k+1}, b_{k+1}] : If g(a_k) g(c_k) < 0 then a_{k+1} = a_k and b_{k+1} = c_k If g(b_k) g(c_k) < 0 then a_{k+1} = c_k and b_{k+1} = b_k","title":"Method"},{"location":"optimization/root-methods/bisection/#convergence","text":"Actual error is not gauranteed to go down in any iteration Only the bound on error is gauranteed to go down Therefore, we have R-linear convergence with c = 1/2 since the bound on the error is cut in half at each iteration","title":"Convergence"},{"location":"optimization/root-methods/bisection/#pros","text":"Requires no derivative information of g (only need f' of objective function) Reliable- guaranteed to converge in a predictable way given an initial interval that contains a root Can calculate the min number of iterations required to guarantee an error less than \\epsilon > 0 ( Link )","title":"Pros"},{"location":"optimization/root-methods/bisection/#cons","text":"Slow","title":"Cons"},{"location":"optimization/root-methods/newtons/","text":"Newton's Method Finds the root of a function g i.e. finds x such that g(x) = 0 Uses g and g' In the context of optimization where g = f' , we would need first and second derivatives of the cost function f Method Given x_k we will approximate g(x) by the tangent line l_k(x) at x_k . This tangent line is the 1st order taylor polynomial of g at x_k and represents a linear approximation of g . l_k(k) = g(x_k) + g'(x_k)(x - x_k) If we let x_{k+1} solve l_k(x) = 0 we get the equation for the next step in the iterative process of finding the root: x_{k + 1} = x_k - \\frac{g(x_k)}{g'(x_k)} Convergence Q-quadratic convergence The constant C > \\frac{g''(x^*)}{2g'(x^*)} This tells us that that curvature of g around x^* in a scale invariant way Less curvature -> smaller C -> faster convergence. Pros Very fast Cons May diverge or converge to something unexpected Can get misdirected when g' is small (i.e. g is flat) when the cost function f doesn't change very rapidly, g will be flat and newton's method will not perform well due to g'(x_k) \\neq 0 in the denominator of the update step Requires g' which is the second derivative of the cost function in an optimization context, so this can be unfeasible for certain problems or computationally expensive to compute Other Notes Since we are using a linear approximation of g to find its roots, the actual curvature of g affects how fast Newton's method converges. For low curvature, the method is very fast.","title":"Newtons Method"},{"location":"optimization/root-methods/newtons/#newtons-method","text":"Finds the root of a function g i.e. finds x such that g(x) = 0 Uses g and g' In the context of optimization where g = f' , we would need first and second derivatives of the cost function f","title":"Newton's Method"},{"location":"optimization/root-methods/newtons/#method","text":"Given x_k we will approximate g(x) by the tangent line l_k(x) at x_k . This tangent line is the 1st order taylor polynomial of g at x_k and represents a linear approximation of g . l_k(k) = g(x_k) + g'(x_k)(x - x_k) If we let x_{k+1} solve l_k(x) = 0 we get the equation for the next step in the iterative process of finding the root: x_{k + 1} = x_k - \\frac{g(x_k)}{g'(x_k)}","title":"Method"},{"location":"optimization/root-methods/newtons/#convergence","text":"Q-quadratic convergence The constant C > \\frac{g''(x^*)}{2g'(x^*)} This tells us that that curvature of g around x^* in a scale invariant way Less curvature -> smaller C -> faster convergence.","title":"Convergence"},{"location":"optimization/root-methods/newtons/#pros","text":"Very fast","title":"Pros"},{"location":"optimization/root-methods/newtons/#cons","text":"May diverge or converge to something unexpected Can get misdirected when g' is small (i.e. g is flat) when the cost function f doesn't change very rapidly, g will be flat and newton's method will not perform well due to g'(x_k) \\neq 0 in the denominator of the update step Requires g' which is the second derivative of the cost function in an optimization context, so this can be unfeasible for certain problems or computationally expensive to compute","title":"Cons"},{"location":"optimization/root-methods/newtons/#other-notes","text":"Since we are using a linear approximation of g to find its roots, the actual curvature of g affects how fast Newton's method converges. For low curvature, the method is very fast.","title":"Other Notes"},{"location":"optimization/root-methods/regula_falsi/","text":"Regula Falsi Known as the method of false position Only uses g Bracketing method like Bisection to guaranteed to converge Method Construct a secant line using the the two interval endpoints Approximates g l_k(x) = g(a_k) + \\frac{g(b_k) - g(a_k)}{b_k - a_k}(x - a_k) Find the root of the line c_k = \\frac{a_k g(b_k) - b_k g(a_k)}{g(b_k) - g(a_k)} Determine the next subinterval [a_{k+1}, b_{k+1}] : If g(a_k) g(c_k) < 0 then a_{k+1} = a_k and b_{k+1} = c_k If g(b_k) g(c_k) < 0 then a_{k+1} = c_k and b_{k+1} = b_k Convergence Q-Linear convergence c depends on the curvature of g and how close a_0 is to x^* Since we are using a linear approximation of g , less curvature = faster convergence Pros Reliable given the interval encloses x^* Cons Slow convergence with a constant that depends on the curvature of g","title":"Regula Falsi"},{"location":"optimization/root-methods/regula_falsi/#regula-falsi","text":"Known as the method of false position Only uses g Bracketing method like Bisection to guaranteed to converge","title":"Regula Falsi"},{"location":"optimization/root-methods/regula_falsi/#method","text":"Construct a secant line using the the two interval endpoints Approximates g l_k(x) = g(a_k) + \\frac{g(b_k) - g(a_k)}{b_k - a_k}(x - a_k) Find the root of the line c_k = \\frac{a_k g(b_k) - b_k g(a_k)}{g(b_k) - g(a_k)} Determine the next subinterval [a_{k+1}, b_{k+1}] : If g(a_k) g(c_k) < 0 then a_{k+1} = a_k and b_{k+1} = c_k If g(b_k) g(c_k) < 0 then a_{k+1} = c_k and b_{k+1} = b_k","title":"Method"},{"location":"optimization/root-methods/regula_falsi/#convergence","text":"Q-Linear convergence c depends on the curvature of g and how close a_0 is to x^* Since we are using a linear approximation of g , less curvature = faster convergence","title":"Convergence"},{"location":"optimization/root-methods/regula_falsi/#pros","text":"Reliable given the interval encloses x^*","title":"Pros"},{"location":"optimization/root-methods/regula_falsi/#cons","text":"Slow convergence with a constant that depends on the curvature of g","title":"Cons"},{"location":"optimization/root-methods/secant/","text":"Secant Method Finds the root of a function g i.e. finds x such that g(x) = 0 Uses only g Method Use the two most recent points to construct a secant line l_k(x) Compute the root of the secant line to find the next point x_{k+1} = \\frac{x_{k-1}g(x_k) - x_kg(x_{k-1})}{g(x_k) - g(x_{k-1})} equivalent to x_{k+1} = x_k - \\frac{g(x_k)(x_{k-1} - x_k)}{g(x_k) - g(x_{k-1})} numerically less likely to lose preceision Convergence Superlinear convergence with a q-rate of 1.618 (golden ratio) Slower than newton's but faster than bisection and regula falls The constant C > \\frac{g''(x^*)}{2g'(x^*)} This tells us that that curvature of g around x^* in a scale invariant way Less curvature -> smaller C -> faster convergence. Pros Relatively fast Doesn't require g' Cons May diverge or converge to something unexpected Other Notes Since we are using a linear approximation of g to find its roots, the actual curvature of g affects how fast Newton's method converges. For low curvature, the method is very fast.","title":"Secant Method"},{"location":"optimization/root-methods/secant/#secant-method","text":"Finds the root of a function g i.e. finds x such that g(x) = 0 Uses only g","title":"Secant Method"},{"location":"optimization/root-methods/secant/#method","text":"Use the two most recent points to construct a secant line l_k(x) Compute the root of the secant line to find the next point x_{k+1} = \\frac{x_{k-1}g(x_k) - x_kg(x_{k-1})}{g(x_k) - g(x_{k-1})} equivalent to x_{k+1} = x_k - \\frac{g(x_k)(x_{k-1} - x_k)}{g(x_k) - g(x_{k-1})} numerically less likely to lose preceision","title":"Method"},{"location":"optimization/root-methods/secant/#convergence","text":"Superlinear convergence with a q-rate of 1.618 (golden ratio) Slower than newton's but faster than bisection and regula falls The constant C > \\frac{g''(x^*)}{2g'(x^*)} This tells us that that curvature of g around x^* in a scale invariant way Less curvature -> smaller C -> faster convergence.","title":"Convergence"},{"location":"optimization/root-methods/secant/#pros","text":"Relatively fast Doesn't require g'","title":"Pros"},{"location":"optimization/root-methods/secant/#cons","text":"May diverge or converge to something unexpected","title":"Cons"},{"location":"optimization/root-methods/secant/#other-notes","text":"Since we are using a linear approximation of g to find its roots, the actual curvature of g affects how fast Newton's method converges. For low curvature, the method is very fast.","title":"Other Notes"}]}