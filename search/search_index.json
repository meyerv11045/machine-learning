{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Machine Learning Notebook","title":"Home"},{"location":"#machine-learning-notebook","text":"","title":"Machine Learning Notebook"},{"location":"svms/","text":"Kernel Methods Attributes - original input values Features - new set of quantities from the attributes A feature map \\phi maps attributes to features GD update becomes computationally expensive when the features are high dimensional since its computing \\theta := \\theta - \\alpha \\sum_{i=1}^n (y^{(i)} - \\theta^T \\phi(x^{(i)}))\\phi(x^{(i)}) Kernel Trick Initialize \\theta = 0 Theta can be represented as a linear combination of vectors \\theta = \\sum_{i=1}^n \\beta_i \\phi(x^{(i)}) radial basis function kernel when || x - z||^2 is large, the value of the kernel is small far away -> small kernel values","title":"Kernel Methods"},{"location":"svms/#kernel-methods","text":"Attributes - original input values Features - new set of quantities from the attributes A feature map \\phi maps attributes to features GD update becomes computationally expensive when the features are high dimensional since its computing \\theta := \\theta - \\alpha \\sum_{i=1}^n (y^{(i)} - \\theta^T \\phi(x^{(i)}))\\phi(x^{(i)})","title":"Kernel Methods"},{"location":"svms/#kernel-trick","text":"Initialize \\theta = 0 Theta can be represented as a linear combination of vectors \\theta = \\sum_{i=1}^n \\beta_i \\phi(x^{(i)})","title":"Kernel Trick"},{"location":"svms/#radial-basis-function-kernel","text":"when || x - z||^2 is large, the value of the kernel is small far away -> small kernel values","title":"radial basis function kernel"},{"location":"machine_learning/classification/","text":"Classification Logistic Regression Gaussian Discriminant Analysis Naive Bayes Features are discrete unlike the continue features in Gaussian Discriminant Analysis Ex: dictionary of words showing up in an email to classify whether it is spam mispelled words in spam messages to try to get past these dictionaries of possible spam values the feature vector of words that show up in the desired dictionary is considered a bernoulli event model a multinomial event model takes into account the structure of the sentence or how the words appear in order to help prevent stuffing an email with a bunch of hidden good words that help the email get past the spam filter using a bernoulli event model naive bayes is no longer valid the feature vector will now depend on email length laplace mothing based on dictionary size instead of the size of the feature vector Support Vector Machines","title":"Classification"},{"location":"machine_learning/classification/#classification","text":"","title":"Classification"},{"location":"machine_learning/classification/#logistic-regression","text":"","title":"Logistic Regression"},{"location":"machine_learning/classification/#gaussian-discriminant-analysis","text":"","title":"Gaussian Discriminant Analysis"},{"location":"machine_learning/classification/#naive-bayes","text":"Features are discrete unlike the continue features in Gaussian Discriminant Analysis Ex: dictionary of words showing up in an email to classify whether it is spam mispelled words in spam messages to try to get past these dictionaries of possible spam values the feature vector of words that show up in the desired dictionary is considered a bernoulli event model a multinomial event model takes into account the structure of the sentence or how the words appear in order to help prevent stuffing an email with a bunch of hidden good words that help the email get past the spam filter using a bernoulli event model naive bayes is no longer valid the feature vector will now depend on email length laplace mothing based on dictionary size instead of the size of the feature vector","title":"Naive Bayes"},{"location":"machine_learning/classification/#support-vector-machines","text":"","title":"Support Vector Machines"},{"location":"machine_learning/feature_scaling/","text":"Feature Scaling Datasets can have features with varying magnitudes, ranges, and units Pre-processing the features can improve model performance decision tree based algorithms are invariant to feature scaling due to making decisions at each node based on a single feature Use when optimizing using gradient desecent (e.g. regressions, NNs) feature value in the cost function derivative and gradient step will cause different step sizes for each feature due to the difference in feature ranges scale the data to ensure params are updated at the same rate for all features similarly scaled features can make it converge faster applying distance based algorithms (e.g. KNN, SVMs) using distance btw data points as a measure of similarity will bias the measure towards higher magnitude features scaling data ensures unbiased measure of similarity Normalization Values are shifted and rescaled to the range [0,1] aka min-max scaling X' = \\frac{X - X_{min}}{X_{max} -X_{min}} where X_{min} and X_{max} are the min and max values of the feature more affected by outliers from sklearn.preprocessing import MinMaxScaler scale = MinMaxScaler().fit(x_train) std_x_train = scale.transform(x_train) std_x_test = scale.transform(x_test) #can also compute the fit and then transform in one step std_x_train = MinMaxScaler().fit_transform(x_train) Use when data is not gaussianly distributed useful in algorithms that don't assume any distribution of the data (KNN, neural nets) Standardization Values are centered around the mean with a unit standard deviation new mean of feature is 0 and new standard deviation is 1 X' = \\frac{X - \\mu}{\\sigma} where \\mu is the mean of that feature's values and \\sigma is the standard deviation values are not restricted to a particular range more robust to outliers from sklearn.preprocessing import StandardScaler scale = StandardScaler().fit(x_train) std_x_train = scale.transform(x_train) std_x_test = scale.transform(x_test) Use when data is gaussianly distributed can be useful in some cases where this isn't true Notes Normalization vs. Standardization depends on the problem and the learning algorithm being used Best practice to scale the training data and use the same values to scale the test set Don't need to scale the target values normally Resources Analytics Vidhya Machine Learning Mastery Scikit-Learn Preprocessing Docs","title":"Feature Scaling"},{"location":"machine_learning/feature_scaling/#feature-scaling","text":"Datasets can have features with varying magnitudes, ranges, and units Pre-processing the features can improve model performance decision tree based algorithms are invariant to feature scaling due to making decisions at each node based on a single feature","title":"Feature Scaling"},{"location":"machine_learning/feature_scaling/#use-when","text":"optimizing using gradient desecent (e.g. regressions, NNs) feature value in the cost function derivative and gradient step will cause different step sizes for each feature due to the difference in feature ranges scale the data to ensure params are updated at the same rate for all features similarly scaled features can make it converge faster applying distance based algorithms (e.g. KNN, SVMs) using distance btw data points as a measure of similarity will bias the measure towards higher magnitude features scaling data ensures unbiased measure of similarity","title":"Use when"},{"location":"machine_learning/feature_scaling/#normalization","text":"Values are shifted and rescaled to the range [0,1] aka min-max scaling X' = \\frac{X - X_{min}}{X_{max} -X_{min}} where X_{min} and X_{max} are the min and max values of the feature more affected by outliers from sklearn.preprocessing import MinMaxScaler scale = MinMaxScaler().fit(x_train) std_x_train = scale.transform(x_train) std_x_test = scale.transform(x_test) #can also compute the fit and then transform in one step std_x_train = MinMaxScaler().fit_transform(x_train)","title":"Normalization"},{"location":"machine_learning/feature_scaling/#use-when_1","text":"data is not gaussianly distributed useful in algorithms that don't assume any distribution of the data (KNN, neural nets)","title":"Use when"},{"location":"machine_learning/feature_scaling/#standardization","text":"Values are centered around the mean with a unit standard deviation new mean of feature is 0 and new standard deviation is 1 X' = \\frac{X - \\mu}{\\sigma} where \\mu is the mean of that feature's values and \\sigma is the standard deviation values are not restricted to a particular range more robust to outliers from sklearn.preprocessing import StandardScaler scale = StandardScaler().fit(x_train) std_x_train = scale.transform(x_train) std_x_test = scale.transform(x_test)","title":"Standardization"},{"location":"machine_learning/feature_scaling/#use-when_2","text":"data is gaussianly distributed can be useful in some cases where this isn't true","title":"Use when"},{"location":"machine_learning/feature_scaling/#notes","text":"Normalization vs. Standardization depends on the problem and the learning algorithm being used Best practice to scale the training data and use the same values to scale the test set Don't need to scale the target values normally","title":"Notes"},{"location":"machine_learning/feature_scaling/#resources","text":"Analytics Vidhya Machine Learning Mastery Scikit-Learn Preprocessing Docs","title":"Resources"},{"location":"machine_learning/model_selection/","text":"Model Selection Learning algorithms have many different hyperparameters that can be tuned We want to select the hyperparameters that lead to the best models We utilize algorithms to perform an optimizing search in the space of possible models to determine the bset one Given a set of models M = \\{M_1, \\dots \\} Cross Validation Hold-Out (Simple) Hold-out cross validation (aka simple cross validation): Randomly slit the train set S into a S_{train} and S_{cv} (the hold-out cross validation set) Train each model M_i on S_{train} to get a hypothesis h_i Select h_i with the smallest error on the hold out cross validation set S_{cv} Can optionally retrain the best model h_i on the entire train set S Even then, we are still selecting best model based on 0.8m training examples rather than m Wastes some of the dataset on holding out- bad for situations with scare data (e.g. m =20 ) k-fold Holds out less data compared to simple cv Randomly split train set S into k disjoint subsets S_1, \\dots, S_k of m / k training examples each For each model M_i evaluate: for each of the k folds, hold out S_j and train the model on all the other folds test the hypothesis h_{ij} on S_j to get the error the estimated generalization error of model M_i is the average of the errors over each if the k tests Pick the model M_i with the lowest estimated generalization error and retrain the model on the entire training set S . The resulting hypothesis is our final output Typical choice is k =10 Computationally more expensive than hold-out since we need to train each model k times leave-one-out Useful for when data is scare Perform k-fold cross validation where k = m in order to leave out as little data as possible each time Holds out one training example each time and then averages together the resulting m = k errors to get an estimate for the generalization error of a model Feature Selection If n >> m , its best to reduce the number of features to learn from Reducing the number of input variables to produce simpler models simpler models = more explainable since decisions are made based on less features Reduces overfitting Reduces training time/compute time For n features, there are 2^n possible feature subsets Each of the n features can be included or excluded from the subset Feature selection can be viewed as a model selection problem over 2^n possible models For large n , its too expensive to compare all 2^n models explicitly Therefore, a heuristic search procedure is used to find a good feature subset Wrapper Procedure that wraps your learning algorithm Dependent on the algorithm you are selecting features for Different learning algorithms may have different best features from the same wrapper algorithm More computationally expensive O(n^2) calls to learning algorithm Forward Search Makes assumption that best answer is a small set of features, likely those that come first Order the features are specified in matters Unable to remove feature Define feature set F = \\empty to be the indices of the features that matter Repeat until |F| = n or |F| > max features: For each feature i = 1, \\dots, n : train a model using F \\cup \\{i\\} measure the generalization using some version of cross validation Set F to be the best feature subset found in the previous step Backward Search Makes assumption that best answer is a near full set of the features Spends most time checking large subsets Cannot reinstate removed features Define feature set F = \\{1, \\dots, n \\} Repeat until |F| = \\empty : For each feature i = 1, \\dots, n : train a model using F \\setminus \\{i\\} measure the generalization using some version of cross validation Set F to be the best feature subset found in the previous step Filter Computationally cheaper than wrapper method Compute a score on the data that measures the effectiveness Indepedent of the underyling learning algorithm in determining most important features Define some score S(j) which measure how informative feature j is for the output Choose the features with the largest scores This heuristic for selecting intital features can be used to select an initial set of features to use with forward and backward search Example heuristics: mutual infromation absolute value of the correletion Mutual Information Common scoring heuristic (especially for discrete-valued features) Can be expressed as Kullback-Leibler (KL) divergence which measures the difference between two probability distributions Resources Feature Selection (ML Mastery)","title":"Model Selection"},{"location":"machine_learning/model_selection/#model-selection","text":"Learning algorithms have many different hyperparameters that can be tuned We want to select the hyperparameters that lead to the best models We utilize algorithms to perform an optimizing search in the space of possible models to determine the bset one Given a set of models M = \\{M_1, \\dots \\}","title":"Model Selection"},{"location":"machine_learning/model_selection/#cross-validation","text":"","title":"Cross Validation"},{"location":"machine_learning/model_selection/#hold-out-simple","text":"Hold-out cross validation (aka simple cross validation): Randomly slit the train set S into a S_{train} and S_{cv} (the hold-out cross validation set) Train each model M_i on S_{train} to get a hypothesis h_i Select h_i with the smallest error on the hold out cross validation set S_{cv} Can optionally retrain the best model h_i on the entire train set S Even then, we are still selecting best model based on 0.8m training examples rather than m Wastes some of the dataset on holding out- bad for situations with scare data (e.g. m =20 )","title":"Hold-Out (Simple)"},{"location":"machine_learning/model_selection/#k-fold","text":"Holds out less data compared to simple cv Randomly split train set S into k disjoint subsets S_1, \\dots, S_k of m / k training examples each For each model M_i evaluate: for each of the k folds, hold out S_j and train the model on all the other folds test the hypothesis h_{ij} on S_j to get the error the estimated generalization error of model M_i is the average of the errors over each if the k tests Pick the model M_i with the lowest estimated generalization error and retrain the model on the entire training set S . The resulting hypothesis is our final output Typical choice is k =10 Computationally more expensive than hold-out since we need to train each model k times","title":"k-fold"},{"location":"machine_learning/model_selection/#leave-one-out","text":"Useful for when data is scare Perform k-fold cross validation where k = m in order to leave out as little data as possible each time Holds out one training example each time and then averages together the resulting m = k errors to get an estimate for the generalization error of a model","title":"leave-one-out"},{"location":"machine_learning/model_selection/#feature-selection","text":"If n >> m , its best to reduce the number of features to learn from Reducing the number of input variables to produce simpler models simpler models = more explainable since decisions are made based on less features Reduces overfitting Reduces training time/compute time For n features, there are 2^n possible feature subsets Each of the n features can be included or excluded from the subset Feature selection can be viewed as a model selection problem over 2^n possible models For large n , its too expensive to compare all 2^n models explicitly Therefore, a heuristic search procedure is used to find a good feature subset","title":"Feature Selection"},{"location":"machine_learning/model_selection/#wrapper","text":"Procedure that wraps your learning algorithm Dependent on the algorithm you are selecting features for Different learning algorithms may have different best features from the same wrapper algorithm More computationally expensive O(n^2) calls to learning algorithm","title":"Wrapper"},{"location":"machine_learning/model_selection/#forward-search","text":"Makes assumption that best answer is a small set of features, likely those that come first Order the features are specified in matters Unable to remove feature Define feature set F = \\empty to be the indices of the features that matter Repeat until |F| = n or |F| > max features: For each feature i = 1, \\dots, n : train a model using F \\cup \\{i\\} measure the generalization using some version of cross validation Set F to be the best feature subset found in the previous step","title":"Forward Search"},{"location":"machine_learning/model_selection/#backward-search","text":"Makes assumption that best answer is a near full set of the features Spends most time checking large subsets Cannot reinstate removed features Define feature set F = \\{1, \\dots, n \\} Repeat until |F| = \\empty : For each feature i = 1, \\dots, n : train a model using F \\setminus \\{i\\} measure the generalization using some version of cross validation Set F to be the best feature subset found in the previous step","title":"Backward Search"},{"location":"machine_learning/model_selection/#filter","text":"Computationally cheaper than wrapper method Compute a score on the data that measures the effectiveness Indepedent of the underyling learning algorithm in determining most important features Define some score S(j) which measure how informative feature j is for the output Choose the features with the largest scores This heuristic for selecting intital features can be used to select an initial set of features to use with forward and backward search Example heuristics: mutual infromation absolute value of the correletion","title":"Filter"},{"location":"machine_learning/model_selection/#mutual-information","text":"Common scoring heuristic (especially for discrete-valued features) Can be expressed as Kullback-Leibler (KL) divergence which measures the difference between two probability distributions","title":"Mutual Information"},{"location":"machine_learning/model_selection/#resources","text":"Feature Selection (ML Mastery)","title":"Resources"},{"location":"machine_learning/optimization/","text":"Optimization Techniques Gradient Descent Optimization technique Stochastic : cost function for single datapoint before doing a model parameter update take step for each datapoint in dataset may get closer to minimum faster than batch but might never converge and just keep oscillating around the minmum (usually approximations of the minimum are good enough) adaptive learning rate that decreases over time will ensure params converge and dont oscillate around the min preferred when the training set is large Batch : cost function over the entire dataset before doing a model parameter update scan entire dataset before taking step For convex cost functions, gradient descent always converges to the global minimum assuming learning rate is not too large","title":"Optimization Techniques"},{"location":"machine_learning/optimization/#optimization-techniques","text":"","title":"Optimization Techniques"},{"location":"machine_learning/optimization/#gradient-descent","text":"Optimization technique Stochastic : cost function for single datapoint before doing a model parameter update take step for each datapoint in dataset may get closer to minimum faster than batch but might never converge and just keep oscillating around the minmum (usually approximations of the minimum are good enough) adaptive learning rate that decreases over time will ensure params converge and dont oscillate around the min preferred when the training set is large Batch : cost function over the entire dataset before doing a model parameter update scan entire dataset before taking step For convex cost functions, gradient descent always converges to the global minimum assuming learning rate is not too large","title":"Gradient Descent"},{"location":"machine_learning/regression/","text":"Regression Simple Linear Regression Multiple/Polynomial Linear Regression Scikit Learn Provides two approaches LinearRegression object- uses ordinary least squares solver from scipy to compite the closed form solution If enough memory for the matrices and inversions, this method is faster and easier SGDRegressor object- generic implementation of stochastic gradient descent so must set loss to L2 for linear regression penality to none for linear regression or L2 for ridge regression (this is the regularization mode) behaves better if loss function can be decomposed into additive terms Probabilistic Linear Regression Locally Weighted Linear Regression","title":"Regression"},{"location":"machine_learning/regression/#regression","text":"","title":"Regression"},{"location":"machine_learning/regression/#simple-linear-regression","text":"","title":"Simple Linear Regression"},{"location":"machine_learning/regression/#multiplepolynomial-linear-regression","text":"","title":"Multiple/Polynomial Linear Regression"},{"location":"machine_learning/regression/#scikit-learn","text":"Provides two approaches LinearRegression object- uses ordinary least squares solver from scipy to compite the closed form solution If enough memory for the matrices and inversions, this method is faster and easier SGDRegressor object- generic implementation of stochastic gradient descent so must set loss to L2 for linear regression penality to none for linear regression or L2 for ridge regression (this is the regularization mode) behaves better if loss function can be decomposed into additive terms","title":"Scikit Learn"},{"location":"machine_learning/regression/#probabilistic-linear-regression","text":"","title":"Probabilistic Linear Regression"},{"location":"machine_learning/regression/#locally-weighted-linear-regression","text":"","title":"Locally Weighted Linear Regression"},{"location":"machine_learning/reinforcement_learning/","text":"Reinforcement Learning The science of learning to make decisions from interaction Differs from other machine learning paradigms in that: No supervision, only a reward signal Feedback can be delayed, not instantaneous Time matters Earlier decisions affect later interactions Sequential decision problems can be solved w/ RL RL is a framework for how to think about these problems as well as a set of algorithms that can be used to solved these problems Core Concepts: Environment Reward Signal External to the learning algorithm even if its internal to the learning system as a whole Agent containing: Agent state Policy Value function (probably) Model (optionally) Rewards A reward R_t is a scalar feedback signal Indicates how well agent is doing at step t The agent's job is to maximize cumulative reward G_t = R_{t+1} + R_{t+2} + ... RL is based on the reward hypothesis Any goal can be formalized as the outcome of maximizing a cumulative reward Dense- reward on every step Sparse- reward only when the desired event happens Can be negative or positive rewads (Carrot vs. Stick in helping train the algorithm) Values Value - the expected cumulative reward from state s States use the v function to find their value v(s) = E[G_t | S_t = s] where G_t = R_{t+1} + R_{t+2} + ... Goal is to maximize value by picking suitable actions Rewards and values define desirability of state or acton Returns and values can be defined recursively G_t = R_{t+1} + G_{t+1} Actions Goal: Select actions to maximize value Actions may have long term consequences meaning rewards may be delayed Might be better to sacrific immediate reward to gain more long-term reward Ex: Financial investments might take months to mature Blocking opponent moves might help winning chances many moves in the future Policy - a mapping from states to actions Action Values It is possible to condition the value on actions q(s,a) = E[G_t | S_t = s, A_t = a] State action pairs use the Q-function to find their value Agent Components Agent State Policy Value Function Model State Actions depend on the state of the agent Both agent and environment may have an internal state In the simplest case, there is only one shared state by the environemnt and agent Often there are multiple (sometimes infinite) different states The agent's state is usually different than the environment's state The agent might not know the full state of the environment Environment State The environment's internal state Usually not visible to the agent May contain lots of irrelevant information if visible Agent State A history is a sequence of observations, action, rewards H_t = O_0,A_0,R_1,O_1,...,O_{t-1},A_{t-1},R_t,O_t The agent state S_t can be constructed from the history and is considerd to be a function of the history S_{t+1} = f(S_t,A_t,R_{t+1},O_{t+1}) where f is a state update function Means the next agent state is dependent on the current agent state, the action taken, the reward, and the observation A simple update function can be concatinating the previous states together to create the current state (Done w/ the Atari game agents by cocatenating frames together since frames are the state) Actions depend on this state Agent state is typically much smaller than the environment state and the full history Fully Observable Environments When the agent sees the full environment state observation = environment state Agent state could just be the observation S_t = O_t = environment state Then the agent is in a Markov Decision Process Ex: Single player board games in which the player can see the entire board Smart Pong is an example of fully observable environment (VERIFY) Markov Decision Processes MDPs provide a useful mathematical framework for talking about many RL topics Easier to reason about than the full problem (long markovian) Limited b/c of markov assumption Def- A decision process is Markov if the future is independent of the past given the present H_t => S_t => H_{t+1} Once the current state ( S_t ) is known , the history ( H_t ) may be thrown away (useful for space consideration) The current state gives enough information for the next state to be predicted The environment state is typically Markov The history is Markov Partially Observable Environments The agent gets parial information A robot w/ camera vision doesn't have its absolute location A poker playing agent only observes public cards Now the observation is not Markov Formally this is a partially observable Markov decision process (POMDP) The environment state can still be markov, but the agent does not know it The state should contain enough information for good policies/value predictions Policy Defines the agent's behavior A map from agent state to action Deterministic Policy - function that outputs an action based on an input agent state Stochastic Policy - a probability of selecting each action in each state Value Function: The actual value function is the expected return V_\\pi(s)=E[G_t|S_t=s,\\pi] = E[R_{t+1}+\\gamma R_{t+2}+\\gamma^2R_{t+3}+... | S_t = s,\\pi] The return has a recursive form since G_t = R_{t+1}+\\gamma G_{t+1} Bellman Equation : V_\\pi (s) = E[R_{t+1} + \\gamma G_{t+1} | S_t = s,A_t \\sim \\pi (s)] where a \\sim \\pi (s) means action a is chosen by policy \\pi in state s Discount Factor ( \\gamma \\in [0,1] ) trades off importance of immediate vs. long-term rewards Closer to 1 means higher importane on immediate rewards (AKA discounting future rewards in favor of immediate rewards) The value depends on the policy Can be used to evaluate desirability of states Can be used to select btw actions","title":"Reinforcement Learning"},{"location":"machine_learning/reinforcement_learning/#reinforcement-learning","text":"The science of learning to make decisions from interaction Differs from other machine learning paradigms in that: No supervision, only a reward signal Feedback can be delayed, not instantaneous Time matters Earlier decisions affect later interactions Sequential decision problems can be solved w/ RL RL is a framework for how to think about these problems as well as a set of algorithms that can be used to solved these problems","title":"Reinforcement Learning"},{"location":"machine_learning/reinforcement_learning/#core-concepts","text":"Environment Reward Signal External to the learning algorithm even if its internal to the learning system as a whole Agent containing: Agent state Policy Value function (probably) Model (optionally)","title":"Core Concepts:"},{"location":"machine_learning/reinforcement_learning/#rewards","text":"A reward R_t is a scalar feedback signal Indicates how well agent is doing at step t The agent's job is to maximize cumulative reward G_t = R_{t+1} + R_{t+2} + ... RL is based on the reward hypothesis Any goal can be formalized as the outcome of maximizing a cumulative reward Dense- reward on every step Sparse- reward only when the desired event happens Can be negative or positive rewads (Carrot vs. Stick in helping train the algorithm)","title":"Rewards"},{"location":"machine_learning/reinforcement_learning/#values","text":"Value - the expected cumulative reward from state s States use the v function to find their value v(s) = E[G_t | S_t = s] where G_t = R_{t+1} + R_{t+2} + ... Goal is to maximize value by picking suitable actions Rewards and values define desirability of state or acton Returns and values can be defined recursively G_t = R_{t+1} + G_{t+1}","title":"Values"},{"location":"machine_learning/reinforcement_learning/#actions","text":"Goal: Select actions to maximize value Actions may have long term consequences meaning rewards may be delayed Might be better to sacrific immediate reward to gain more long-term reward Ex: Financial investments might take months to mature Blocking opponent moves might help winning chances many moves in the future Policy - a mapping from states to actions","title":"Actions"},{"location":"machine_learning/reinforcement_learning/#action-values","text":"It is possible to condition the value on actions q(s,a) = E[G_t | S_t = s, A_t = a] State action pairs use the Q-function to find their value","title":"Action Values"},{"location":"machine_learning/reinforcement_learning/#agent-components","text":"Agent State Policy Value Function Model","title":"Agent Components"},{"location":"machine_learning/reinforcement_learning/#state","text":"Actions depend on the state of the agent Both agent and environment may have an internal state In the simplest case, there is only one shared state by the environemnt and agent Often there are multiple (sometimes infinite) different states The agent's state is usually different than the environment's state The agent might not know the full state of the environment","title":"State"},{"location":"machine_learning/reinforcement_learning/#environment-state","text":"The environment's internal state Usually not visible to the agent May contain lots of irrelevant information if visible","title":"Environment State"},{"location":"machine_learning/reinforcement_learning/#agent-state","text":"A history is a sequence of observations, action, rewards H_t = O_0,A_0,R_1,O_1,...,O_{t-1},A_{t-1},R_t,O_t The agent state S_t can be constructed from the history and is considerd to be a function of the history S_{t+1} = f(S_t,A_t,R_{t+1},O_{t+1}) where f is a state update function Means the next agent state is dependent on the current agent state, the action taken, the reward, and the observation A simple update function can be concatinating the previous states together to create the current state (Done w/ the Atari game agents by cocatenating frames together since frames are the state) Actions depend on this state Agent state is typically much smaller than the environment state and the full history","title":"Agent State"},{"location":"machine_learning/reinforcement_learning/#fully-observable-environments","text":"When the agent sees the full environment state observation = environment state Agent state could just be the observation S_t = O_t = environment state Then the agent is in a Markov Decision Process Ex: Single player board games in which the player can see the entire board Smart Pong is an example of fully observable environment (VERIFY)","title":"Fully Observable Environments"},{"location":"machine_learning/reinforcement_learning/#markov-decision-processes","text":"MDPs provide a useful mathematical framework for talking about many RL topics Easier to reason about than the full problem (long markovian) Limited b/c of markov assumption Def- A decision process is Markov if the future is independent of the past given the present H_t => S_t => H_{t+1} Once the current state ( S_t ) is known , the history ( H_t ) may be thrown away (useful for space consideration) The current state gives enough information for the next state to be predicted The environment state is typically Markov The history is Markov","title":"Markov Decision Processes"},{"location":"machine_learning/reinforcement_learning/#partially-observable-environments","text":"The agent gets parial information A robot w/ camera vision doesn't have its absolute location A poker playing agent only observes public cards Now the observation is not Markov Formally this is a partially observable Markov decision process (POMDP) The environment state can still be markov, but the agent does not know it The state should contain enough information for good policies/value predictions","title":"Partially Observable Environments"},{"location":"machine_learning/reinforcement_learning/#policy","text":"Defines the agent's behavior A map from agent state to action Deterministic Policy - function that outputs an action based on an input agent state Stochastic Policy - a probability of selecting each action in each state","title":"Policy"},{"location":"machine_learning/reinforcement_learning/#value-function","text":"The actual value function is the expected return V_\\pi(s)=E[G_t|S_t=s,\\pi] = E[R_{t+1}+\\gamma R_{t+2}+\\gamma^2R_{t+3}+... | S_t = s,\\pi] The return has a recursive form since G_t = R_{t+1}+\\gamma G_{t+1} Bellman Equation : V_\\pi (s) = E[R_{t+1} + \\gamma G_{t+1} | S_t = s,A_t \\sim \\pi (s)] where a \\sim \\pi (s) means action a is chosen by policy \\pi in state s Discount Factor ( \\gamma \\in [0,1] ) trades off importance of immediate vs. long-term rewards Closer to 1 means higher importane on immediate rewards (AKA discounting future rewards in favor of immediate rewards) The value depends on the policy Can be used to evaluate desirability of states Can be used to select btw actions","title":"Value Function:"},{"location":"machine_learning/statistical_learning/","text":"Statistical Learning Supervised Learning- building a statistical model for prediting/estimating an ouput based on some inputs. Basically is function approximation Regression- predicting continuous or quantitative outputs Least squares was the earliest form Classifiction- predicting discrete/categorial or qualitative outputs Linear discriminant analysis was the earliest form Unsupervised Learning- learn the relationships and strucuture from input data Clustering- want to group datapoints according to similarity Generalized Linear Model- describes a class of stat learning methods including both linear and logistic regression Only linear methods were used till the 80s becuase non-linear relationships were too computationally expensive to fit at the time In the 80s, computational power was sufficient for nonlinear methods to flourish Classification and Regression trees Generalized additive models Neural Networks Support Vector Machines (90s) In stat learning, inputs are referred to as predictors or independent vars (inputs/features in ML/CS) outputs are responses or dependent variables (ouputs/target in ML) Fitting Linear Models Let there be n datapoints where each datapoint has m features. We want to find the optimal parameters for f(x) = a_0 + \\sum_{i=1}^m a_i x_i . Linear regression of single var: model for making predictions is a line in 2D of 2 vars: model is a plane in 3D (for nonlinear models it would be a mesh/surface) of m vars: model is a hyperplane in the m+1 dimensional input-output space Least Squares Makes huge assumptions about structure of data and yields stable but possibly inaccurate predictions relies heavily on assumption that a linear model/decision boundary is appropriate low variance, high bias We fit a linear model to a set of training data by minimizing a cost function defined by the residual sum of squares RSS RSS(\\theta) = \\sum_{i=1}^n(y_i - x_i^T\\theta)^2 Vectorized: We can take the gradient of the cost function, set equal to zero and then solve for the parameters This way of computing the optimal paramters for a linear model is known as the normal equation Instead of the normal equation, could use gradient descent algorithm to iteratively find the minimum of the quadratic cost function k-nearest neighbor Makes very mild assumptions about structure of data and yields unstable but accurate predictions no assumptions on the underlying data like in least squares resulting in wiggly, unstable decision boundaries that depend on only a handful of neighboring points high variance, low bias Uses observations in the training set closest to the input to create predictions h(x) = \\frac{1}{k}\\sum_{x_i \\in N_k(x)} y_i N_k(x) is the neighborhood of x defined by the closest k points x_i in the training set Prediction is just the average of the k-closest datapoints in the training set How do we define closeness in building the neighborhood? Euclidean distance is nice Summary these two techniques are very popular (e.g. 1-nearest-neighbor is most used technique in low-dim problems) advancements/improvements: kernel methods use weights that decrease smoothly to zero as the distance increases from the target point (rather than the abrupt 0/1 used by k-nearest neighbors) distance kernels are modified to emphasize some variables more than others in high dimensional spaces Local regression fits linear models by locally weighted least squares Linear models fit to an expanded basis of the original inputs allow arbitrarily complex models idk why you would ever want this neural networks constist of sums of nonlinearly transformed linear models model selection & cross validation Given a good model for our training data, is the prediction's error on unseen data good or bad? k-fold cross validation better than simple train test split for estimating model prediction error divide dataset into k-sets For each subset: use it once as a test set and the other k-1 sets for training and record the results discard the model and start over again End result is k performance measures which can be averaged to get the models overall performance gives insight into which d is best for polynomial fit Common k = \\{2, 5, 10\\} better than hold-out leave one out cross validation (k = m) useful when m is small k = 5 or 10 is a good compromise select d based on training and validation best performer (hold out the test data) train final model on training + validation data and then measure performance on testing data Training Set: fit model to best parameters Validation Set: evaluate model with MSE At the end, retrain and measure performance with test set split into train and test set hold out test set for final evaluation (never used for training/model updates) variety of models with diff hyperparams train on training data and evaluate with validation data once the best model hyperparams have been selected, train on the training and validation data evaluate final model with test data that has never been seen before train, val, test = 60%, 20%, 20% of dataset as you increase d meaning the polynomial becomes more complex, it is able to fit the data better so the training loss will continue going down as d increses since essentially you reach a point where the curve is overfitting the data comparing the train loss to the validation loss and picking the d with the minimum validation loss is the best","title":"Statistical Learning"},{"location":"machine_learning/statistical_learning/#statistical-learning","text":"Supervised Learning- building a statistical model for prediting/estimating an ouput based on some inputs. Basically is function approximation Regression- predicting continuous or quantitative outputs Least squares was the earliest form Classifiction- predicting discrete/categorial or qualitative outputs Linear discriminant analysis was the earliest form Unsupervised Learning- learn the relationships and strucuture from input data Clustering- want to group datapoints according to similarity Generalized Linear Model- describes a class of stat learning methods including both linear and logistic regression Only linear methods were used till the 80s becuase non-linear relationships were too computationally expensive to fit at the time In the 80s, computational power was sufficient for nonlinear methods to flourish Classification and Regression trees Generalized additive models Neural Networks Support Vector Machines (90s) In stat learning, inputs are referred to as predictors or independent vars (inputs/features in ML/CS) outputs are responses or dependent variables (ouputs/target in ML)","title":"Statistical Learning"},{"location":"machine_learning/statistical_learning/#fitting-linear-models","text":"Let there be n datapoints where each datapoint has m features. We want to find the optimal parameters for f(x) = a_0 + \\sum_{i=1}^m a_i x_i . Linear regression of single var: model for making predictions is a line in 2D of 2 vars: model is a plane in 3D (for nonlinear models it would be a mesh/surface) of m vars: model is a hyperplane in the m+1 dimensional input-output space","title":"Fitting Linear Models"},{"location":"machine_learning/statistical_learning/#least-squares","text":"Makes huge assumptions about structure of data and yields stable but possibly inaccurate predictions relies heavily on assumption that a linear model/decision boundary is appropriate low variance, high bias We fit a linear model to a set of training data by minimizing a cost function defined by the residual sum of squares RSS RSS(\\theta) = \\sum_{i=1}^n(y_i - x_i^T\\theta)^2 Vectorized: We can take the gradient of the cost function, set equal to zero and then solve for the parameters This way of computing the optimal paramters for a linear model is known as the normal equation Instead of the normal equation, could use gradient descent algorithm to iteratively find the minimum of the quadratic cost function","title":"Least Squares"},{"location":"machine_learning/statistical_learning/#k-nearest-neighbor","text":"Makes very mild assumptions about structure of data and yields unstable but accurate predictions no assumptions on the underlying data like in least squares resulting in wiggly, unstable decision boundaries that depend on only a handful of neighboring points high variance, low bias Uses observations in the training set closest to the input to create predictions h(x) = \\frac{1}{k}\\sum_{x_i \\in N_k(x)} y_i N_k(x) is the neighborhood of x defined by the closest k points x_i in the training set Prediction is just the average of the k-closest datapoints in the training set How do we define closeness in building the neighborhood? Euclidean distance is nice","title":"k-nearest neighbor"},{"location":"machine_learning/statistical_learning/#summary","text":"these two techniques are very popular (e.g. 1-nearest-neighbor is most used technique in low-dim problems) advancements/improvements: kernel methods use weights that decrease smoothly to zero as the distance increases from the target point (rather than the abrupt 0/1 used by k-nearest neighbors) distance kernels are modified to emphasize some variables more than others in high dimensional spaces Local regression fits linear models by locally weighted least squares Linear models fit to an expanded basis of the original inputs allow arbitrarily complex models idk why you would ever want this neural networks constist of sums of nonlinearly transformed linear models","title":"Summary"},{"location":"machine_learning/statistical_learning/#model-selection-cross-validation","text":"Given a good model for our training data, is the prediction's error on unseen data good or bad? k-fold cross validation better than simple train test split for estimating model prediction error divide dataset into k-sets For each subset: use it once as a test set and the other k-1 sets for training and record the results discard the model and start over again End result is k performance measures which can be averaged to get the models overall performance gives insight into which d is best for polynomial fit Common k = \\{2, 5, 10\\} better than hold-out leave one out cross validation (k = m) useful when m is small k = 5 or 10 is a good compromise select d based on training and validation best performer (hold out the test data) train final model on training + validation data and then measure performance on testing data Training Set: fit model to best parameters Validation Set: evaluate model with MSE At the end, retrain and measure performance with test set split into train and test set hold out test set for final evaluation (never used for training/model updates) variety of models with diff hyperparams train on training data and evaluate with validation data once the best model hyperparams have been selected, train on the training and validation data evaluate final model with test data that has never been seen before train, val, test = 60%, 20%, 20% of dataset as you increase d meaning the polynomial becomes more complex, it is able to fit the data better so the training loss will continue going down as d increses since essentially you reach a point where the curve is overfitting the data comparing the train loss to the validation loss and picking the d with the minimum validation loss is the best","title":"model selection &amp; cross validation"},{"location":"machine_learning/supervised_learning/","text":"Supervised Learning Regression- predicting continuous values Classification- predicting discrete values Function Approximation Function approximation is a technique for estimating an unknown underyling function using observations from the domain and their resulting image. Given a dataset of inputs and outputs, we can assume there is an unkwnown underyling function that maps the inputs to the outputs. We can use supervised learning techniques to approximate this function. Supervised learning achieves this by minimizing the error between predicted outputs and expected outputs from the domain by changing its parameters (the weights of the network). Thus, simple feedforward neural networks can be better thought of as function approximators rather than models of how the brain works. The true function mapping inputs to outputs is referred to as the target function since it is the target of the learning process. Thus, intuitively we can better get a sense of what the function is by observing more input-output data points. The universal approximation theorem states a feedforward NN with a linear output layer and at least 1 hidden layer with a nonlinear activation function can approximate any function mapping between two finite dimensional spaces with any desired non-zero amount of error (provided enough hidden units). Below is an example of y=x^2 being approximated by a simple feedforward NN with 2 hidden layers of 10 nodes with relu activation trained on 100 observations: . Resources Function Approximators","title":"Supervised Learning"},{"location":"machine_learning/supervised_learning/#supervised-learning","text":"Regression- predicting continuous values Classification- predicting discrete values","title":"Supervised Learning"},{"location":"machine_learning/supervised_learning/#function-approximation","text":"Function approximation is a technique for estimating an unknown underyling function using observations from the domain and their resulting image. Given a dataset of inputs and outputs, we can assume there is an unkwnown underyling function that maps the inputs to the outputs. We can use supervised learning techniques to approximate this function. Supervised learning achieves this by minimizing the error between predicted outputs and expected outputs from the domain by changing its parameters (the weights of the network). Thus, simple feedforward neural networks can be better thought of as function approximators rather than models of how the brain works. The true function mapping inputs to outputs is referred to as the target function since it is the target of the learning process. Thus, intuitively we can better get a sense of what the function is by observing more input-output data points. The universal approximation theorem states a feedforward NN with a linear output layer and at least 1 hidden layer with a nonlinear activation function can approximate any function mapping between two finite dimensional spaces with any desired non-zero amount of error (provided enough hidden units). Below is an example of y=x^2 being approximated by a simple feedforward NN with 2 hidden layers of 10 nodes with relu activation trained on 100 observations: .","title":"Function Approximation"},{"location":"machine_learning/supervised_learning/#resources","text":"Function Approximators","title":"Resources"},{"location":"machine_learning/nn/activation_functions/","text":"Activation Functions Mathematical equations that determine the output of a node/neuron in a neural network Determines whether each neuron should be activated based on the neuron's input and the specific activation function Normalize the output of a neuron to a range: [0,1] or [-1,1] Must be computationally efficient b/c they are calculated across thousands or millions of neurons for each pass Having a computationally efficient derivative is good too for backpropagation Can be as simple as a step function that turns the neuron output on or off (e.g. 0 or 1) Binary Step Function (Doesn't allow multi-value outputs so it can't do multi-class classification) Can be a trasnformation that maps input signals to output signals Linear activations functions collapse a neural network into one layer b/c a linear combination of linear functions is still a linear function (not very useful) Can't use backpropagation with linear activation functions b/c the derivative is a constant so its impossible to see the effect of changing weights Non-linear activation functions allow the neural network to learn more complex data like images, videos, and audio Allow backpropagation b/c the derivative function is related to the inputs Allow sacking of multiple hidden layers unlike linear activation functions Most all processes can be modeled by a neural network w/ nonlinear activation functions Why do we need activation functions? Matrix multiplication and vector addition are simply linear transformations so in order to learn any nonlinear representations of data, we need to introduce a non-linear function or else the most complex classification boundary it can learn will be linear (e.g. line, plane, etc. ) which isn't that useful for interesting problems. In order to introduce non-linearity (otherwise matrix ops will result in only linear transformations), we apply non-linear activation functions pointwise to the results of the linear transformations at each layer. Sigmoid/Logistic \\sigma(x) = \\dfrac{1}{1 + e^{-x}} \\dfrac{d}{dx}\\sigma(x) = \\sigma(x)(1 - \\sigma(x)) Pros: Smooth Gradient prevents jumps in the output values Output vaues bound btw 0 and 1, normalizing the each neuron's output Clear predictions - any values above 2 or below -2 are very close to the edge of the curve Cons: Vanishing Gradient- almost no change in the prediction for very high and very low input values Can result in the network refusing to learn further or being too slow to reach an accurate prediction Outputs not zero centered Computationally expensive Detailed Derivation: \\begin{align}\\dfrac{d}{dx} \\sigma(x) &= \\dfrac{d}{dx} \\left[ \\dfrac{1}{1 + e^{-x}} \\right] \\\\ &= \\dfrac{d}{dx} \\left( 1 + \\mathrm{e}^{-x} \\right)^{-1} \\\\ &= -(1 + e^{-x})^{-2}(-e^{-x}) \\\\ &= \\dfrac{e^{-x}}{\\left(1 + e^{-x}\\right)^2} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{e^{-x}}{1 + e^{-x}} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{(1 + e^{-x}) - 1}{1 + e^{-x}} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( \\dfrac{1 + e^{-x}}{1 + e^{-x}} - \\dfrac{1}{1 + e^{-x}} \\right) \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( 1 - \\dfrac{1}{1 + e^{-x}} \\right) \\\\ &= \\sigma(x) \\cdot (1 - \\sigma(x)) \\end{align} TanH/ Hyperbolic Tangent Similar in advantages and disadvantages except it has the advantage of being zero centered , making it easier to models input that have strongly negative, neutral, and strongly positive values ReLU (Rectified Linear Unit) Pros: Computationally efficient - allows the network to converge quickly Non-linear - although it seems lik e a lienar function, its derivative allows for backpropagation Cons: The Dying ReLU problem - as inputs approach zero or are negative, the gradient becomes zero, so the network can't perform backpropagation and therefore cannot learn Leaky ReLU Pros: Prevents dying ReLU problem - the smallpositive slope in the negative area enables backpropagation for negative inputs Otherwise like ReLU Cons: Inconsistent Results - does't provide consistent predicitions for negative input values https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/","title":"Activation Functions"},{"location":"machine_learning/nn/activation_functions/#activation-functions","text":"Mathematical equations that determine the output of a node/neuron in a neural network Determines whether each neuron should be activated based on the neuron's input and the specific activation function Normalize the output of a neuron to a range: [0,1] or [-1,1] Must be computationally efficient b/c they are calculated across thousands or millions of neurons for each pass Having a computationally efficient derivative is good too for backpropagation Can be as simple as a step function that turns the neuron output on or off (e.g. 0 or 1) Binary Step Function (Doesn't allow multi-value outputs so it can't do multi-class classification) Can be a trasnformation that maps input signals to output signals Linear activations functions collapse a neural network into one layer b/c a linear combination of linear functions is still a linear function (not very useful) Can't use backpropagation with linear activation functions b/c the derivative is a constant so its impossible to see the effect of changing weights Non-linear activation functions allow the neural network to learn more complex data like images, videos, and audio Allow backpropagation b/c the derivative function is related to the inputs Allow sacking of multiple hidden layers unlike linear activation functions Most all processes can be modeled by a neural network w/ nonlinear activation functions","title":"Activation Functions"},{"location":"machine_learning/nn/activation_functions/#why-do-we-need-activation-functions","text":"Matrix multiplication and vector addition are simply linear transformations so in order to learn any nonlinear representations of data, we need to introduce a non-linear function or else the most complex classification boundary it can learn will be linear (e.g. line, plane, etc. ) which isn't that useful for interesting problems. In order to introduce non-linearity (otherwise matrix ops will result in only linear transformations), we apply non-linear activation functions pointwise to the results of the linear transformations at each layer.","title":"Why do we need activation functions?"},{"location":"machine_learning/nn/activation_functions/#sigmoidlogistic","text":"\\sigma(x) = \\dfrac{1}{1 + e^{-x}} \\dfrac{d}{dx}\\sigma(x) = \\sigma(x)(1 - \\sigma(x)) Pros: Smooth Gradient prevents jumps in the output values Output vaues bound btw 0 and 1, normalizing the each neuron's output Clear predictions - any values above 2 or below -2 are very close to the edge of the curve Cons: Vanishing Gradient- almost no change in the prediction for very high and very low input values Can result in the network refusing to learn further or being too slow to reach an accurate prediction Outputs not zero centered Computationally expensive Detailed Derivation: \\begin{align}\\dfrac{d}{dx} \\sigma(x) &= \\dfrac{d}{dx} \\left[ \\dfrac{1}{1 + e^{-x}} \\right] \\\\ &= \\dfrac{d}{dx} \\left( 1 + \\mathrm{e}^{-x} \\right)^{-1} \\\\ &= -(1 + e^{-x})^{-2}(-e^{-x}) \\\\ &= \\dfrac{e^{-x}}{\\left(1 + e^{-x}\\right)^2} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{e^{-x}}{1 + e^{-x}} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{(1 + e^{-x}) - 1}{1 + e^{-x}} \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( \\dfrac{1 + e^{-x}}{1 + e^{-x}} - \\dfrac{1}{1 + e^{-x}} \\right) \\\\ &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( 1 - \\dfrac{1}{1 + e^{-x}} \\right) \\\\ &= \\sigma(x) \\cdot (1 - \\sigma(x)) \\end{align}","title":"Sigmoid/Logistic"},{"location":"machine_learning/nn/activation_functions/#tanh-hyperbolic-tangent","text":"Similar in advantages and disadvantages except it has the advantage of being zero centered , making it easier to models input that have strongly negative, neutral, and strongly positive values","title":"TanH/ Hyperbolic Tangent"},{"location":"machine_learning/nn/activation_functions/#relu-rectified-linear-unit","text":"Pros: Computationally efficient - allows the network to converge quickly Non-linear - although it seems lik e a lienar function, its derivative allows for backpropagation Cons: The Dying ReLU problem - as inputs approach zero or are negative, the gradient becomes zero, so the network can't perform backpropagation and therefore cannot learn","title":"ReLU (Rectified Linear Unit)"},{"location":"machine_learning/nn/activation_functions/#leaky-relu","text":"Pros: Prevents dying ReLU problem - the smallpositive slope in the negative area enables backpropagation for negative inputs Otherwise like ReLU Cons: Inconsistent Results - does't provide consistent predicitions for negative input values https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/","title":"Leaky ReLU"},{"location":"machine_learning/nn/auto_encoders/","text":"Auto Encoders Takes high dim input and tries to compress it to a smaller representation Two Principle Components: Encoder- series of layers that compress input to smaller layer callde the bottleneck Decoder-","title":"AutoEncoders"},{"location":"machine_learning/nn/auto_encoders/#auto-encoders","text":"Takes high dim input and tries to compress it to a smaller representation Two Principle Components: Encoder- series of layers that compress input to smaller layer callde the bottleneck Decoder-","title":"Auto Encoders"},{"location":"machine_learning/nn/pytorch/","text":"PyTorch Blitz autograd- calculates and stores the gradients for each model param in each parameter's .grad attribute simple training loop: import torch model = torchvision.models.resnet18(pretrained=True) data = torch.rand(1,3,64,64) # 64 x 64 img with 3 channels labels = torch.rand(1,1000) prediction = model(data) # Forward pass loss = (prediction - labels).sum() loss.backward() # backward pass (backprop) # register all the model params in the optimizer optimizer = torch.optim.SGD(model.parameters(),lr=1e-2,momentum=0.9) optimizer.step() # initiate gradient descent if a parameter in a NN does not requires_grad then these params are known as frozen meaning the gradients won't be recomputed. NOTE: torch.no_grad also does same thing ( read more ). also useful for finetuning pretrained networks where you only want to modify the params in the classifer layers to make predictions on the new labels: model = torchvision.models.resnet18(pretrained=True) # Freeze all the parameters in the network for param in model.parameters(): param.requires_grad = False # replace last linear layer (the classifier) with new unfrozen classification layer model.fc = nn.Linear(512, 10) torch.nn pkg nn.Module contains layers and a forward(input) method that returns the output. Can build modules of networks that can be used in other networks. Only have to define the forward function and the backward function will be automatically defined using autograd (relies on autograd parsing the operations in the forward function and creating the appropriate computational graph of all the derivatives) net.parameters() returns the learnable parameters of a nn module only supports mini-batch inputs, no single input samples import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = torch.flatten(x, 1) # flatten all dimensions except batch x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target. Many are provided by the nn pkg such as nn.MSELoss but you can also define your own In order to backpropagate the error/loss, we need to clear the existing gradients ( net.zero_grad() ) and then call loss.backwards() https://pytorch.org/docs/stable/nn.html updating the weights can be done using the below code since weights = weights - learning rate * gradient for SGD: learning_rate = 0.01 for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) using the torch.optim module lets you easily use other update rules like SGD, Adam, RMSProp, etc. import torch.optim as optim # create your optimizer optimizer = optim.SGD(net.parameters(), lr=0.01) # in your training loop: optimizer.zero_grad() # zero the gradient buffers output = net(input) loss = criterion(output, target) loss.backward() optimizer.step() # Does the update recap torch.Tensor - A multi-dimensional array with support for autograd operations like backward() . Also holds the gradient w.r.t. the tensor. nn.Module - Neural network module. Convenient way of encapsulating parameters , with helpers for moving them to GPU, exporting, loading, etc. nn.Parameter - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module . autograd.Function - Implements forward and backward definitions of an autograd operation . Every Tensor operation creates at least a single Function node that connects to functions that created a Tensor and encodes its history . gradient clipping optimizer.zero_grad() loss.backward() # by value torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1) # by norm # torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm=1) optimizer.step() Note that it stacks all the paremeters into a single vector then performs the clipping we want to just clip the gradients from th hooks tensor.backward() starts the backward pass on the computational graph with a default starting gradient value of 1.0 allow us to inspect (and possibly change) gradients as they flow backwards through the graph hooks get called on tensors in the order they were added .retain_grad() stores the grad on non-leaf/intermediate nodes in the computational graph when adding hooks to a intermediate node in the forward graph (stored in the backward_hooks dict), the function will also be added as a pre-hook to the corresponding node in the backwards graph to be run on the gradient before the node does its thing def fn(grad): print(grad) return grad + 2 # if you return nothing, the same gradient as before will be used c.register_hook(fn)","title":"PyTorch"},{"location":"machine_learning/nn/pytorch/#pytorch-blitz","text":"autograd- calculates and stores the gradients for each model param in each parameter's .grad attribute simple training loop: import torch model = torchvision.models.resnet18(pretrained=True) data = torch.rand(1,3,64,64) # 64 x 64 img with 3 channels labels = torch.rand(1,1000) prediction = model(data) # Forward pass loss = (prediction - labels).sum() loss.backward() # backward pass (backprop) # register all the model params in the optimizer optimizer = torch.optim.SGD(model.parameters(),lr=1e-2,momentum=0.9) optimizer.step() # initiate gradient descent if a parameter in a NN does not requires_grad then these params are known as frozen meaning the gradients won't be recomputed. NOTE: torch.no_grad also does same thing ( read more ). also useful for finetuning pretrained networks where you only want to modify the params in the classifer layers to make predictions on the new labels: model = torchvision.models.resnet18(pretrained=True) # Freeze all the parameters in the network for param in model.parameters(): param.requires_grad = False # replace last linear layer (the classifier) with new unfrozen classification layer model.fc = nn.Linear(512, 10)","title":"PyTorch Blitz"},{"location":"machine_learning/nn/pytorch/#torchnn-pkg","text":"nn.Module contains layers and a forward(input) method that returns the output. Can build modules of networks that can be used in other networks. Only have to define the forward function and the backward function will be automatically defined using autograd (relies on autograd parsing the operations in the forward function and creating the appropriate computational graph of all the derivatives) net.parameters() returns the learnable parameters of a nn module only supports mini-batch inputs, no single input samples import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = torch.flatten(x, 1) # flatten all dimensions except batch x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target. Many are provided by the nn pkg such as nn.MSELoss but you can also define your own In order to backpropagate the error/loss, we need to clear the existing gradients ( net.zero_grad() ) and then call loss.backwards() https://pytorch.org/docs/stable/nn.html updating the weights can be done using the below code since weights = weights - learning rate * gradient for SGD: learning_rate = 0.01 for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) using the torch.optim module lets you easily use other update rules like SGD, Adam, RMSProp, etc. import torch.optim as optim # create your optimizer optimizer = optim.SGD(net.parameters(), lr=0.01) # in your training loop: optimizer.zero_grad() # zero the gradient buffers output = net(input) loss = criterion(output, target) loss.backward() optimizer.step() # Does the update","title":"torch.nn pkg"},{"location":"machine_learning/nn/pytorch/#recap","text":"torch.Tensor - A multi-dimensional array with support for autograd operations like backward() . Also holds the gradient w.r.t. the tensor. nn.Module - Neural network module. Convenient way of encapsulating parameters , with helpers for moving them to GPU, exporting, loading, etc. nn.Parameter - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module . autograd.Function - Implements forward and backward definitions of an autograd operation . Every Tensor operation creates at least a single Function node that connects to functions that created a Tensor and encodes its history .","title":"recap"},{"location":"machine_learning/nn/pytorch/#gradient-clipping","text":"optimizer.zero_grad() loss.backward() # by value torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1) # by norm # torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm=1) optimizer.step() Note that it stacks all the paremeters into a single vector then performs the clipping we want to just clip the gradients from th","title":"gradient clipping"},{"location":"machine_learning/nn/pytorch/#hooks","text":"tensor.backward() starts the backward pass on the computational graph with a default starting gradient value of 1.0 allow us to inspect (and possibly change) gradients as they flow backwards through the graph hooks get called on tensors in the order they were added .retain_grad() stores the grad on non-leaf/intermediate nodes in the computational graph when adding hooks to a intermediate node in the forward graph (stored in the backward_hooks dict), the function will also be added as a pre-hook to the corresponding node in the backwards graph to be run on the gradient before the node does its thing def fn(grad): print(grad) return grad + 2 # if you return nothing, the same gradient as before will be used c.register_hook(fn)","title":"hooks"},{"location":"optimization/convergence/","text":"Convergence Analysis We are interested in how fast the error goes to zero for a given iterative optimization method where \\lim_{k \\to \\infty} x_k = x^* . Define the error as e_k = ||x_k - x^*|| . Q-Rate Convergence Q-rate tells us how fast a sequence convergences to a value. x_k \\to x^* with q-rate r if there exists a constant c such that for any sufficiently large k : ||x_{k+1} - x^*|| \\leq c ||x_{k+1} -x_k||^r Also can be represented as ||e_{k+1}|| \\leq c||e_k||^r for sufficiently large k where e is the error btw the current point and optimal point at a given iteration. We can alternatively write that ||e_{k_1}|| = O(e_k) using big-o notation to represent asymptotic growth. Taking the negative logarithm of the error term will give us the number of decimal places of accuracy for that given iteration's approximation. For instance, let e_k = 10^{-6} , then the number of decimal places of accuracy is -\\log_{10}10^{-6} = 6 . We can take the negative logarithm of both sides of the above equation to get: -\\log_{10} e_{k+1} \\geq r (- \\log_{10} e_k) - \\log_{10} c This expression tells us how fast the number of decimal places of accuracy is increasing. Linear rate r = 1 and c < 1 Ex: 1, 1/2 1/4, 1/8, ... -> 0 for r = 1 and c = \\frac{1}{2} Superlinear r = 1 The constant c changes over time: c_k \\to 0 as k \\to 0 Quadratic r = 2 r is large enough that it swamps the effect of c no matter its value Ex: 10^{-1}, 10^{-2}, 10^{-4}, 10^{-8}, \\cdots \\to 0 for r= 2 and c = 1 Q-rate Limit Lemma Suppose r \\geq 1 and L exists where ( L < 1 if r = 1 ) L = \\lim_{k \\to \\infty} \\frac{||x_{k+1} - x^* ||}{||x_k - x^* ||^r} Then x_k \\to x^* with Q-rate r and a constant C > L R-Rate Convergence Tells us how fast the bounds on the error converges to 0. The error has a bounds that has a q-rate convergence x_k \\to x^* with r-rate r if ||e_k || \\leq b_k for all k where b_k \\to 0 with q-rate r Summary Linear: moderate to large c is slow small c is fast Superlinear: fast Quadratic: very fast","title":"Convergence"},{"location":"optimization/convergence/#convergence-analysis","text":"We are interested in how fast the error goes to zero for a given iterative optimization method where \\lim_{k \\to \\infty} x_k = x^* . Define the error as e_k = ||x_k - x^*|| .","title":"Convergence Analysis"},{"location":"optimization/convergence/#q-rate-convergence","text":"Q-rate tells us how fast a sequence convergences to a value. x_k \\to x^* with q-rate r if there exists a constant c such that for any sufficiently large k : ||x_{k+1} - x^*|| \\leq c ||x_{k+1} -x_k||^r Also can be represented as ||e_{k+1}|| \\leq c||e_k||^r for sufficiently large k where e is the error btw the current point and optimal point at a given iteration. We can alternatively write that ||e_{k_1}|| = O(e_k) using big-o notation to represent asymptotic growth. Taking the negative logarithm of the error term will give us the number of decimal places of accuracy for that given iteration's approximation. For instance, let e_k = 10^{-6} , then the number of decimal places of accuracy is -\\log_{10}10^{-6} = 6 . We can take the negative logarithm of both sides of the above equation to get: -\\log_{10} e_{k+1} \\geq r (- \\log_{10} e_k) - \\log_{10} c This expression tells us how fast the number of decimal places of accuracy is increasing.","title":"Q-Rate Convergence"},{"location":"optimization/convergence/#linear","text":"rate r = 1 and c < 1 Ex: 1, 1/2 1/4, 1/8, ... -> 0 for r = 1 and c = \\frac{1}{2}","title":"Linear"},{"location":"optimization/convergence/#superlinear","text":"r = 1 The constant c changes over time: c_k \\to 0 as k \\to 0","title":"Superlinear"},{"location":"optimization/convergence/#quadratic","text":"r = 2 r is large enough that it swamps the effect of c no matter its value Ex: 10^{-1}, 10^{-2}, 10^{-4}, 10^{-8}, \\cdots \\to 0 for r= 2 and c = 1","title":"Quadratic"},{"location":"optimization/convergence/#q-rate-limit-lemma","text":"Suppose r \\geq 1 and L exists where ( L < 1 if r = 1 ) L = \\lim_{k \\to \\infty} \\frac{||x_{k+1} - x^* ||}{||x_k - x^* ||^r} Then x_k \\to x^* with Q-rate r and a constant C > L","title":"Q-rate Limit Lemma"},{"location":"optimization/convergence/#r-rate-convergence","text":"Tells us how fast the bounds on the error converges to 0. The error has a bounds that has a q-rate convergence x_k \\to x^* with r-rate r if ||e_k || \\leq b_k for all k where b_k \\to 0 with q-rate r","title":"R-Rate Convergence"},{"location":"optimization/convergence/#summary","text":"Linear: moderate to large c is slow small c is fast Superlinear: fast Quadratic: very fast","title":"Summary"},{"location":"optimization/method-template/","text":"Method A brief overview including what info of the function is used (1st, 2nd, 3rd derivs, ) and any conditions Method Pros Cons","title":"Method"},{"location":"optimization/method-template/#method","text":"A brief overview including what info of the function is used (1st, 2nd, 3rd derivs, ) and any conditions","title":"Method"},{"location":"optimization/method-template/#method_1","text":"","title":"Method"},{"location":"optimization/method-template/#pros","text":"","title":"Pros"},{"location":"optimization/method-template/#cons","text":"","title":"Cons"},{"location":"optimization/mpc/","text":"Model Predictive Control Relies on a dynamic model to make predictions for future values of the controlled variables of the system. It then solves a contrained optimization problem to calculate the optimal control action that minimizes the difference between the predicted controlled values and the desired/set control values. Main limitation: High computational cost since this optimization problem must be solved at each time step The computational cost increases even more as the time horizon over which to predict future control values increases. The computational cost becomes intractable to run realtime as the number of states and contraints grows (NN approximation could fix this) Explicit MPC (eMPC) and learning-based NN approaches have been proposed to overcome this Takes on many forms: one-norm, infinite norm, PWA model, tracking Explicit MPC Goal is to rewrite the optimization problem as a multi-parametric quadtratic programming (mpQP) This is possible for linear systems but cannot be extended to nonlinear systems An explicit solution to the mpQP has been demonstrated and it is of the form of a PWA function The PWA function defined over the state space will act as a fast prediction/lookup function for control policies given a input state meaning there is no need to solve the optimization problem online! This result was achieved by Bemporad et al. (2002) Resources: ( Brief mention ) Optimality: Not always feasible to run the optimization process to complete convergence for the most optimal control output (especially when running online) Therefore, optimality is sometimes relaxed to only require a sufficient decrease in the cost/objective function Then the optimization problem would be terminated at a feasible but sub-optimal stage Recursive Feasability: MPC controllers are not gauranteed to be stabilizing (linear quadratic control is gauranted to be stabilizing) Stability must be built in when defining the optimization problem The optimization problem\u2019s feasbility can be lost when the state is driven to a region in which the optimization problem has no solution typical approach to solve this is to append constrainst to the optimization problem to guarantee that loss of feasability cannot occur (that is what was done in the papers I have read so far) When this is done, an MPC controller is said to be recursively feasible meaning it cannot put the state in a place with no solution to the optimization problem Constructing an MPC controller with the theoretical (a-priori) guarantee of recursive feasability is not always desirable or easy to do Sometimes we might have a situation where given an MPC controller and the goal is to determine if it is recursively feasbile The goal is primarly to invalidate the controller by detecting problematic states where the recursive feasability is lost Second objective is to find certificates of guaranteed recursive feasability DEF: The MPC controller is recursively feasible if and only if for all ini- tially feasible x0 and for all optimal sequences of control inputs the MPC optimization problem remains feasible for all time. DEF: The MPC controller is strongly recursively feasible if and only if for all initially feasible x0 and for all sequences of feasible control inputs the MPC optimization problem remains feasible for all time. Resource (analysis tool for determining recursive feasability of an MPC controller)","title":"Model Predictive Control"},{"location":"optimization/mpc/#model-predictive-control","text":"Relies on a dynamic model to make predictions for future values of the controlled variables of the system. It then solves a contrained optimization problem to calculate the optimal control action that minimizes the difference between the predicted controlled values and the desired/set control values. Main limitation: High computational cost since this optimization problem must be solved at each time step The computational cost increases even more as the time horizon over which to predict future control values increases. The computational cost becomes intractable to run realtime as the number of states and contraints grows (NN approximation could fix this) Explicit MPC (eMPC) and learning-based NN approaches have been proposed to overcome this Takes on many forms: one-norm, infinite norm, PWA model, tracking","title":"Model Predictive Control"},{"location":"optimization/mpc/#explicit-mpc","text":"Goal is to rewrite the optimization problem as a multi-parametric quadtratic programming (mpQP) This is possible for linear systems but cannot be extended to nonlinear systems An explicit solution to the mpQP has been demonstrated and it is of the form of a PWA function The PWA function defined over the state space will act as a fast prediction/lookup function for control policies given a input state meaning there is no need to solve the optimization problem online! This result was achieved by Bemporad et al. (2002) Resources: ( Brief mention )","title":"Explicit MPC"},{"location":"optimization/mpc/#optimality","text":"Not always feasible to run the optimization process to complete convergence for the most optimal control output (especially when running online) Therefore, optimality is sometimes relaxed to only require a sufficient decrease in the cost/objective function Then the optimization problem would be terminated at a feasible but sub-optimal stage","title":"Optimality:"},{"location":"optimization/mpc/#recursive-feasability","text":"MPC controllers are not gauranteed to be stabilizing (linear quadratic control is gauranted to be stabilizing) Stability must be built in when defining the optimization problem The optimization problem\u2019s feasbility can be lost when the state is driven to a region in which the optimization problem has no solution typical approach to solve this is to append constrainst to the optimization problem to guarantee that loss of feasability cannot occur (that is what was done in the papers I have read so far) When this is done, an MPC controller is said to be recursively feasible meaning it cannot put the state in a place with no solution to the optimization problem Constructing an MPC controller with the theoretical (a-priori) guarantee of recursive feasability is not always desirable or easy to do Sometimes we might have a situation where given an MPC controller and the goal is to determine if it is recursively feasbile The goal is primarly to invalidate the controller by detecting problematic states where the recursive feasability is lost Second objective is to find certificates of guaranteed recursive feasability DEF: The MPC controller is recursively feasible if and only if for all ini- tially feasible x0 and for all optimal sequences of control inputs the MPC optimization problem remains feasible for all time. DEF: The MPC controller is strongly recursively feasible if and only if for all initially feasible x0 and for all sequences of feasible control inputs the MPC optimization problem remains feasible for all time. Resource (analysis tool for determining recursive feasability of an MPC controller)","title":"Recursive Feasability:"},{"location":"optimization/overview/","text":"Overview Optimization is a huge field with a tremendous number of applications. It is also intertwined in ML so that is why its include in the ML notebook. Here is where I will try to keep a working glossary of areas I am being introduced to with a brief overview of what they are. Being able to map out how many pieces of a field fit together greatly helps me when diving deeper into any one area. What is Optimization? Optimizing simply stands for finding the maximum or minimum of a function. This max or min is often subject to a series of constraints. The form of the function being optimized and the constraints being adhered to (e.g. linear, nonlinear, continuous, etc.) determines the type of problem and thus the technique used to solve them. Here is the mathematical definition of an optimization problem: \\min f_0(x) subject to the contraints: f_i(x) \\leq b_i Here is the textbook on convex optimization used for reference. Linear Programming (LP) Optimizing a linear objective/cost function subject to linear equality/inequality constraints Nonlinear Programming (NLP) Optimizing an objective/cost function over a set of real variables subject to system of equations called constraints (more general class that includes LP) Quadtratic Programming (QP) Methods for optimizing a quadtratic objective function subject to linear constraints Sequential Programming Sequential Quadtratic Programming (SQP) Involves solving a series of subproblems where each is a quadratic program Sequential Linear Quadtratic Programming (SLQP) Involves solving a linear program and equality constrained quadratic program at each step Dynamic Programming (DP) Involves breaking down the main optimization problem into simpler sub-problems. The Bellman Equation describes the relationship between the value of the larger problem and the values of the sub-problems Piece Wise Affine Functions (PWAs) Approximates a function using a series of lines in 2D, planes in 3D, & hyperplanes in higher dimensional spaces. While the picture above shows the PWA given a known function, PWAs are most often used when there is a set of datapoints representing the functions inputs and outputs. The task is then to construct a PWA from the data such that it best approximates the true function that is mapping the inputs to the outputs. Once you have a PWA made from your dataset, you can then make predictions on new inputs as to what the true function would map to. Constraints A constraint is a condition of an optimization problem that the solution must satisfy Feasible Set - is the set of candidate solutions that satisfy all constraints A point is infeasible if it does not satisfy a constraint A constraint is binding if an inequality constraint holds with equality at the optimal point i.e. the point is at the edge of the feasible set and cannot move anymore in the direction of the constraint, even if doing so would improve the value of the objective function, since it would move outside the feasible set A constraint is non-binding if the point could be moved in the direction of the constraint i.e. the point is not at the edge of the feasible set and can be moved more in the direction of the constraint, however doing so would not be optimal This can mean under certain conditions that the optimization problem would have the same solution in the absence of the constraint since the objective function's minimum does not push up against the edge of the feasible set Slack Variable - A variable added to an inequality constraint to transform it into an equality If the slack variable for a constraint is zero at a point, the constraint is binding there If the slack variable for a constraint is positive at a point, the constraint is non-binding there If the slack variable for a constraint is negative at a point, the point is infeasible as it does not satisfy the constraint (can be thought of as outside the feasible set) Slack variables can never be negative for most interior point and simplex solvers For example, introducing the slack variable y \\geq 0 , the inequality constraint of Ax \\leq b can be transformed into an equality of Ax + y = b Intuition: slack can be thought of like a distance from a candidate solution to the constraint boundary ( y = Ax - b ) A constraint of Ax > b will be transformed into two constraints of s = Ax - b and s > 0 that is used by most solvers Lagrange Multipliers KKT conditions generalize the method of lagrange multipliers and allow inequality constraints","title":"Overview"},{"location":"optimization/overview/#overview","text":"Optimization is a huge field with a tremendous number of applications. It is also intertwined in ML so that is why its include in the ML notebook. Here is where I will try to keep a working glossary of areas I am being introduced to with a brief overview of what they are. Being able to map out how many pieces of a field fit together greatly helps me when diving deeper into any one area.","title":"Overview"},{"location":"optimization/overview/#what-is-optimization","text":"Optimizing simply stands for finding the maximum or minimum of a function. This max or min is often subject to a series of constraints. The form of the function being optimized and the constraints being adhered to (e.g. linear, nonlinear, continuous, etc.) determines the type of problem and thus the technique used to solve them. Here is the mathematical definition of an optimization problem: \\min f_0(x) subject to the contraints: f_i(x) \\leq b_i Here is the textbook on convex optimization used for reference.","title":"What is Optimization?"},{"location":"optimization/overview/#linear-programming-lp","text":"Optimizing a linear objective/cost function subject to linear equality/inequality constraints","title":"Linear Programming (LP)"},{"location":"optimization/overview/#nonlinear-programming-nlp","text":"Optimizing an objective/cost function over a set of real variables subject to system of equations called constraints (more general class that includes LP)","title":"Nonlinear Programming (NLP)"},{"location":"optimization/overview/#quadtratic-programming-qp","text":"Methods for optimizing a quadtratic objective function subject to linear constraints","title":"Quadtratic Programming (QP)"},{"location":"optimization/overview/#sequential-programming","text":"","title":"Sequential Programming"},{"location":"optimization/overview/#sequential-quadtratic-programming-sqp","text":"Involves solving a series of subproblems where each is a quadratic program","title":"Sequential Quadtratic Programming (SQP)"},{"location":"optimization/overview/#sequential-linear-quadtratic-programming-slqp","text":"Involves solving a linear program and equality constrained quadratic program at each step","title":"Sequential Linear Quadtratic Programming (SLQP)"},{"location":"optimization/overview/#dynamic-programming-dp","text":"Involves breaking down the main optimization problem into simpler sub-problems. The Bellman Equation describes the relationship between the value of the larger problem and the values of the sub-problems","title":"Dynamic Programming (DP)"},{"location":"optimization/overview/#piece-wise-affine-functions-pwas","text":"Approximates a function using a series of lines in 2D, planes in 3D, & hyperplanes in higher dimensional spaces. While the picture above shows the PWA given a known function, PWAs are most often used when there is a set of datapoints representing the functions inputs and outputs. The task is then to construct a PWA from the data such that it best approximates the true function that is mapping the inputs to the outputs. Once you have a PWA made from your dataset, you can then make predictions on new inputs as to what the true function would map to.","title":"Piece Wise Affine Functions (PWAs)"},{"location":"optimization/overview/#constraints","text":"A constraint is a condition of an optimization problem that the solution must satisfy Feasible Set - is the set of candidate solutions that satisfy all constraints A point is infeasible if it does not satisfy a constraint A constraint is binding if an inequality constraint holds with equality at the optimal point i.e. the point is at the edge of the feasible set and cannot move anymore in the direction of the constraint, even if doing so would improve the value of the objective function, since it would move outside the feasible set A constraint is non-binding if the point could be moved in the direction of the constraint i.e. the point is not at the edge of the feasible set and can be moved more in the direction of the constraint, however doing so would not be optimal This can mean under certain conditions that the optimization problem would have the same solution in the absence of the constraint since the objective function's minimum does not push up against the edge of the feasible set","title":"Constraints"},{"location":"optimization/overview/#slack-variable","text":"","title":"Slack Variable"},{"location":"optimization/overview/#-a-variable-added-to-an-inequality-constraint-to-transform-it-into-an-equality","text":"If the slack variable for a constraint is zero at a point, the constraint is binding there If the slack variable for a constraint is positive at a point, the constraint is non-binding there If the slack variable for a constraint is negative at a point, the point is infeasible as it does not satisfy the constraint (can be thought of as outside the feasible set) Slack variables can never be negative for most interior point and simplex solvers For example, introducing the slack variable y \\geq 0 , the inequality constraint of Ax \\leq b can be transformed into an equality of Ax + y = b Intuition: slack can be thought of like a distance from a candidate solution to the constraint boundary ( y = Ax - b ) A constraint of Ax > b will be transformed into two constraints of s = Ax - b and s > 0 that is used by most solvers","title":"- A variable added to an inequality constraint to transform it into an equality"},{"location":"optimization/overview/#lagrange-multipliers","text":"KKT conditions generalize the method of lagrange multipliers and allow inequality constraints","title":"Lagrange Multipliers"},{"location":"optimization/root-methods/bisection/","text":"Bisection Method Uses only g Uses only g = f' where f is an objective function in an optimization problem Roughly equivalent to binary search over a continuous domain instead of over a discerete list Method Choose a starting interval [a_0, b_0] such that g(a_0) g(b_0) < 0 (alternatively x^* \\in [a_0, b_0] where g(x^*) = 0 ) interval should contain the root different signs at the endpoints of the interval implies by the intermediate value theorem that a root exists in between them due to continuity of g Compute g(c_k) where c_k = \\frac{a_k + b_k}{2} (i.e. the midpoint of the interval) Determine the next subinterval [a_{k+1}, b_{k+1}] : If g(a_k) g(c_k) < 0 then a_{k+1} = a_k and b_{k+1} = c_k If g(b_k) g(c_k) < 0 then a_{k+1} = c_k and b_{k+1} = b_k Convergence Actual error is not gauranteed to go down in any iteration Only the bound on error is gauranteed to go down Therefore, we have R-linear convergence with c = 1/2 since the bound on the error is cut in half at each iteration Pros Requires no derivative information of g (only need f' of objective function) Reliable- guaranteed to converge in a predictable way given an initial interval that contains a root Can calculate the min number of iterations required to guarantee an error less than \\epsilon > 0 ( Link ) Cons Slow","title":"Bisection"},{"location":"optimization/root-methods/bisection/#bisection-method","text":"Uses only g Uses only g = f' where f is an objective function in an optimization problem Roughly equivalent to binary search over a continuous domain instead of over a discerete list","title":"Bisection Method"},{"location":"optimization/root-methods/bisection/#method","text":"Choose a starting interval [a_0, b_0] such that g(a_0) g(b_0) < 0 (alternatively x^* \\in [a_0, b_0] where g(x^*) = 0 ) interval should contain the root different signs at the endpoints of the interval implies by the intermediate value theorem that a root exists in between them due to continuity of g Compute g(c_k) where c_k = \\frac{a_k + b_k}{2} (i.e. the midpoint of the interval) Determine the next subinterval [a_{k+1}, b_{k+1}] : If g(a_k) g(c_k) < 0 then a_{k+1} = a_k and b_{k+1} = c_k If g(b_k) g(c_k) < 0 then a_{k+1} = c_k and b_{k+1} = b_k","title":"Method"},{"location":"optimization/root-methods/bisection/#convergence","text":"Actual error is not gauranteed to go down in any iteration Only the bound on error is gauranteed to go down Therefore, we have R-linear convergence with c = 1/2 since the bound on the error is cut in half at each iteration","title":"Convergence"},{"location":"optimization/root-methods/bisection/#pros","text":"Requires no derivative information of g (only need f' of objective function) Reliable- guaranteed to converge in a predictable way given an initial interval that contains a root Can calculate the min number of iterations required to guarantee an error less than \\epsilon > 0 ( Link )","title":"Pros"},{"location":"optimization/root-methods/bisection/#cons","text":"Slow","title":"Cons"},{"location":"optimization/root-methods/newtons/","text":"Newton's Method Finds the root of a function g i.e. finds x such that g(x) = 0 Uses g and g' In the context of optimization where g = f' , we would need first and second derivatives of the cost function f Method Given x_k we will approximate g(x) by the tangent line l_k(x) at x_k . This tangent line is the 1st order taylor polynomial of g at x_k and represents a linear approximation of g . l_k(k) = g(x_k) + g'(x_k)(x - x_k) If we let x_{k+1} solve l_k(x) = 0 we get the equation for the next step in the iterative process of finding the root: x_{k + 1} = x_k - \\frac{g(x_k)}{g'(x_k)} Convergence Q-quadratic convergence The constant C > \\frac{g''(x^*)}{2g'(x^*)} This tells us that that curvature of g around x^* in a scale invariant way Less curvature -> smaller C -> faster convergence. Pros Very fast Cons May diverge or converge to something unexpected Can get misdirected when g' is small (i.e. g is flat) when the cost function f doesn't change very rapidly, g will be flat and newton's method will not perform well due to g'(x_k) \\neq 0 in the denominator of the update step Requires g' which is the second derivative of the cost function in an optimization context, so this can be unfeasible for certain problems or computationally expensive to compute Other Notes Since we are using a linear approximation of g to find its roots, the actual curvature of g affects how fast Newton's method converges. For low curvature, the method is very fast.","title":"Newtons Method"},{"location":"optimization/root-methods/newtons/#newtons-method","text":"Finds the root of a function g i.e. finds x such that g(x) = 0 Uses g and g' In the context of optimization where g = f' , we would need first and second derivatives of the cost function f","title":"Newton's Method"},{"location":"optimization/root-methods/newtons/#method","text":"Given x_k we will approximate g(x) by the tangent line l_k(x) at x_k . This tangent line is the 1st order taylor polynomial of g at x_k and represents a linear approximation of g . l_k(k) = g(x_k) + g'(x_k)(x - x_k) If we let x_{k+1} solve l_k(x) = 0 we get the equation for the next step in the iterative process of finding the root: x_{k + 1} = x_k - \\frac{g(x_k)}{g'(x_k)}","title":"Method"},{"location":"optimization/root-methods/newtons/#convergence","text":"Q-quadratic convergence The constant C > \\frac{g''(x^*)}{2g'(x^*)} This tells us that that curvature of g around x^* in a scale invariant way Less curvature -> smaller C -> faster convergence.","title":"Convergence"},{"location":"optimization/root-methods/newtons/#pros","text":"Very fast","title":"Pros"},{"location":"optimization/root-methods/newtons/#cons","text":"May diverge or converge to something unexpected Can get misdirected when g' is small (i.e. g is flat) when the cost function f doesn't change very rapidly, g will be flat and newton's method will not perform well due to g'(x_k) \\neq 0 in the denominator of the update step Requires g' which is the second derivative of the cost function in an optimization context, so this can be unfeasible for certain problems or computationally expensive to compute","title":"Cons"},{"location":"optimization/root-methods/newtons/#other-notes","text":"Since we are using a linear approximation of g to find its roots, the actual curvature of g affects how fast Newton's method converges. For low curvature, the method is very fast.","title":"Other Notes"},{"location":"optimization/root-methods/regula_falsi/","text":"Regula Falsi Known as the method of false position Only uses g Bracketing method like Bisection to guaranteed to converge Method Construct a secant line using the the two interval endpoints Approximates g l_k(x) = g(a_k) + \\frac{g(b_k) - g(a_k)}{b_k - a_k}(x - a_k) Find the root of the line c_k = \\frac{a_k g(b_k) - b_k g(a_k)}{g(b_k) - g(a_k)} Determine the next subinterval [a_{k+1}, b_{k+1}] : If g(a_k) g(c_k) < 0 then a_{k+1} = a_k and b_{k+1} = c_k If g(b_k) g(c_k) < 0 then a_{k+1} = c_k and b_{k+1} = b_k Convergence Q-Linear convergence c depends on the curvature of g and how close a_0 is to x^* Since we are using a linear approximation of g , less curvature = faster convergence Pros Reliable given the interval encloses x^* Cons Slow convergence with a constant that depends on the curvature of g","title":"Regula Falsi"},{"location":"optimization/root-methods/regula_falsi/#regula-falsi","text":"Known as the method of false position Only uses g Bracketing method like Bisection to guaranteed to converge","title":"Regula Falsi"},{"location":"optimization/root-methods/regula_falsi/#method","text":"Construct a secant line using the the two interval endpoints Approximates g l_k(x) = g(a_k) + \\frac{g(b_k) - g(a_k)}{b_k - a_k}(x - a_k) Find the root of the line c_k = \\frac{a_k g(b_k) - b_k g(a_k)}{g(b_k) - g(a_k)} Determine the next subinterval [a_{k+1}, b_{k+1}] : If g(a_k) g(c_k) < 0 then a_{k+1} = a_k and b_{k+1} = c_k If g(b_k) g(c_k) < 0 then a_{k+1} = c_k and b_{k+1} = b_k","title":"Method"},{"location":"optimization/root-methods/regula_falsi/#convergence","text":"Q-Linear convergence c depends on the curvature of g and how close a_0 is to x^* Since we are using a linear approximation of g , less curvature = faster convergence","title":"Convergence"},{"location":"optimization/root-methods/regula_falsi/#pros","text":"Reliable given the interval encloses x^*","title":"Pros"},{"location":"optimization/root-methods/regula_falsi/#cons","text":"Slow convergence with a constant that depends on the curvature of g","title":"Cons"},{"location":"optimization/root-methods/secant/","text":"Secant Method Finds the root of a function g i.e. finds x such that g(x) = 0 Uses only g Method Use the two most recent points to construct a secant line l_k(x) Compute the root of the secant line to find the next point x_{k+1} = \\frac{x_{k-1}g(x_k) - x_kg(x_{k-1})}{g(x_k) - g(x_{k-1})} equivalent to x_{k+1} = x_k - \\frac{g(x_k)(x_{k-1} - x_k)}{g(x_k) - g(x_{k-1})} numerically less likely to lose preceision Convergence Superlinear convergence with a q-rate of 1.618 (golden ratio) Slower than newton's but faster than bisection and regula falls The constant C > \\frac{g''(x^*)}{2g'(x^*)} This tells us that that curvature of g around x^* in a scale invariant way Less curvature -> smaller C -> faster convergence. Pros Relatively fast Doesn't require g' Cons May diverge or converge to something unexpected Other Notes Since we are using a linear approximation of g to find its roots, the actual curvature of g affects how fast Newton's method converges. For low curvature, the method is very fast.","title":"Secant Method"},{"location":"optimization/root-methods/secant/#secant-method","text":"Finds the root of a function g i.e. finds x such that g(x) = 0 Uses only g","title":"Secant Method"},{"location":"optimization/root-methods/secant/#method","text":"Use the two most recent points to construct a secant line l_k(x) Compute the root of the secant line to find the next point x_{k+1} = \\frac{x_{k-1}g(x_k) - x_kg(x_{k-1})}{g(x_k) - g(x_{k-1})} equivalent to x_{k+1} = x_k - \\frac{g(x_k)(x_{k-1} - x_k)}{g(x_k) - g(x_{k-1})} numerically less likely to lose preceision","title":"Method"},{"location":"optimization/root-methods/secant/#convergence","text":"Superlinear convergence with a q-rate of 1.618 (golden ratio) Slower than newton's but faster than bisection and regula falls The constant C > \\frac{g''(x^*)}{2g'(x^*)} This tells us that that curvature of g around x^* in a scale invariant way Less curvature -> smaller C -> faster convergence.","title":"Convergence"},{"location":"optimization/root-methods/secant/#pros","text":"Relatively fast Doesn't require g'","title":"Pros"},{"location":"optimization/root-methods/secant/#cons","text":"May diverge or converge to something unexpected","title":"Cons"},{"location":"optimization/root-methods/secant/#other-notes","text":"Since we are using a linear approximation of g to find its roots, the actual curvature of g affects how fast Newton's method converges. For low curvature, the method is very fast.","title":"Other Notes"}]}